{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: qiskit in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.23.2)\n",
      "Requirement already satisfied: qiskit-aer==0.7.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.7.2)\n",
      "Requirement already satisfied: qiskit-ibmq-provider==0.11.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.11.1)\n",
      "Requirement already satisfied: qiskit-ignis==0.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.5.1)\n",
      "Requirement already satisfied: qiskit-terra==0.16.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.16.1)\n",
      "Requirement already satisfied: qiskit-aqua==0.8.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.8.1)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.16.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (1.19.2)\n",
      "Requirement already satisfied: cython>=0.27.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (0.29.21)\n",
      "Requirement already satisfied: pybind11>=2.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (2.6.1)\n",
      "Requirement already satisfied: nest-asyncio!=1.1.0,>=1.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: requests-ntlm>=1.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (2.8.1)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.25.10)\n",
      "Requirement already satisfied: websockets>=8 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (8.1)\n",
      "Requirement already satisfied: requests>=2.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (2.24.0)\n",
      "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ignis==0.5.1->qiskit) (0.23.2)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ignis==0.5.1->qiskit) (50.3.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.14.5)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.2.0)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (0.3.3)\n",
      "Requirement already satisfied: ply>=3.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.11)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.5)\n",
      "Requirement already satisfied: retworkx>=0.5.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (0.7.2)\n",
      "Requirement already satisfied: psutil>=5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (5.8.0)\n",
      "Requirement already satisfied: python-constraint>=1.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: sympy>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.7.1)\n",
      "Requirement already satisfied: quandl in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (3.6.0)\n",
      "Requirement already satisfied: dlx in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.0.4)\n",
      "Requirement already satisfied: h5py in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.10.0)\n",
      "Requirement already satisfied: pandas in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.1.3)\n",
      "Requirement already satisfied: fastdtw in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.3.4)\n",
      "Requirement already satisfied: yfinance in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.1.55)\n",
      "Requirement already satisfied: docplex in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.19.202)\n",
      "Requirement already satisfied: cryptography>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (3.3.1)\n",
      "Requirement already satisfied: ntlm-auth>=1.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from python-dateutil>=2.8.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.15.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2020.6.20)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.17->qiskit-ignis==0.5.1->qiskit) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.17->qiskit-ignis==0.5.1->qiskit) (2.1.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (20.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (0.17.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from networkx>=2.2->qiskit-terra==0.16.1->qiskit) (4.4.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from sympy>=1.3->qiskit-terra==0.16.1->qiskit) (1.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflection>=0.3.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (0.5.1)\n",
      "Requirement already satisfied: more-itertools in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (8.6.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from pandas->qiskit-aqua==0.8.1->qiskit) (2020.1)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (0.0.9)\n",
      "Requirement already satisfied: lxml>=4.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (4.6.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.14.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (2.20)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pylatexenc in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (2.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torchvision in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.8.2)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: torch==1.7.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.7.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch==1.7.1->torchvision) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (4.56.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for itertools\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install qiskit\n",
    "!{sys.executable} -m pip install pylatexenc\n",
    "!{sys.executable} -m pip install torchvision\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import execute\n",
    "from qiskit.circuit import Parameter,ControlledGate\n",
    "from qiskit import Aer\n",
    "import qiskit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "\n",
    "NUM_QUBITS = 2\n",
    "NUM_SHOTS = 5000\n",
    "SHIFT = np.pi/4\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "SIMULATOR = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '01', '10', '11']\n"
     ]
    }
   ],
   "source": [
    "# create list of all possible outputs of quantum circuit (2**NUM_QUBITS possible)\n",
    "import itertools\n",
    "\n",
    "def create_QC_OUTPUTS():\n",
    "    measurements = list(itertools.product([0, 1], repeat=NUM_QUBITS))\n",
    "    return [''.join([str(bit) for bit in measurement]) for measurement in measurements]\n",
    "\n",
    "QC_OUTPUTS = create_QC_OUTPUTS()\n",
    "print(QC_OUTPUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contruct QuantumCircuit QFT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumCircuit:\n",
    "    def __init__(self, n_qubits, backend, shots):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.circuit = qiskit.QuantumCircuit(n_qubits)\n",
    "        self.thetas = { k: Parameter('Theta'+str(k)) for k in range(self.n_qubits) }\n",
    "        \n",
    "        all_qubits = [i for i in range(n_qubits)]\n",
    "        \n",
    "        self.circuit.h(all_qubits)\n",
    "        self.circuit.barrier()\n",
    "        for k in range(n_qubits):\n",
    "            self.circuit.ry(self.thetas[k], k)\n",
    "            \n",
    "        self.circuit.measure_all()\n",
    "        \n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "        \n",
    "    def N_qubits_expectation_Z(self, counts, shots, n_qubits):\n",
    "        expects = np.zeros(len(QC_OUTPUTS))\n",
    "        for k in range(len(QC_OUTPUTS)):\n",
    "            key = QC_OUTPUTS[k]\n",
    "            perc = counts.get(key, 0) / shots\n",
    "            expects[k] = perc\n",
    "        return expects\n",
    "    \n",
    "    def run(self, i):\n",
    "        params = i\n",
    "        \n",
    "        job_sim = execute(self.circuit, \n",
    "                          self.backend, \n",
    "                          shots = self.shots, \n",
    "                          parameter_binds = [{ self.thetas[k]: params[k].item() for k in range(self.n_qubits) }])\n",
    "        \n",
    "        result_sim = job_sim.result()\n",
    "        counts = result_sim.get_counts(self.circuit)\n",
    "        \n",
    "        return self.N_qubits_expectation_Z(counts, self.shots, self.n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value for rotation [pi/4]: [0.0172 0.1204 0.1148 0.7476]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAACoCAYAAAAb366wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdzElEQVR4nO3de1iUdf7/8ScgAspBkQAFSRFBQTmJWqYCguaqqatgq679NA0lTE2/ZZ4x81CK0MHw25q67nZQ0dB1dT0Cm9JaeEoMM0wDygOKaBigDPz+4OtsBMJgMPcw9/txXV3XcM9n5n7NZ5eXN/dpTCoqKioQQghhVEyVDiCEEKLhSbkLIYQRknIXQggjJOUuhBBGSMpdCCGMkJS7EEIYISl3IYQwQlLuQghhhKTchRDCCEm5CyGEEZJyF0IIIyTlLoQQRkjKXQghjJCUuxBCGCEpdyGEMEJS7kIIYYSk3IUQwghJuQshhBGSchdCCCPUTOkAwnicP3++zjHvvfce06dPr3VMly5dGiqS0ZK5FnWRLXehV+vWrVM6gmrIXKublLsQQhghKXchhDBCUu5Cr5KSkpSOoBoy1+om5S6EEEZIyl3oVUREhNIRVEPmWt3kVEgF7cyAH28ps26X1jAqSJl1K+HbI/DzdWXWbeMIXgOUWbcSZs2axenTp/W+Xn9/fxISEvS+XkMl5a6gH2/BRYUKR21+vg6FeUqnUIfTp0+TlpamdAzVk90yQq9iYmKUjqAaMtfqJuUu9KquKyZFw5G5Vjcpd6FX/fv3VzqCashcq5uUu9Cr/Px8pSOohsy1uskBVSH+z5zEELJ++AIzM3NMTc1wbt2RcWELCPaLVDqaEPUm5S70ytvbW+kItRofvojx4QvRaMrYlf4eKz8eh4dLAC4OHkpHqzdDn2vRuGS3jNCrHTt2KB1BJ2ZmzfhD7xfQlJdx8afTSsd5JE1lrgFatmxJr169GDhwICEhIbi6utY6vnfv3oSHh+spXdMk5S70avHixUpH0Mn9snvsSU8EwNXBU+E0j8bQ57p169a8/PLLnDlzhjt37nD8+HEOHDhASkoKubm5XLt2jb/85S8EBARUeV3v3r3Zv38/u3fvxt/fX5nwTYCUu9Cr7du3Kx2hVh8fXs7IRa0YNt+KTfsXMjtyA+7tfAFY8dE4/vPNHu3YJZtHkvHtAaWi1smQ5/qFF17g8uXLrF27Fl9fXzQaDadPn+bgwYN8/vnnFBQU4OjoyJQpUzh58iTbtm3DwcFBW+x2dnbs3r2bs2fPKv1RDJYqy728vJw1a9bQuXNnLC0t8fPzIy0tDS8vL6KiopSO91BJb4TwZfIbOi8X9TcubAHJywpJir1Bry5DOJOdon0uekQCm/cvori0iM/P7qSlpR1BXoMUTNv0WFlZkZyczAcffICtrS2HDx9m5MiR2NjYEBAQwKBBg+jfvz9t2rTB29ub+Ph4ioqKiIyM5Ntvv+XQoUPY2dmxdetWxo8fj0ajUfojGSxVlvvkyZNZtmwZU6dOZd++fYwZM4axY8fy/fff06NHD6XjCQNg06I1syM3cPz8P0nP3AVAa2tH/th3Jut2zeDjw28wbXi8wimbFnNzc5KTkxkxYgQFBQU8++yzhIeHs2vXLkpLS6uNz8rKYvbs2XTr1o2MjAzs7e2xtrbm4MGDUuw6UN3ZMp988gmbN28mNTWV4OBgAEJDQzl58iQ7d+4kMDBQ4YTGrSndc8S2hT2j+81m47/m84T3M5iamvJ0z4ns+3IDI5+agW0Le6Uj1srQ5nrJkiUMGjSIa9euERISotP3wAI4OzvTuXNn7c8eHh5YWlpy9+7dxopqFFS35b5ixQoGDx6sLfYHPDw8MDc3x9e3cv/q5cuXCQ4OxtPTk+7du/P5558rEdfonDt3TukI9fLHfjMpuHOFgye2aJe1a+PRJE6NNKS59vf3Z+7cuZSXlzN69Gidi/3X+9i3bdvGyZMn6dixIytXrmzkxE2fqrbc8/LyyMzM5OWXX672XE5ODj4+PlhYWAAwdepUnn32WV588UXS09OJjIzk0qVLNG/evM71mJiY6JRn9IIUXLuG1OszfLlrOSf2rqmy7H5JEW7d6ndaWFpaKjMGhdbrNXWpaV5/Kz4+vs5x8fENv7tjzbQU/DqF1DomLjq12rKWlrbsfL3gd607LS2VnmPVM9c1eeWVV2jWrBnvvPMOx44d0+k1vy72B/vYfXx8OHXqFFFRUSxdupSbN29qx6elpen8u9eUVVRU6DROVVvueXmV93x1dnausry4uJi0tDTtLpkbN25w9OhRJk+eDECfPn1o164dKSkpKK3XiAVEf1BY5b92nn2VjiXEQzk4OBAREYFGo2H16tU6vaamYtdoNHz99dfs3bsXCwsLJk2a1MjJmzZVbbk7ODgAcOHCBYYMGaJd/tZbb3HlyhXtwdScnBycnJy0W/EAHTt25IcfftBpPbr+y/ruQeXu5x4cHELSG7rl1JUuf2rHx8fXeUbS2rVrGyqSVsanDXc/91f/tLle44ODQ6hIVM9ch4SEVNnf369fP5o3b86hQ4e0G1i1eVixP7BlyxaGDRtGWFgYa9b896/Y4OBgUlNTG/SzNGWqKnd3d3d8fX1ZsWIF9vb2uLi4kJSUxN69ewHkTBk9WLp0qdIRVMNQ5vrB79Xx48frHFtXsf/6feT3tXaq2i1jamrK9u3b8fHxITo6mkmTJuHg4EBMTAxmZmbag6lubm5cu3atyulZly5d4vHHH1cqutEYM2aM0hFUw1Dm2sXFBYCLFy/WOk6XYofKv6zv37/PY489RrNmqto+rRfVzYynp2e1fecTJkzA29sbKysroHL3zVNPPcWHH36oPaD6448/EhrasAfF6itiYWq9lhuirl27kpWVpXQMVTCUuZ46dSqzZs2ipKSk1nEODg5YWVnpdIGSs7MzJSUllJWVNXRco6G6cq9JRkYGTzzxRJVl69evZ+LEiSQkJNC8eXM++eQTnc6UEYbtbvFtFm8eAUD2T6fwaBeAs31HwgLGE+hZ+xlHZy6m4tjKjbZt3Gt8XqMpY/W2SVwtuMQTXYfxpwGvNXj+pujevXvcu3evznH//Oc/6du3LydPnqzzAqWCgt93BpMaqGq3TE2Kioq4cOFCtYuX3N3d+fe//82FCxfIzMysdl68aJpaWtkRF51KXHQqHZ27ExedilNr3Xa3nbmYypWb3z/0+fRvdtPesQsJMUfJvHyUgjtXGyq2anz11Vdy5WkDUf2Wu7W1tfyfSY9CQkKUjlCjgye28GnKKpzsOzAncgOFRfnEbZ9McenPuDl2JXp4AgcyNnMs8zMCOocTHjiB93fP5P79Ep70GcG4sPmc/+E/9PONAMCvUyjnc7+kj89wxT6Toc610A/Vb7kL/UpMTFQ6Qo08XAJ4a+ohrt/Koai4kE9TVjE2dB5rpqVgZWHDd3knGBQ0kanD4pj2TBztHb2Im5bKuzOOc/K7g5TeL6aopJAWlrYAtLS0425xoaKfyVDnWuiHlLvQq+joaKUj1KiDczcA2ti2427JbXKuZ7Fh32vMSQzhVPZhbt75qcr4qwWXWPDhEGYnBpNzLYvCouu0tLTjl5I7APxScoeWVq30/TGqMNS5Fvqh+t0yQr8M9SITE/572XpFRQXtH/MiLPDPeLpWnkut0ZSRk38eTUXlLrx/fJHIs6Fz8esUwqx1famoqKDr409yKvswXdx6ceZiCqEBYxX5LA8Y6lwL/ZByF6IGYwfMJyEpirsltzExMWV25Ab83EPYuG8e53OO07vLUN77bDpuTt6Ym1WeRfWk9zOs3rqDWev60qvLENrYtlX4Uwg1k3IXqpUQcxSA5wbFapf9+tYCsRM/qzLe2b4Da1/8t/bnnl0GV3vPeeM+atiQQjwi2ecu9MoQLqpRC5lrdZMtdwW5tFbfurdt26bIZfE2jnpfpeLrVmquH+VLq7/PuQKAu1vbKo8be73GzKRC11sYClEHXe5UqMsl8V26dGmoSEbL2Ob6tTc/AGDV3Kgqj8Wjk90yQghhhKTchRDCCEm5C716//33lY6gGjLX6iblLvTKx8dH6QiqIXOtblLuQq/k7pr6I3OtblLuQghhhKTchV717NlT6QiqIXOtblLuQq+++uorpSOohsy1ukm5CyGEEZJyF0IIIyTlLvQqKSlJ6QiqIXOtblLuQghhhKTchV5FREQoHUE1ZK7VTW75q6CdGfDjLWXW7dIaRgUps24lfHsEfr6uzLptHMFrgDLrFrqZNWsWp0+fVmTd/v7+JCQkNPj7Srkr6MdbcFGhwlGbn69DYZ7SKYShOn36NGlpaUrHaFCyW0boVUxMjNIRVEPmWt2k3IVeTZ8+XekIqiFzrW5S7kKv+vfvr3QE1ZC5Vjcpd6FX+fn5SkdQDZlrdZMDqkL8nzmJIWT98AVmZuaYmprh3Loj48IWEOwXqXQ0IepNyl3olbe3t9IRajU+fBHjwxei0ZSxK/09Vn48Dg+XAFwcPJSOVm+GPteiccluGaFXO3bsUDqCTszMmvGH3i+gKS/j4k+nlY7zSJrKXKuJnZ2d3tYl5S70avHixUpH0Mn9snvsSU8EwNXBU+E0j6apzHVT1KtXLxYtWsSuXbs4deoUZ86c4ciRI8TFxTFq1CjMzc2rvWbw4MFcvnyZgQMH6iWjlLvQq+3btysdoVYfH17OyEWtGDbfik37FzI7cgPu7XwBWPHROP7zzR7t2CWbR5Lx7QGlotbJ0Oe6KRo+fDgnTpzg+PHjvP766wwfPhx/f398fX0JDQ1l9uzZ7Nixg5ycHObPn0/z5s2BymJPTk6mVatWDBo0SC9ZVVvu5eXlrFmzhs6dO2NpaYmfnx9paWl4eXkRFRWldDyhkHFhC0heVkhS7A16dRnCmewU7XPRIxLYvH8RxaVFfH52Jy0t7Qjy0s8vqlCWra0tH330Ebt27SIwMJAbN27wzjvvMHbsWHr06IGfnx9Dhgxh8eLFnD17FmdnZ5YvX05GRgbR0dEkJydjYWHBu+++yyuvvKKXzKo9oDp58mR27tzJokWL6NGjB+np6YwdO5b8/Hxmz56tdLwaJb0Rglu3cHqNXKjTcvHobFq0ZnbkBv7fqk6kZ+6iT7cRtLZ25I99Z7Ju1wwu/nSaN6MOKR1T6EGrVq04ePAgQUFB3L17lwULFrB+/XpKS0urjPv666/Zt28fy5YtIywsjMTERLp37866deswMTHh3XffZcaMGXrLrcot908++YTNmzeze/du/ud//ofQ0FAWLFjAk08+SVlZGYGBgUpHNFpN6f4dti3sGd1vNhv/NZ/y8nIAnu45kbz8C4x8aga2LewVTli7pjTXhsrExISdO3cSFBREdnY2/v7+vP3229WK/bcOHz7MK6+8gkajwcTEhJKSEtasWaOn1JVUWe4rVqxg8ODBBAcHV1nu4eGBubk5vr6V+1gXL16Mp6cnpqam8sUHDeTcuXNKR6iXP/abScGdKxw8sUW7rF0bjyZxamRTm2tD9NJLLxEaGsrVq1cJDQ0lOztbp9cNHjyYrVu3YmZmxqVLl7C0tGTDhg2NnLYq1e2WycvLIzMzk5dffrnaczk5Ofj4+GBhYQFU/g80ceJEnn/++Xqtw8TERKdxoxek4No1pF7v3VDS0lKZMSi0Qd+zpjn9rfj4+DrHxcfHN1QkrTXTUvDrFFLrmLjo1GrLWlrasvP1gt+17rS0VHqOVc9cP4q5q/4XqPzd+fVjJdnZ2bF8+XIAoqKiyMvT7baiDw6ePtjHvmzZMs6dO8fAgQMZOXIkycnJVcanpaXV67NWVFToNE6V5Q7g7OxcZXlxcTFpaWn84Q9/0C7r06ePXrPp4stdyzmxt+qfd/dLinDrFq5QIiGM03PPPYe1tTVHjhzhH//4h06v+W2xP9jHvnz5chISEnjxxRerlXtjUV25Ozg4AHDhwgWGDBmiXf7WW29x5coVevTo8bvXoeu/rO8erP/93HuNWFDjAdX6Cg4OIekN3XLq6vz583WOiY+Pr/NspLVr1zZUJK2MTxvufu6v/mlzvcYHB4dQkaieuX4Ur735AVD5u/Prx/oSEhJS7RjF2LFjAXj//fd1eo+HFTvAX//6V1auXMnAgQNxcHDgxo0b2ueCg4NJTU39/R/iN1RX7u7u7vj6+rJixQrs7e1xcXEhKSmJvXv3AjRIuYuHW7p0qdIRVEPm+tGZmZnh7+8PVB4crUttxQ5QWFhIRkYG/fr1o0ePHuzfv78xYlehugOqpqambN++HR8fH6Kjo5k0aRIODg7ExMRgZmamPZgqGseYMWOUjqAaMtePzt3dHSsrK3744QcKCwtrHVtXsT/w4Gv8fHx8GjhtzVS35Q7g6elJSkpKlWUTJkzA29sbKysrhVKpQ9euXcnKylI6hirIXD+627dvExsbW2ext27dmq1bt9ZZ7AB79uzh1q1bHD9+vIHT1kyV5V6TjIwMnnjiiSrLFi1axKZNm8jPz+fs2bPMmjWLtLQ0OnXqpEjGiIWp9VouqrtbfJvFm0cAkP3TKTzaBeBs35GwgPEEetZ+UPrMxVQcW7nRto17jc9n/3iKNz99juLSn/n7/MsNHV3o0fXr13XarXXr1i3Gjx9PaGgoc+bMqXXsgQMHOHBAf7erUN1umZoUFRVx4cKFahcvLVu2jLy8PEpLS7l58yZ5eXmKFbtoGC2t7IiLTiUuOpWOzt2Ji07FqfXjOr32zMVUrtz8/qHPt2vjwTsv/QcHO9eGiiuagD179tRZ7EqQLXfA2toajUajdAxVCAkJUTpCjQ6e2MKnKatwsu/AnMgNFBblE7d9MsWlP+Pm2JXo4QkcyNjMsczPCOgcTnjgBN7fPZP790t40mcE48Lm08LSRumPUYWhzrXQD9lyF3qVmJiodIQaebgE8NbUQ1y/lUNRcSGfpqxibOg81kxLwcrChu/yTjAoaCJTh8Ux7Zk42jt6ETctlXdnHOfkdwcpvV+s9EeoxlDnWuiHlLvQq+joaKUj1KiDczcA2ti2427JbXKuZ7Fh32vMSQzhVPZhbt75qcr4qwWXWPDhEGYnBpNzLYvConpesKAHhjrXQj9kt4zQq8a4WKMhmPDfy78rKipo/5gXYYF/xtO18roHjaaMnPzzaCoqd9/944tEng2di1+nEGat66vXC250ZahzLfRDyl2IGowdMJ+EpCjultzGxMSU2ZEb8HMPYeO+eZzPOU7vLkN577PpuDl5Y25W+YUM1wtzWbN1EpevZvLq/4YzO3IDzvYdlP0gQrWk3IVqJcQcBeC5QbHaZb++tUDsxM+qjHe278DaF/+t/blnl8HV3vOtqXKPd2EYZJ+70Cu5qEZ/ZK7VTcpd6NW2bduUjqAaMtfqJrtlFOTSWn3rXrJkiSL3PLFx1PsqFV+3UnPdFD24SVh9fZ9zBQB3t7ZVHutj3XWRclfQqCClE6iH1wClEwhDlpCQ8Eive3B74lVzo6o8NgSyW0YIIYyQlLvQK12/+ED8fjLX6iblLvRKX/eyFjLXaiflLvQqODhY6QiqIXOtblLuQghhhKTchRDCCMmpkKLBdOnSpc4xS5Ys0WmcqJ3MtaiLbLkLvYqNjVU6gmrIXKublLsQQhghKXchhDBCUu5CCGGEpNyFEMIISbkLIYQRknIXQggjJOUuhBBGSMpdz3JzcwkLC6Nr1674+Pgwb948pSMJ0eSlpqbi4+ODh4cHU6ZMQaPRKB2pTjNnzsTV1ZVmzRrnWlIpdz1r1qwZb775JllZWZw6dYqjR4+ya9cupWMJ0WSVl5czZcoUtm/fTnZ2Nnfu3OHvf/+70rHqFBkZSUZGRqO9v5S7nrVt25agoMqvYGrevDkBAQHk5OQonEqIpuurr76iXbt2eHt7AzB58mR27NihcKq69e3bF2dn50Z7f7m3jIIKCgpITk7mwIEDSkcRQq/ul5WxZccBin4prrL87U07anw8JLQ3nTu41vheeXl5tG/fXvuzm5sbubm5DZy40pdnzvPFyXPVlteUu62jPZFDQjAxMWmULHWRLXeF3Lt3j4iICGbOnCk3dxKqY96sGU/28OHK9ZtcuX5Tu/y3j69cv0kr25Z4PO7y0PeqqKho1Ky/FuDjQXl5eZ25r9+8Rf9efooVO0i5K0Kj0TBu3Dj8/f2ZM2eO0nGEUIS3x+ME+XrVOqallSWjBvevtSTbt29fZUs9JycHV9eat/J/L/NmzXj2mQGYmdZenYP69cT5MftGyaArKXcFREVFYWNjQ1xcnNJRhFDUMwOexN7O5qHPjxrcH5uWLWp9j6CgIPLy8vjmm28A+PDDDxk1alSD5vy1do5tGNgv6KHPd2zfln49uzfa+nUl5a5nx44dY+PGjWRkZBAQEIC/vz/vvPMOoN8/L4UwBBYWzRkzLJSatst7dPfEx7NDne9hZmbGhg0biIiIoFOnTlhbWzNhwoQGz/pr/Xv50sG1+sFQi+bmRA4NwbSOLXuAqVOn4urqikajwdXVlZiYmAbNaFIhjWIwPtv/OWZmpgwPf0rpKELo1b/SviT1P6e1P7e2s2HmpNFYWjRXLlQdCgrvkLBpB/fu3dcuixgSTFD32nc16YtsuRuIm4V3+Orr81DjNowQxi28bw/aOrYBKn8DIoeGGHSxA9i3suWZsCe1P3t37kCPbp4KJqrKYMo9NjYWExMTMjMzGTp0KNbW1rRt25bVq1cDsG/fPgIDA2nRogUBAQEcPXq0yuvT09N5+umnsbOzw8rKin79+lUbk5GRwZgxY3Bzc8PKygoPDw9eeuklbt++XWVcdnY2ERERODs7Y2FhgYuLC8OHD+fmzZs0lpQvTmFqYkpIb79GW4cQhqqZmRnPDgvFzMyUfr18cW/fVulIOgnq7kVXj8exbmHFqMH9FD075rcMZrdMbGwsS5cupUuXLkyZMgU/Pz+2bNnC3/72N+bOncuePXtYuHAhNjY2LFiwgNzcXC5fvoyNjQ0HDhxg2LBhDBgwgKioKCwsLFi3bh2HDx/m6NGj9OzZE4CkpCSysrLw8/PDzs6O7OxsVq5ciZOTE8eOHdNm8fLywtbWlldffRUnJyeuXr3KwYMHWbJkiU5H4V9784NGmychhLqtmhul0ziDK/fExESmTZsGQGlpKU5OTvzyyy9cuHCBDh06AHDkyBHCwsJISkpi9OjReHp64uDgwNGjR7UHMsrKyujWrRvu7u7s3bu3xnWWlZXxxRdf0L9/f06dOoW/vz83btzgscceIzk5mREjRjzSZ5FyF0I0Fl3L3eCuUB0yZIj2sYWFBe7u7mg0Gm2xw3+/+T03N5fs7Gy+++47Zs2aRXl5OeXl5dpx4eHhbNq0SftzUVERq1atYuvWreTm5lJaWqp97ttvv8Xf3582bdrg7u7Oa6+9xrVr1+jfv3+9LzLSdfKhcl973F+28kSAD8PD+9RrPUII8TAGV+729lVP/G/evDmWlpbVlgGUlJRw7do1AGJiYh56KlFxcTFWVlY8//zz7Nu3j9jYWAIDA7GxsSE3N5dRo0ZRXFx5GbSJiQmHDh3i9ddfZ+HCheTn52tPU5o7d65O+9QeZcs9/UQm6Scy6/06IYS6NNkt9/pq06byCHtsbCxDhw6tcYyFhQUlJSV89tlnLF68uMpVob89mArQsWNHNm3aREVFBefOnWPjxo3MmzcPBwcHpkyZ0jgfRAghGlCTL3cvLy/c3d05e/YsS5Yseei40tJSysrKMDc3r7J848aND32NiYkJ3bp1Y+3ataxfv56zZ8/qlEnXf1mT9qVx+lw2r079E7Y2LXV6jRBC6KLJl7uJiQnr169n6NChjBgxgj//+c84OjqSn5/PyZMnuX//PqtXr8bOzo4+ffqwZs0anJycaNeuHdu2beP48eNV3u/rr79mxowZjBkzhs6dOwOwfft2iouLefrppxss983CO5zMvMATAT5S7EKIBtfkyx1g4MCBpKens3z5cqKjo/n5559xdHQkMDCQF154QTvu448/Zvr06cyaNQszMzOGDRvG1q1btfdXB3B2dqZDhw68/fbb5OXlYW5uTteuXdm2bVuVg72/V8GtO9hat5Tz2oUQjcJgToVUo/Lycp3uQSGEEPUl5S6EEEZINhuFEMIISbkLIYQRknIXQggjJOUuhBBGSMpdCCGMkJS7EEIYISl3IYQwQlLuQghhhKTchRDCCEm5CyGEEZJyF0IIIyTlLoQQRkjKXQghjJCUuxBCGCEpdyGEMEJS7kIIYYSk3IUQwghJuQshhBGSchdCCCMk5S6EEEZIyl0IIYyQlLsQQhghKXchhDBCUu5CCGGEpNyFEMIISbkLIYQRknIXQggj9P8BXDOf0nS4iRAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 478.079x204.68 with 1 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, NUM_SHOTS)\n",
    "print(f'Expected value for rotation [pi/4]: {circuit.run(torch.Tensor([np.pi/4] * len(QC_OUTPUTS)))}')\n",
    "circuit.circuit.draw(output='mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCircuit(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        if not hasattr(ctx, 'QuantumCircuit'):\n",
    "            ctx.QuantumCircuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, shots=NUM_SHOTS)\n",
    "        \n",
    "        exp_value = ctx.QuantumCircuit.run(i)\n",
    "        \n",
    "        result = torch.Tensor([exp_value])\n",
    "        \n",
    "        ctx.save_for_backward(result, i)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        forward_tensor,i = ctx.saved_tensors\n",
    "        \n",
    "        input_numbers = i\n",
    "        gradients = torch.Tensor()\n",
    "        \n",
    "        for k in range(NUM_QUBITS):\n",
    "            shift_right = input_numbers.detach().clone()\n",
    "            shift_right[k] = shift_right[k] + SHIFT\n",
    "            \n",
    "            shift_left = input_numbers.detach().clone()\n",
    "            shift_left[k] = shift_left[k] - SHIFT\n",
    "            \n",
    "            expectation_right = ctx.QuantumCircuit.run(shift_right)\n",
    "            expectation_left = ctx.QuantumCircuit.run(shift_left)\n",
    "            \n",
    "            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])\n",
    "            gradient = gradient / torch.norm(gradient)\n",
    "            \n",
    "            gradients = torch.cat((gradients, gradient.float()))\n",
    "            \n",
    "        result = torch.Tensor(gradients)\n",
    "        \n",
    "        return (result.float() * grad_output.float()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 after quantum layer: tensor([[0.0202, 0.1200, 0.1274, 0.7324]], grad_fn=<TorchCircuitBackward>)\n",
      "x.grad = tensor([ 0.1519, -0.2446])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "qc = TorchCircuit.apply\n",
    "\n",
    "y1 = qc(x)\n",
    "print(f'y1 after quantum layer: {y1}')\n",
    "y1 = nn.Linear(2**NUM_QUBITS, 1)(y1.float())\n",
    "\n",
    "y1.backward()\n",
    "print(f'x.grad = {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13c3c6c10>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qc = TorchCircuit.apply\n",
    "\n",
    "def cost(x):\n",
    "    target = -1\n",
    "    expval = qc(x)[0]\n",
    "    \n",
    "    val = sum([(i + 1) * expval[i] for i in range(2**NUM_QUBITS)]) / 2**NUM_QUBITS\n",
    "    \n",
    "    return torch.abs(target - val) ** 2, expval\n",
    "\n",
    "\n",
    "x = torch.tensor([-np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "opt = optim.Adam([x], lr = 0.1)\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "loss_list = []\n",
    "expval_list = []\n",
    "\n",
    "for i in tqdm(range(num_epoch)):\n",
    "    opt.zero_grad()\n",
    "    loss, expval = cost(x)\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()\n",
    "    loss_list.append(loss.item())\n",
    "    expval_list.append(expval)\n",
    "    \n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "X_train = datasets.CIFAR10(root='./data', \n",
    "                                train=True,\n",
    "                                download=True,\n",
    "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "idx = np.append(np.where(np.array(X_train.targets) == 0)[0][:n_samples], \n",
    "                np.where(np.array(X_train.targets) == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 3)[0][:n_samples])\n",
    "\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = np.array(X_train.targets)[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "n_samples = 50\n",
    "\n",
    "X_test = datasets.CIFAR10(root='./data', \n",
    "                               train=False,\n",
    "                               download=True,\n",
    "                               transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "idx = np.append(np.where(np.array(X_test.targets) == 0)[0][:n_samples], \n",
    "                np.where(np.array(X_test.targets) == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 3)[0][:n_samples])\n",
    "\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = np.array(X_test.targets)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(X_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "#         self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc1 = nn.Linear(500, 320)\n",
    "        self.fc2 = nn.Linear(320, 50)\n",
    "        self.fc3 = nn.Linear(50, NUM_QUBITS)\n",
    "        self.qc = TorchCircuit.apply\n",
    "        self.qcsim = nn.Linear(NUM_QUBITS, 1)\n",
    "        self.fc4 = nn.Linear(1, 3) # TODO: is this correct ???\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "#         x = x.view(-1, 320)\n",
    "#         x = x.view(-1, 500)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        x = np.pi*torch.tanh(x)\n",
    "        \n",
    "        MODE = 'QC'\n",
    "        \n",
    "        if MODE == 'QC':\n",
    "            x = self.qc(x[0])\n",
    "        else:\n",
    "            x = self.qcsim(x)\n",
    "            \n",
    "        x = torch.sigmoid(x)\n",
    "        x = torch.cat((x, 1-x), -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        pred = self.forward(x)\n",
    "        ans = torch.argmax(pred[0]).item()\n",
    "        \n",
    "        return torch.tensor(ans)\n",
    "    \n",
    "network = Net()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, optimizer, train_loader, val_loader):\n",
    "    epochs = 20\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader\n",
    "    }\n",
    "    \n",
    "    loss_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    acc_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': test_loader\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "        total_acc = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                network.train(True)\n",
    "            else:\n",
    "                network.train(False)\n",
    "\n",
    "            correct = 0\n",
    "            number = 0\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(loaders[phase]):\n",
    "                optimizer.zero_grad()\n",
    "                output = network(data)\n",
    "\n",
    "                loss = loss_func(output, target)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                total_loss[phase].append(loss.item())\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                number += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "            loss_results[phase].append(sum(total_loss[phase]) / len(total_loss[phase]))\n",
    "            acc_results[phase].append(100 * correct / number)\n",
    "\n",
    "        progress = 100.0 * (epoch + 1) / epochs\n",
    "\n",
    "        print(f'Training [{int(progress)}%]\\tTraining loss {loss_results[\"train\"][-1]}\\tValidation loss {loss_results[\"val\"][-1]}\\tTraining acc {acc_results[\"train\"][-1]}\\tValidation acc {acc_results[\"val\"][-1]}')\n",
    "        \n",
    "    return {\n",
    "        'train_loss': loss_results[\"train\"][-1],\n",
    "        'val_loss': loss_results[\"val\"][-1],\n",
    "        'train_acc': acc_results[\"train\"][-1],\n",
    "        'val_acc': acc_results[\"val\"][-1]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 2.0179812175532184\tValidation loss 2.0192825162410735\tTraining acc 28.125\tValidation acc 22.5\n",
      "Training [10%]\tTraining loss 2.020689264933268\tValidation loss 2.0190744960308074\tTraining acc 23.958333333333332\tValidation acc 22.0\n",
      "Training [15%]\tTraining loss 2.019366652518511\tValidation loss 2.0190693020820616\tTraining acc 24.583333333333332\tValidation acc 29.0\n",
      "Training [20%]\tTraining loss 2.019088848431905\tValidation loss 2.01941819190979\tTraining acc 28.541666666666668\tValidation acc 18.0\n",
      "Training [25%]\tTraining loss 2.019115245590607\tValidation loss 2.0191507387161254\tTraining acc 26.25\tValidation acc 24.0\n",
      "Training [30%]\tTraining loss 2.0190174003442127\tValidation loss 2.019172269105911\tTraining acc 27.708333333333332\tValidation acc 25.0\n",
      "Training [35%]\tTraining loss 2.0191368095576765\tValidation loss 2.0191981995105746\tTraining acc 23.75\tValidation acc 25.0\n",
      "Training [40%]\tTraining loss 2.0192738590141137\tValidation loss 2.01917334318161\tTraining acc 25.208333333333332\tValidation acc 25.5\n",
      "Training [45%]\tTraining loss 2.019708019743363\tValidation loss 2.019232279062271\tTraining acc 25.0\tValidation acc 22.0\n",
      "Training [50%]\tTraining loss 2.019394792119662\tValidation loss 2.0192088973522186\tTraining acc 26.041666666666668\tValidation acc 26.5\n",
      "Training [55%]\tTraining loss 2.0195554648836453\tValidation loss 2.01917277097702\tTraining acc 25.833333333333332\tValidation acc 22.0\n",
      "Training [60%]\tTraining loss 2.0196595544616383\tValidation loss 2.019276490211487\tTraining acc 23.333333333333332\tValidation acc 21.0\n",
      "Training [65%]\tTraining loss 2.018887360394001\tValidation loss 2.019090111255646\tTraining acc 26.25\tValidation acc 29.0\n",
      "Training [70%]\tTraining loss 2.0196081486841044\tValidation loss 2.019124311208725\tTraining acc 25.416666666666668\tValidation acc 27.0\n",
      "Training [75%]\tTraining loss 2.0206024564802645\tValidation loss 2.019072961807251\tTraining acc 25.416666666666668\tValidation acc 30.0\n",
      "Training [80%]\tTraining loss 2.0192149316271144\tValidation loss 2.019230405092239\tTraining acc 25.208333333333332\tValidation acc 21.0\n",
      "Training [85%]\tTraining loss 2.018657546987136\tValidation loss 2.019087369441986\tTraining acc 25.416666666666668\tValidation acc 26.5\n",
      "Training [90%]\tTraining loss 2.0201911740005016\tValidation loss 2.0192819666862487\tTraining acc 26.666666666666668\tValidation acc 20.5\n",
      "Training [95%]\tTraining loss 2.0192701352139313\tValidation loss 2.019363260269165\tTraining acc 23.958333333333332\tValidation acc 25.5\n",
      "Training [100%]\tTraining loss 2.0197276644408704\tValidation loss 2.0190661585330965\tTraining acc 26.041666666666668\tValidation acc 29.0\n",
      "{'train_loss': 2.0197276644408704, 'val_loss': 2.0190661585330965, 'train_acc': 26.041666666666668, 'val_acc': 29.0}\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 2.020495211084684\tValidation loss 2.0160042667388915\tTraining acc 25.625\tValidation acc 31.5\n",
      "Training [10%]\tTraining loss 2.018998145063718\tValidation loss 2.012628458738327\tTraining acc 26.25\tValidation acc 26.0\n",
      "Training [15%]\tTraining loss 2.0154728561639788\tValidation loss 2.0162916201353074\tTraining acc 26.458333333333332\tValidation acc 27.0\n",
      "Training [20%]\tTraining loss 2.020975307871898\tValidation loss 2.0193393385410308\tTraining acc 23.75\tValidation acc 22.5\n",
      "Training [25%]\tTraining loss 2.0187992841005324\tValidation loss 2.0192229652404787\tTraining acc 23.958333333333332\tValidation acc 24.5\n",
      "Training [30%]\tTraining loss 2.0201980620622635\tValidation loss 2.019269354343414\tTraining acc 24.583333333333332\tValidation acc 25.0\n",
      "Training [35%]\tTraining loss 2.020947768787543\tValidation loss 2.019214230775833\tTraining acc 25.0\tValidation acc 21.5\n",
      "Training [40%]\tTraining loss 2.019469028711319\tValidation loss 2.019227888584137\tTraining acc 26.666666666666668\tValidation acc 21.0\n",
      "Training [45%]\tTraining loss 2.0190238701800505\tValidation loss 2.0191777002811433\tTraining acc 26.875\tValidation acc 29.0\n",
      "Training [50%]\tTraining loss 2.019604912648598\tValidation loss 2.0192373633384704\tTraining acc 27.083333333333332\tValidation acc 19.5\n",
      "Training [55%]\tTraining loss 2.0194687828421594\tValidation loss 2.019361367225647\tTraining acc 26.25\tValidation acc 21.0\n",
      "Training [60%]\tTraining loss 2.019481477389733\tValidation loss 2.0191587054729463\tTraining acc 25.0\tValidation acc 26.5\n",
      "Training [65%]\tTraining loss 2.019363216062387\tValidation loss 2.0192735850811006\tTraining acc 25.833333333333332\tValidation acc 23.0\n",
      "Training [70%]\tTraining loss 2.0196576135853928\tValidation loss 2.019260207414627\tTraining acc 22.083333333333332\tValidation acc 22.5\n",
      "Training [75%]\tTraining loss 2.0192757594088713\tValidation loss 2.019378430843353\tTraining acc 23.125\tValidation acc 21.5\n",
      "Training [80%]\tTraining loss 2.0194143831729887\tValidation loss 2.019285262823105\tTraining acc 24.583333333333332\tValidation acc 20.5\n",
      "Training [85%]\tTraining loss 2.018195357422034\tValidation loss 2.019308936595917\tTraining acc 28.125\tValidation acc 23.5\n",
      "Training [90%]\tTraining loss 2.0199218578636646\tValidation loss 2.019205641746521\tTraining acc 24.375\tValidation acc 24.5\n",
      "Training [95%]\tTraining loss 2.0193611212074756\tValidation loss 2.0192928600311277\tTraining acc 23.333333333333332\tValidation acc 24.0\n",
      "Training [100%]\tTraining loss 2.0193367960552373\tValidation loss 2.0190845263004302\tTraining acc 24.375\tValidation acc 28.5\n",
      "{'train_loss': 2.0193367960552373, 'val_loss': 2.0190845263004302, 'train_acc': 24.375, 'val_acc': 28.5}\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 2.0198668065170446\tValidation loss 2.0192508482933045\tTraining acc 26.041666666666668\tValidation acc 26.0\n",
      "Training [10%]\tTraining loss 2.019313149402539\tValidation loss 2.019326338768005\tTraining acc 21.458333333333332\tValidation acc 20.0\n",
      "Training [15%]\tTraining loss 2.018673355380694\tValidation loss 2.019151040315628\tTraining acc 26.458333333333332\tValidation acc 26.5\n",
      "Training [20%]\tTraining loss 2.0194173522293566\tValidation loss 2.0198780125379563\tTraining acc 24.583333333333332\tValidation acc 22.0\n",
      "Training [25%]\tTraining loss 2.0186289908985295\tValidation loss 2.0190748143196107\tTraining acc 25.208333333333332\tValidation acc 24.0\n",
      "Training [30%]\tTraining loss 2.0198323306938013\tValidation loss 2.0190068221092226\tTraining acc 22.291666666666668\tValidation acc 31.5\n",
      "Training [35%]\tTraining loss 2.019509064654509\tValidation loss 2.0189498805999757\tTraining acc 25.416666666666668\tValidation acc 31.0\n",
      "Training [40%]\tTraining loss 2.0186897369722527\tValidation loss 2.0192296826839446\tTraining acc 26.875\tValidation acc 24.5\n",
      "Training [45%]\tTraining loss 2.018652933835983\tValidation loss 2.0192064225673674\tTraining acc 28.125\tValidation acc 25.0\n",
      "Training [50%]\tTraining loss 2.0196880800028643\tValidation loss 2.0191713523864747\tTraining acc 24.375\tValidation acc 25.5\n",
      "Training [55%]\tTraining loss 2.0190896493693193\tValidation loss 2.019336109161377\tTraining acc 23.958333333333332\tValidation acc 19.5\n",
      "Training [60%]\tTraining loss 2.018889686961969\tValidation loss 2.0193256437778473\tTraining acc 23.333333333333332\tValidation acc 19.0\n",
      "Training [65%]\tTraining loss 2.0190521905819576\tValidation loss 2.019109888076782\tTraining acc 25.208333333333332\tValidation acc 25.0\n",
      "Training [70%]\tTraining loss 2.0190275805691877\tValidation loss 2.019094250202179\tTraining acc 27.083333333333332\tValidation acc 26.5\n",
      "Training [75%]\tTraining loss 2.019466798255841\tValidation loss 2.01922908782959\tTraining acc 22.5\tValidation acc 22.5\n",
      "Training [80%]\tTraining loss 2.019804319490989\tValidation loss 2.0191187608242034\tTraining acc 22.5\tValidation acc 28.5\n",
      "Training [85%]\tTraining loss 2.0189928891758124\tValidation loss 2.019236807823181\tTraining acc 24.583333333333332\tValidation acc 22.0\n",
      "Training [90%]\tTraining loss 2.0202613972127437\tValidation loss 2.0193324863910673\tTraining acc 22.5\tValidation acc 19.5\n",
      "Training [95%]\tTraining loss 2.019540996104479\tValidation loss 2.0190858948230743\tTraining acc 24.583333333333332\tValidation acc 25.0\n",
      "Training [100%]\tTraining loss 2.018389821300904\tValidation loss 2.0192614471912385\tTraining acc 27.083333333333332\tValidation acc 25.5\n",
      "{'train_loss': 2.018389821300904, 'val_loss': 2.0192614471912385, 'train_acc': 27.083333333333332, 'val_acc': 25.5}\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [5%]\tTraining loss 2.021476316700379\tValidation loss 2.0192023092508316\tTraining acc 20.416666666666668\tValidation acc 25.5\n",
      "Training [10%]\tTraining loss 2.019138575345278\tValidation loss 2.0192737221717834\tTraining acc 24.375\tValidation acc 22.0\n",
      "Training [15%]\tTraining loss 2.0195483202735582\tValidation loss 2.019123946428299\tTraining acc 26.875\tValidation acc 28.5\n",
      "Training [20%]\tTraining loss 2.0192100023229917\tValidation loss 2.0193212485313414\tTraining acc 25.0\tValidation acc 27.5\n",
      "Training [25%]\tTraining loss 2.0192062365512053\tValidation loss 2.0191677701473236\tTraining acc 25.0\tValidation acc 29.0\n",
      "Training [30%]\tTraining loss 2.018891549358765\tValidation loss 2.019249805212021\tTraining acc 23.958333333333332\tValidation acc 30.5\n",
      "Training [35%]\tTraining loss 2.019532131652037\tValidation loss 2.019102258682251\tTraining acc 22.5\tValidation acc 27.5\n",
      "Training [40%]\tTraining loss 2.019206568102042\tValidation loss 2.019026015996933\tTraining acc 22.5\tValidation acc 31.0\n",
      "Training [45%]\tTraining loss 2.018649186690648\tValidation loss 2.0191608667373657\tTraining acc 23.75\tValidation acc 28.0\n",
      "Training [50%]\tTraining loss 2.018611504137516\tValidation loss 2.0191759026050566\tTraining acc 26.25\tValidation acc 24.5\n",
      "Training [55%]\tTraining loss 2.0195959207912284\tValidation loss 2.019147888422012\tTraining acc 24.166666666666668\tValidation acc 23.5\n",
      "Training [60%]\tTraining loss 2.01927965482076\tValidation loss 2.0192254865169526\tTraining acc 24.166666666666668\tValidation acc 26.0\n",
      "Training [65%]\tTraining loss 2.0192505203187467\tValidation loss 2.0192399442195894\tTraining acc 24.166666666666668\tValidation acc 20.0\n",
      "Training [70%]\tTraining loss 2.01726716508468\tValidation loss 2.0191997611522674\tTraining acc 26.875\tValidation acc 25.0\n",
      "Training [75%]\tTraining loss 2.019682241231203\tValidation loss 2.0191365480422974\tTraining acc 23.125\tValidation acc 28.5\n",
      "Training [80%]\tTraining loss 2.018983530004819\tValidation loss 2.019260506629944\tTraining acc 26.458333333333332\tValidation acc 22.5\n",
      "Training [85%]\tTraining loss 2.0197068206965922\tValidation loss 2.0190781021118163\tTraining acc 26.458333333333332\tValidation acc 25.5\n",
      "Training [90%]\tTraining loss 2.018432203431924\tValidation loss 2.0191351985931396\tTraining acc 27.708333333333332\tValidation acc 29.0\n",
      "Training [95%]\tTraining loss 2.0174926839768887\tValidation loss 2.0192097175121306\tTraining acc 28.125\tValidation acc 26.0\n",
      "Training [100%]\tTraining loss 2.0199138614038628\tValidation loss 2.0192859673500063\tTraining acc 21.041666666666668\tValidation acc 23.0\n",
      "{'train_loss': 2.0199138614038628, 'val_loss': 2.0192859673500063, 'train_acc': 21.041666666666668, 'val_acc': 23.0}\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 2.020125171293815\tValidation loss 2.0194677823781966\tTraining acc 25.625\tValidation acc 25.0\n",
      "Training [10%]\tTraining loss 2.0197312931219735\tValidation loss 2.0191821587085723\tTraining acc 23.958333333333332\tValidation acc 26.0\n",
      "Training [15%]\tTraining loss 2.019677079220613\tValidation loss 2.0193994081020357\tTraining acc 26.25\tValidation acc 25.0\n",
      "Training [20%]\tTraining loss 2.0196670033037663\tValidation loss 2.01939721763134\tTraining acc 27.708333333333332\tValidation acc 25.0\n",
      "Training [25%]\tTraining loss 2.0192954927682876\tValidation loss 2.019297467470169\tTraining acc 27.083333333333332\tValidation acc 25.0\n",
      "Training [30%]\tTraining loss 2.0191747640570004\tValidation loss 2.0194744312763215\tTraining acc 27.083333333333332\tValidation acc 25.0\n",
      "Training [35%]\tTraining loss 2.0190701318283875\tValidation loss 2.01935633122921\tTraining acc 27.083333333333332\tValidation acc 25.0\n",
      "Training [40%]\tTraining loss 2.019279408454895\tValidation loss 2.019538603425026\tTraining acc 26.875\tValidation acc 25.0\n",
      "Training [45%]\tTraining loss 2.0193028964102266\tValidation loss 2.0194576561450956\tTraining acc 26.875\tValidation acc 25.0\n",
      "Training [50%]\tTraining loss 2.0193673128883045\tValidation loss 2.019569243788719\tTraining acc 27.083333333333332\tValidation acc 25.0\n",
      "Training [55%]\tTraining loss 2.0190442780653637\tValidation loss 2.0196812081336977\tTraining acc 26.875\tValidation acc 25.0\n",
      "Training [60%]\tTraining loss 2.019081011662881\tValidation loss 2.0195528078079223\tTraining acc 27.083333333333332\tValidation acc 25.0\n",
      "Training [65%]\tTraining loss 2.0190286425252757\tValidation loss 2.019601619243622\tTraining acc 27.083333333333332\tValidation acc 25.0\n",
      "Training [70%]\tTraining loss 2.0191727886597315\tValidation loss 2.0195126098394396\tTraining acc 27.083333333333332\tValidation acc 25.0\n",
      "Training [75%]\tTraining loss 2.0191009474297363\tValidation loss 2.0194625985622405\tTraining acc 27.083333333333332\tValidation acc 25.0\n",
      "Training [80%]\tTraining loss 2.0188623428344727\tValidation loss 2.019743949174881\tTraining acc 27.083333333333332\tValidation acc 25.0\n",
      "Training [85%]\tTraining loss 2.0192942914863425\tValidation loss 2.0194999039173127\tTraining acc 27.083333333333332\tValidation acc 25.0\n",
      "Training [90%]\tTraining loss 2.018817118555307\tValidation loss 2.0196881985664366\tTraining acc 27.083333333333332\tValidation acc 25.0\n",
      "Training [95%]\tTraining loss 2.0190247582892575\tValidation loss 2.01952350795269\tTraining acc 27.083333333333332\tValidation acc 25.0\n",
      "Training [100%]\tTraining loss 2.0191173193355403\tValidation loss 2.0196011501550672\tTraining acc 27.083333333333332\tValidation acc 25.0\n",
      "{'train_loss': 2.0191173193355403, 'val_loss': 2.0196011501550672, 'train_acc': 27.083333333333332, 'val_acc': 25.0}\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "FINAL RESULTS\n",
      "--------------------------------\n",
      "{0: {'train_loss': 2.0197276644408704, 'val_loss': 2.0190661585330965, 'train_acc': 26.041666666666668, 'val_acc': 29.0}, 1: {'train_loss': 2.0193367960552373, 'val_loss': 2.0190845263004302, 'train_acc': 24.375, 'val_acc': 28.5}, 2: {'train_loss': 2.018389821300904, 'val_loss': 2.0192614471912385, 'train_acc': 27.083333333333332, 'val_acc': 25.5}, 3: {'train_loss': 2.0199138614038628, 'val_loss': 2.0192859673500063, 'train_acc': 21.041666666666668, 'val_acc': 23.0}, 4: {'train_loss': 2.0191173193355403, 'val_loss': 2.0196011501550672, 'train_acc': 27.083333333333332, 'val_acc': 25.0}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "\n",
    "dataset = ConcatDataset([X_train, X_test])\n",
    "\n",
    "k_folds = 5\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "results = {}\n",
    "\n",
    "print('--------------------------------')\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "  \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=1, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=1, sampler=test_subsampler)\n",
    "\n",
    "    network = Net()\n",
    "    network.apply(reset_weights)\n",
    "    \n",
    "    optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "    \n",
    "    result = train_network(network, optimizer, trainloader, testloader)\n",
    "    \n",
    "    results[fold] = result\n",
    "    \n",
    "    print(result)\n",
    "    print('--------------------------------')\n",
    "\n",
    "print('--------------------------------')\n",
    "print('FINAL RESULTS')\n",
    "print('--------------------------------')\n",
    "print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 25\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# loss_results = {\n",
    "#     'train': [],\n",
    "#     'val': []\n",
    "# }\n",
    "\n",
    "# acc_results = {\n",
    "#     'train': [],\n",
    "#     'val': []\n",
    "# }\n",
    "\n",
    "# loaders = {\n",
    "#     'train': train_loader,\n",
    "#     'val': test_loader\n",
    "# }\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = {\n",
    "#         'train': [],\n",
    "#         'val': []\n",
    "#     }\n",
    "    \n",
    "#     total_acc = {\n",
    "#         'train': [],\n",
    "#         'val': []\n",
    "#     }\n",
    "    \n",
    "    \n",
    "#     for phase in ['train', 'val']:\n",
    "#         if phase == 'train':\n",
    "#             network.train(True)\n",
    "#         else:\n",
    "#             network.train(False)\n",
    "        \n",
    "#         correct = 0\n",
    "#         number = 0\n",
    "        \n",
    "#         for batch_idx, (data, target) in enumerate(loaders[phase]):\n",
    "#             optimizer.zero_grad()\n",
    "#             output = network(data)\n",
    "            \n",
    "#             loss = loss_func(output, target)\n",
    "            \n",
    "#             if phase == 'train':\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             total_loss[phase].append(loss.item())\n",
    "\n",
    "#             _, predicted = torch.max(output.data, 1)\n",
    "#             number += target.size(0)\n",
    "#             correct += (predicted == target).sum().item()\n",
    "\n",
    "#         loss_results[phase].append(sum(total_loss[phase]) / len(total_loss[phase]))\n",
    "#         acc_results[phase].append(100 * correct / number)\n",
    "\n",
    "#     progress = 100.0 * (epoch + 1) / epochs\n",
    "    \n",
    "#     print(f'Training [{int(progress)}%]\\tTraining loss {loss_results[\"train\"][-1]}\\tValidation loss {loss_results[\"val\"][-1]}\\tTraining acc {acc_results[\"train\"][-1]}\\tValidation acc {acc_results[\"val\"][-1]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_results[\"train\"])\n",
    "# plt.title(f'Hybrid NN training convergence for {NUM_QUBITS}-qubits')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_results[\"val\"])\n",
    "# plt.title(f'Hybrid NN training convergence for {NUM_QUBITS}-qubits')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = 0\n",
    "# number = 0\n",
    "# for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#     number += 1\n",
    "#     output = network.predict(data).item()\n",
    "#     accuracy += (output == target[0].item())*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Performance on test data: {accuracy}/{number} = {accuracy/number*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_sample_shape = (8, 6)\n",
    "# count = 0\n",
    "# fig, axes = plt.subplots(nrows=n_sample_shape[0], ncols=n_sample_shape[1], figsize=(10, 2*n_sample_shape[0]))\n",
    "\n",
    "# classes_map = {\n",
    "#     0: 'T-shirt/top',\n",
    "#     1: 'Trouser',\n",
    "#     2: 'Pullover',\n",
    "#     3: 'Dress',\n",
    "#     4: 'Coat',\n",
    "#     5: 'Sandal',\n",
    "#     6: 'Shirt',\n",
    "#     7: 'Sneaker',\n",
    "#     8: 'Bag',\n",
    "#     9: 'Ankle boot'\n",
    "# }\n",
    "\n",
    "# network.eval()\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#         if count == n_sample_shape[0]*n_sample_shape[1]:\n",
    "#             break\n",
    "        \n",
    "#         prediction = network.predict(data).item()\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].imshow(data[0].numpy().squeeze(), cmap='gray')\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_xticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_yticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_title(classes_map[prediction])\n",
    "        \n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
