{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: qiskit in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.23.2)\n",
      "Requirement already satisfied: qiskit-terra==0.16.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.16.1)\n",
      "Requirement already satisfied: qiskit-aer==0.7.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.7.2)\n",
      "Requirement already satisfied: qiskit-ibmq-provider==0.11.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.11.1)\n",
      "Requirement already satisfied: qiskit-ignis==0.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.5.1)\n",
      "Requirement already satisfied: qiskit-aqua==0.8.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.8.1)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (0.3.3)\n",
      "Requirement already satisfied: retworkx>=0.5.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (0.7.2)\n",
      "Requirement already satisfied: sympy>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.8.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.2.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.19.2)\n",
      "Requirement already satisfied: scipy>=1.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.5.2)\n",
      "Requirement already satisfied: python-constraint>=1.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: psutil>=5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (5.8.0)\n",
      "Requirement already satisfied: ply>=3.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.11)\n",
      "Requirement already satisfied: fastjsonschema>=2.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.14.5)\n",
      "Requirement already satisfied: cython>=0.27.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (0.29.21)\n",
      "Requirement already satisfied: pybind11>=2.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (2.6.1)\n",
      "Requirement already satisfied: requests>=2.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (2.24.0)\n",
      "Requirement already satisfied: requests-ntlm>=1.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.1.0)\n",
      "Requirement already satisfied: websockets>=8 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (8.1)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.25.10)\n",
      "Requirement already satisfied: nest-asyncio!=1.1.0,>=1.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ignis==0.5.1->qiskit) (50.3.0)\n",
      "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ignis==0.5.1->qiskit) (0.23.2)\n",
      "Requirement already satisfied: quandl in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (3.6.0)\n",
      "Requirement already satisfied: pandas in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.1.3)\n",
      "Requirement already satisfied: yfinance in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.1.55)\n",
      "Requirement already satisfied: dlx in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.0.4)\n",
      "Requirement already satisfied: h5py in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.10.0)\n",
      "Requirement already satisfied: docplex in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.19.202)\n",
      "Requirement already satisfied: fastdtw in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from sympy>=1.3->qiskit-terra==0.16.1->qiskit) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from python-dateutil>=2.8.0->qiskit-terra==0.16.1->qiskit) (1.15.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (20.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (0.17.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from networkx>=2.2->qiskit-terra==0.16.1->qiskit) (4.4.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2020.6.20)\n",
      "Requirement already satisfied: ntlm-auth>=1.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.5.0)\n",
      "Requirement already satisfied: cryptography>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (3.3.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.17->qiskit-ignis==0.5.1->qiskit) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.17->qiskit-ignis==0.5.1->qiskit) (2.1.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (8.6.0)\n",
      "Requirement already satisfied: inflection>=0.3.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from pandas->qiskit-aqua==0.8.1->qiskit) (2020.1)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (0.0.9)\n",
      "Requirement already satisfied: lxml>=4.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (4.6.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cffi>=1.12 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.14.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (2.20)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pylatexenc in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (2.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torchvision in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.8.2)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: torch==1.7.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch==1.7.1->torchvision) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (4.56.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for itertools\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install qiskit\n",
    "!{sys.executable} -m pip install pylatexenc\n",
    "!{sys.executable} -m pip install torchvision\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import execute\n",
    "from qiskit.circuit import Parameter,ControlledGate\n",
    "from qiskit import Aer\n",
    "import qiskit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "\n",
    "NUM_QUBITS = 2\n",
    "NUM_SHOTS = 5000\n",
    "SHIFT = np.pi/4\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "SIMULATOR = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '01', '10', '11']\n"
     ]
    }
   ],
   "source": [
    "# create list of all possible outputs of quantum circuit (2**NUM_QUBITS possible)\n",
    "import itertools\n",
    "\n",
    "def create_QC_OUTPUTS():\n",
    "    measurements = list(itertools.product([0, 1], repeat=NUM_QUBITS))\n",
    "    return [''.join([str(bit) for bit in measurement]) for measurement in measurements]\n",
    "\n",
    "QC_OUTPUTS = create_QC_OUTPUTS()\n",
    "print(QC_OUTPUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contruct QuantumCircuit QFT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumCircuit:\n",
    "    def __init__(self, n_qubits, backend, shots):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.circuit = qiskit.QuantumCircuit(n_qubits)\n",
    "        self.thetas = { k: Parameter('Theta'+str(k)) for k in range(self.n_qubits) }\n",
    "        \n",
    "        all_qubits = [i for i in range(n_qubits)]\n",
    "        \n",
    "        self.circuit.h(all_qubits)\n",
    "        self.circuit.barrier()\n",
    "        for k in range(n_qubits):\n",
    "            self.circuit.ry(self.thetas[k], k)\n",
    "            \n",
    "        self.circuit.measure_all()\n",
    "        \n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "        \n",
    "    def N_qubits_expectation_Z(self, counts, shots, n_qubits):\n",
    "        expects = np.zeros(len(QC_OUTPUTS))\n",
    "        for k in range(len(QC_OUTPUTS)):\n",
    "            key = QC_OUTPUTS[k]\n",
    "            perc = counts.get(key, 0) / shots\n",
    "            expects[k] = perc\n",
    "        return expects\n",
    "    \n",
    "    def run(self, i):\n",
    "        params = i\n",
    "        \n",
    "        job_sim = execute(self.circuit, \n",
    "                          self.backend, \n",
    "                          shots = self.shots, \n",
    "                          parameter_binds = [{ self.thetas[k]: params[k].item() for k in range(self.n_qubits) }])\n",
    "        \n",
    "        result_sim = job_sim.result()\n",
    "        counts = result_sim.get_counts(self.circuit)\n",
    "        \n",
    "        return self.N_qubits_expectation_Z(counts, self.shots, self.n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value for rotation [pi/4]: [0.0222 0.1218 0.1268 0.7292]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 478.079x204.68 with 1 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, NUM_SHOTS)\n",
    "print(f'Expected value for rotation [pi/4]: {circuit.run(torch.Tensor([np.pi/4] * len(QC_OUTPUTS)))}')\n",
    "circuit.circuit.draw(output='mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCircuit(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        if not hasattr(ctx, 'QuantumCircuit'):\n",
    "            ctx.QuantumCircuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, shots=NUM_SHOTS)\n",
    "        \n",
    "        exp_value = ctx.QuantumCircuit.run(i)\n",
    "        \n",
    "        result = torch.Tensor([exp_value])\n",
    "        \n",
    "        ctx.save_for_backward(result, i)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        forward_tensor,i = ctx.saved_tensors\n",
    "        \n",
    "        input_numbers = i\n",
    "        gradients = torch.Tensor()\n",
    "        \n",
    "        for k in range(NUM_QUBITS):\n",
    "            shift_right = input_numbers.detach().clone()\n",
    "            shift_right[k] = shift_right[k] + SHIFT\n",
    "            \n",
    "            shift_left = input_numbers.detach().clone()\n",
    "            shift_left[k] = shift_left[k] - SHIFT\n",
    "            \n",
    "            expectation_right = ctx.QuantumCircuit.run(shift_right)\n",
    "            expectation_left = ctx.QuantumCircuit.run(shift_left)\n",
    "            \n",
    "            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])\n",
    "            gradient = gradient / torch.norm(gradient)\n",
    "            \n",
    "            gradients = torch.cat((gradients, gradient.float()))\n",
    "            \n",
    "        result = torch.Tensor(gradients)\n",
    "        \n",
    "        return (result.float() * grad_output.float()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 after quantum layer: tensor([[0.0210, 0.1270, 0.1272, 0.7248]], grad_fn=<TorchCircuitBackward>)\n",
      "x.grad = tensor([ 0.2129, -0.0054])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "qc = TorchCircuit.apply\n",
    "\n",
    "y1 = qc(x)\n",
    "print(f'y1 after quantum layer: {y1}')\n",
    "y1 = nn.Linear(2**NUM_QUBITS, 1)(y1.float())\n",
    "\n",
    "y1.backward()\n",
    "print(f'x.grad = {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:13<00:00,  7.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x144f0c040>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qc = TorchCircuit.apply\n",
    "\n",
    "def cost(x):\n",
    "    target = -1\n",
    "    expval = qc(x)[0]\n",
    "    \n",
    "    val = sum([(i + 1) * expval[i] for i in range(2**NUM_QUBITS)]) / 2**NUM_QUBITS\n",
    "    \n",
    "    return torch.abs(target - val) ** 2, expval\n",
    "\n",
    "\n",
    "x = torch.tensor([-np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "opt = optim.Adam([x], lr = 0.1)\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "loss_list = []\n",
    "expval_list = []\n",
    "\n",
    "for i in tqdm(range(num_epoch)):\n",
    "    opt.zero_grad()\n",
    "    loss, expval = cost(x)\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()\n",
    "    loss_list.append(loss.item())\n",
    "    expval_list.append(expval)\n",
    "    \n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "n_samples = 300\n",
    "\n",
    "X_train = datasets.CIFAR10(root='./data', \n",
    "                                train=True,\n",
    "                                download=True,\n",
    "                                transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "\n",
    "idx = np.append(np.where(np.array(X_train.targets) == 0)[0][:n_samples], \n",
    "                np.where(np.array(X_train.targets) == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 3)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 4)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 5)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 6)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 7)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 8)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 9)[0][:n_samples])\n",
    "\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = np.array(X_train.targets)[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "n_samples = 50\n",
    "\n",
    "X_test = datasets.CIFAR10(root='./data', \n",
    "                               train=False,\n",
    "                               download=True,\n",
    "                               transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "\n",
    "idx = np.append(np.where(np.array(X_test.targets) == 0)[0][:n_samples], \n",
    "                np.where(np.array(X_test.targets) == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 3)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 4)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 5)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 6)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 7)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 8)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 9)[0][:n_samples])\n",
    "\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = np.array(X_test.targets)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(X_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, NUM_QUBITS)\n",
    "        self.qc = TorchCircuit.apply\n",
    "        \n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "#         self.conv2_drop = nn.Dropout2d()\n",
    "# #         self.fc1 = nn.Linear(320, 50)\n",
    "#         self.fc1 = nn.Linear(500, 320)\n",
    "#         self.fc2 = nn.Linear(320, 50)\n",
    "#         self.fc3 = nn.Linear(50, NUM_QUBITS)\n",
    "#         self.qc = TorchCircuit.apply\n",
    "#         self.qcsim = nn.Linear(NUM_QUBITS, 1)\n",
    "#         self.fc4 = nn.Linear(1, 3) # TODO: is this correct ???\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        x = np.pi*torch.tanh(x)\n",
    "        x = self.qc(x[0])\n",
    "        \n",
    "        x = torch.sigmoid(x)\n",
    "        x = torch.cat((x, 1-x), -1)\n",
    "        \n",
    "#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "# #         x = x.view(-1, 320)\n",
    "# #         x = x.view(-1, 500)\n",
    "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.fc3(x)\n",
    "#         x = np.pi*torch.tanh(x)\n",
    "        \n",
    "#         MODE = 'QC'\n",
    "        \n",
    "#         if MODE == 'QC':\n",
    "#             x = self.qc(x[0])\n",
    "#         else:\n",
    "#             x = self.qcsim(x)\n",
    "            \n",
    "#         x = torch.sigmoid(x)\n",
    "#         x = torch.cat((x, 1-x), -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        pred = self.forward(x)\n",
    "        ans = torch.argmax(pred[0]).item()\n",
    "        \n",
    "        return torch.tensor(ans)\n",
    "    \n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, optimizer, train_loader, val_loader):\n",
    "    epochs = 20\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader\n",
    "    }\n",
    "    \n",
    "    loss_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    acc_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': test_loader\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "        total_acc = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                network.train(True)\n",
    "            else:\n",
    "                network.train(False)\n",
    "\n",
    "            correct = 0\n",
    "            number = 0\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(loaders[phase]):\n",
    "                optimizer.zero_grad()\n",
    "                output = network(data)\n",
    "\n",
    "                loss = loss_func(output, target)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                total_loss[phase].append(loss.item())\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                number += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "            loss_results[phase].append(sum(total_loss[phase]) / len(total_loss[phase]))\n",
    "            acc_results[phase].append(100 * correct / number)\n",
    "\n",
    "        progress = 100.0 * (epoch + 1) / epochs\n",
    "\n",
    "        print(f'Training [{int(progress)}%]\\tTraining loss {loss_results[\"train\"][-1]}\\tValidation loss {loss_results[\"val\"][-1]}\\tTraining acc {acc_results[\"train\"][-1]}\\tValidation acc {acc_results[\"val\"][-1]}')\n",
    "        \n",
    "    return {\n",
    "        'train_loss': loss_results[\"train\"][-1],\n",
    "        'val_loss': loss_results[\"val\"][-1],\n",
    "        'train_acc': acc_results[\"train\"][-1],\n",
    "        'val_acc': acc_results[\"val\"][-1]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 2.010647970012256\tValidation loss 1.983031308054924\tTraining acc 34.464285714285715\tValidation acc 47.0\n",
      "Training [10%]\tTraining loss 1.974025877990893\tValidation loss 1.9678063577413558\tTraining acc 50.892857142857146\tValidation acc 54.0\n",
      "Training [15%]\tTraining loss 1.9595973533179079\tValidation loss 1.9646981531381607\tTraining acc 58.214285714285715\tValidation acc 55.0\n",
      "Training [20%]\tTraining loss 1.9518643532480513\tValidation loss 1.953167013525963\tTraining acc 60.714285714285715\tValidation acc 61.0\n",
      "Training [25%]\tTraining loss 1.9437852848853383\tValidation loss 1.9487663722038269\tTraining acc 64.55357142857143\tValidation acc 62.0\n",
      "Training [30%]\tTraining loss 1.9392086537820952\tValidation loss 1.93960180580616\tTraining acc 65.71428571428571\tValidation acc 66.5\n",
      "Training [35%]\tTraining loss 1.9315417386591434\tValidation loss 1.9336231797933578\tTraining acc 69.01785714285714\tValidation acc 69.5\n",
      "Training [40%]\tTraining loss 1.9272965059748717\tValidation loss 1.9333370339870453\tTraining acc 70.98214285714286\tValidation acc 68.0\n",
      "Training [45%]\tTraining loss 1.924859286312546\tValidation loss 1.9291239559650422\tTraining acc 70.80357142857143\tValidation acc 68.5\n",
      "Training [50%]\tTraining loss 1.9184498340955802\tValidation loss 1.927511460185051\tTraining acc 74.28571428571429\tValidation acc 68.5\n",
      "Training [55%]\tTraining loss 1.9188797388757979\tValidation loss 1.9276491421461106\tTraining acc 72.94642857142857\tValidation acc 69.0\n",
      "Training [60%]\tTraining loss 1.9115593070430414\tValidation loss 1.9196469354629517\tTraining acc 76.78571428571429\tValidation acc 73.5\n",
      "Training [65%]\tTraining loss 1.909571730771235\tValidation loss 1.9210723459720611\tTraining acc 77.41071428571429\tValidation acc 71.0\n",
      "Training [70%]\tTraining loss 1.9034595931214946\tValidation loss 1.9164495605230332\tTraining acc 80.17857142857143\tValidation acc 74.5\n",
      "Training [75%]\tTraining loss 1.8989295716796601\tValidation loss 1.918048050403595\tTraining acc 82.67857142857143\tValidation acc 74.5\n",
      "Training [80%]\tTraining loss 1.8952007281993117\tValidation loss 1.915259992480278\tTraining acc 84.28571428571429\tValidation acc 75.0\n",
      "Training [85%]\tTraining loss 1.8941300421953202\tValidation loss 1.90415496468544\tTraining acc 84.10714285714286\tValidation acc 79.0\n",
      "Training [90%]\tTraining loss 1.890632691447224\tValidation loss 1.9036443293094636\tTraining acc 86.51785714285714\tValidation acc 80.0\n",
      "Training [95%]\tTraining loss 1.8863776737025806\tValidation loss 1.9001005184650421\tTraining acc 87.5\tValidation acc 80.5\n",
      "Training [100%]\tTraining loss 1.8821674409721578\tValidation loss 1.9070368802547455\tTraining acc 89.46428571428571\tValidation acc 79.0\n",
      "{'train_loss': 1.8821674409721578, 'val_loss': 1.9070368802547455, 'train_acc': 89.46428571428571, 'val_acc': 79.0}\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 2.0150598581348147\tValidation loss 1.9881780344247817\tTraining acc 30.803571428571427\tValidation acc 43.5\n",
      "Training [10%]\tTraining loss 1.9796552415405\tValidation loss 1.9839588260650636\tTraining acc 47.5\tValidation acc 46.5\n",
      "Training [15%]\tTraining loss 1.9662496511425291\tValidation loss 1.9691405683755874\tTraining acc 54.017857142857146\tValidation acc 52.0\n",
      "Training [20%]\tTraining loss 1.9548722863197328\tValidation loss 1.953467149734497\tTraining acc 59.375\tValidation acc 58.5\n",
      "Training [25%]\tTraining loss 1.9465030368949685\tValidation loss 1.9568615931272506\tTraining acc 63.75\tValidation acc 55.5\n",
      "Training [30%]\tTraining loss 1.9399974434503486\tValidation loss 1.965191369652748\tTraining acc 65.53571428571429\tValidation acc 53.5\n",
      "Training [35%]\tTraining loss 1.9355159550905228\tValidation loss 1.9413547575473786\tTraining acc 68.03571428571429\tValidation acc 66.5\n",
      "Training [40%]\tTraining loss 1.9322766811719962\tValidation loss 1.9372366428375245\tTraining acc 69.10714285714286\tValidation acc 66.0\n",
      "Training [45%]\tTraining loss 1.927864029045616\tValidation loss 1.9334333479404449\tTraining acc 70.35714285714286\tValidation acc 69.0\n",
      "Training [50%]\tTraining loss 1.9241081754011766\tValidation loss 1.9399789887666703\tTraining acc 72.05357142857143\tValidation acc 65.0\n",
      "Training [55%]\tTraining loss 1.9181058255689485\tValidation loss 1.9240104991197586\tTraining acc 74.82142857142857\tValidation acc 71.0\n",
      "Training [60%]\tTraining loss 1.9161558035228934\tValidation loss 1.914853811264038\tTraining acc 74.91071428571429\tValidation acc 74.5\n",
      "Training [65%]\tTraining loss 1.910476012102195\tValidation loss 1.9241581690311431\tTraining acc 77.67857142857143\tValidation acc 72.5\n",
      "Training [70%]\tTraining loss 1.907918914194618\tValidation loss 1.9143976348638534\tTraining acc 79.19642857142857\tValidation acc 77.0\n",
      "Training [75%]\tTraining loss 1.904902516837631\tValidation loss 1.9088737678527832\tTraining acc 80.26785714285714\tValidation acc 79.0\n",
      "Training [80%]\tTraining loss 1.9003353356250694\tValidation loss 1.901388396024704\tTraining acc 82.94642857142857\tValidation acc 81.5\n",
      "Training [85%]\tTraining loss 1.8953859421823707\tValidation loss 1.9003397285938264\tTraining acc 84.64285714285714\tValidation acc 82.5\n",
      "Training [90%]\tTraining loss 1.947786888905934\tValidation loss 2.0235711550712585\tTraining acc 60.44642857142857\tValidation acc 25.0\n",
      "Training [95%]\tTraining loss 2.0255709227706706\tValidation loss 2.0236165034770965\tTraining acc 23.928571428571427\tValidation acc 25.0\n",
      "Training [100%]\tTraining loss 2.0256188534200192\tValidation loss 2.023657767176628\tTraining acc 23.928571428571427\tValidation acc 25.0\n",
      "{'train_loss': 2.0256188534200192, 'val_loss': 2.023657767176628, 'train_acc': 23.928571428571427, 'val_acc': 25.0}\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 2.016695582653795\tValidation loss 2.0025422197580336\tTraining acc 30.625\tValidation acc 40.5\n",
      "Training [10%]\tTraining loss 1.9903440771358354\tValidation loss 1.9831885814666748\tTraining acc 42.410714285714285\tValidation acc 45.0\n",
      "Training [15%]\tTraining loss 1.981016929447651\tValidation loss 1.976249986886978\tTraining acc 46.607142857142854\tValidation acc 47.5\n",
      "Training [20%]\tTraining loss 1.9635753874267852\tValidation loss 1.9832054662704468\tTraining acc 55.892857142857146\tValidation acc 47.5\n",
      "Training [25%]\tTraining loss 1.950712468049356\tValidation loss 1.9469787323474883\tTraining acc 62.05357142857143\tValidation acc 60.0\n",
      "Training [30%]\tTraining loss 1.9431992231735162\tValidation loss 1.9436709374189376\tTraining acc 63.392857142857146\tValidation acc 64.5\n",
      "Training [35%]\tTraining loss 1.9383939354547433\tValidation loss 1.9417646592855453\tTraining acc 65.71428571428571\tValidation acc 65.5\n",
      "Training [40%]\tTraining loss 1.931017135402986\tValidation loss 1.948593990802765\tTraining acc 70.08928571428571\tValidation acc 59.5\n",
      "Training [45%]\tTraining loss 1.9289529519421713\tValidation loss 1.9332785534858703\tTraining acc 69.64285714285714\tValidation acc 67.5\n",
      "Training [50%]\tTraining loss 1.9250039729688848\tValidation loss 1.930515605211258\tTraining acc 72.05357142857143\tValidation acc 70.0\n",
      "Training [55%]\tTraining loss 1.9190462771270955\tValidation loss 1.9258467841148377\tTraining acc 74.73214285714286\tValidation acc 71.5\n",
      "Training [60%]\tTraining loss 1.9179013845111643\tValidation loss 1.9286290341615677\tTraining acc 74.28571428571429\tValidation acc 70.5\n",
      "Training [65%]\tTraining loss 1.9125996194779873\tValidation loss 1.9213537847995759\tTraining acc 76.69642857142857\tValidation acc 73.0\n",
      "Training [70%]\tTraining loss 1.9082704231142997\tValidation loss 1.9235033339262009\tTraining acc 78.48214285714286\tValidation acc 71.0\n",
      "Training [75%]\tTraining loss 1.907815342183624\tValidation loss 1.917162405848503\tTraining acc 78.75\tValidation acc 74.5\n",
      "Training [80%]\tTraining loss 1.921192442625761\tValidation loss 2.0235646069049835\tTraining acc 72.32142857142857\tValidation acc 25.0\n",
      "Training [85%]\tTraining loss 2.0228608942457607\tValidation loss 2.023530187010765\tTraining acc 25.267857142857142\tValidation acc 25.0\n",
      "Training [90%]\tTraining loss 2.0228200848613467\tValidation loss 2.023654875159264\tTraining acc 25.267857142857142\tValidation acc 25.0\n",
      "Training [95%]\tTraining loss 2.0228005969098635\tValidation loss 2.0236624282598497\tTraining acc 25.267857142857142\tValidation acc 25.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [100%]\tTraining loss 2.0228484766823907\tValidation loss 2.0236448496580124\tTraining acc 25.267857142857142\tValidation acc 25.0\n",
      "{'train_loss': 2.0228484766823907, 'val_loss': 2.0236448496580124, 'train_acc': 25.267857142857142, 'val_acc': 25.0}\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 2.0121203336332525\tValidation loss 1.9860899239778518\tTraining acc 29.017857142857142\tValidation acc 48.0\n",
      "Training [10%]\tTraining loss 1.9802685144756522\tValidation loss 1.968844201564789\tTraining acc 47.589285714285715\tValidation acc 54.0\n",
      "Training [15%]\tTraining loss 1.9654256901570728\tValidation loss 1.9558082985877991\tTraining acc 54.107142857142854\tValidation acc 62.0\n",
      "Training [20%]\tTraining loss 1.9572320003594672\tValidation loss 1.9525043088197709\tTraining acc 58.660714285714285\tValidation acc 61.0\n",
      "Training [25%]\tTraining loss 1.9503068255526679\tValidation loss 1.9459553855657576\tTraining acc 60.892857142857146\tValidation acc 62.5\n",
      "Training [30%]\tTraining loss 1.9438618129917553\tValidation loss 1.9404416155815125\tTraining acc 64.28571428571429\tValidation acc 63.5\n",
      "Training [35%]\tTraining loss 1.9393225444214686\tValidation loss 1.9370785075426102\tTraining acc 66.69642857142857\tValidation acc 65.0\n",
      "Training [40%]\tTraining loss 1.9335956593709334\tValidation loss 1.9437269812822342\tTraining acc 69.01785714285714\tValidation acc 61.0\n",
      "Training [45%]\tTraining loss 1.929594180413655\tValidation loss 1.9339531642198562\tTraining acc 69.28571428571429\tValidation acc 67.5\n",
      "Training [50%]\tTraining loss 1.925129529514483\tValidation loss 1.9282843285799027\tTraining acc 72.14285714285714\tValidation acc 69.5\n",
      "Training [55%]\tTraining loss 1.9205945884542805\tValidation loss 1.9305025166273118\tTraining acc 73.48214285714286\tValidation acc 67.5\n",
      "Training [60%]\tTraining loss 1.9167233513934272\tValidation loss 1.9224344795942307\tTraining acc 75.26785714285714\tValidation acc 71.0\n",
      "Training [65%]\tTraining loss 1.9126036930297101\tValidation loss 1.9140395629405975\tTraining acc 76.96428571428571\tValidation acc 78.5\n",
      "Training [70%]\tTraining loss 1.90832445195743\tValidation loss 1.9107719826698304\tTraining acc 78.83928571428571\tValidation acc 79.0\n",
      "Training [75%]\tTraining loss 1.9031339579394886\tValidation loss 1.9086129707098007\tTraining acc 81.33928571428571\tValidation acc 78.0\n",
      "Training [80%]\tTraining loss 1.8996048628219537\tValidation loss 1.921956454515457\tTraining acc 82.14285714285714\tValidation acc 72.0\n",
      "Training [85%]\tTraining loss 1.8952967657574586\tValidation loss 1.9009840238094329\tTraining acc 84.64285714285714\tValidation acc 82.0\n",
      "Training [90%]\tTraining loss 1.8923800667481763\tValidation loss 1.8986027479171752\tTraining acc 85.80357142857143\tValidation acc 84.0\n",
      "Training [95%]\tTraining loss 1.8884788194937365\tValidation loss 1.8982788962125778\tTraining acc 87.85714285714286\tValidation acc 83.0\n",
      "Training [100%]\tTraining loss 2.0151021360286645\tValidation loss 2.02363305747509\tTraining acc 29.285714285714285\tValidation acc 25.0\n",
      "{'train_loss': 2.0151021360286645, 'val_loss': 2.02363305747509, 'train_acc': 29.285714285714285, 'val_acc': 25.0}\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 2.013639803443636\tValidation loss 1.9989437758922577\tTraining acc 28.660714285714285\tValidation acc 38.0\n",
      "Training [10%]\tTraining loss 1.9886446850640433\tValidation loss 1.982269525527954\tTraining acc 43.125\tValidation acc 47.0\n",
      "Training [15%]\tTraining loss 1.9709994797195707\tValidation loss 1.9708767223358155\tTraining acc 52.142857142857146\tValidation acc 49.5\n",
      "Training [20%]\tTraining loss 1.9615252287260123\tValidation loss 1.954116563796997\tTraining acc 56.69642857142857\tValidation acc 60.5\n",
      "Training [25%]\tTraining loss 1.9515569105744361\tValidation loss 1.95027980864048\tTraining acc 61.160714285714285\tValidation acc 62.5\n",
      "Training [30%]\tTraining loss 1.9419190860220363\tValidation loss 1.9506624466180802\tTraining acc 66.16071428571429\tValidation acc 63.0\n",
      "Training [35%]\tTraining loss 1.9381472675928049\tValidation loss 1.9510649752616882\tTraining acc 67.05357142857143\tValidation acc 61.0\n",
      "Training [40%]\tTraining loss 1.9348179456378733\tValidation loss 1.9364924442768097\tTraining acc 67.85714285714286\tValidation acc 67.0\n",
      "Training [45%]\tTraining loss 1.9314600603921073\tValidation loss 1.9368082934617996\tTraining acc 69.73214285714286\tValidation acc 66.0\n",
      "Training [50%]\tTraining loss 1.926674588343927\tValidation loss 1.9307219117879868\tTraining acc 72.05357142857143\tValidation acc 68.5\n",
      "Training [55%]\tTraining loss 1.9240754468100412\tValidation loss 1.9272818487882615\tTraining acc 72.23214285714286\tValidation acc 71.0\n",
      "Training [60%]\tTraining loss 1.9208082431129048\tValidation loss 1.922879496216774\tTraining acc 75.0\tValidation acc 71.5\n",
      "Training [65%]\tTraining loss 1.9143540164189679\tValidation loss 1.9238580912351608\tTraining acc 76.78571428571429\tValidation acc 70.5\n",
      "Training [70%]\tTraining loss 1.9118845144552843\tValidation loss 1.9154767799377441\tTraining acc 77.58928571428571\tValidation acc 75.5\n",
      "Training [75%]\tTraining loss 1.9066663620727402\tValidation loss 1.9113126134872436\tTraining acc 80.35714285714286\tValidation acc 77.0\n",
      "Training [80%]\tTraining loss 1.9032866474773202\tValidation loss 1.9140112751722336\tTraining acc 81.60714285714286\tValidation acc 75.5\n",
      "Training [85%]\tTraining loss 1.8995731790150916\tValidation loss 1.9109207248687745\tTraining acc 83.125\tValidation acc 78.5\n",
      "Training [90%]\tTraining loss 1.8975263914891651\tValidation loss 1.910468955039978\tTraining acc 83.125\tValidation acc 77.5\n",
      "Training [95%]\tTraining loss 1.8938750212746007\tValidation loss 1.9038806730508804\tTraining acc 85.17857142857143\tValidation acc 80.0\n",
      "Training [100%]\tTraining loss 1.8904157172356333\tValidation loss 1.9084087473154068\tTraining acc 86.42857142857143\tValidation acc 79.5\n",
      "{'train_loss': 1.8904157172356333, 'val_loss': 1.9084087473154068, 'train_acc': 86.42857142857143, 'val_acc': 79.5}\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "FINAL RESULTS\n",
      "--------------------------------\n",
      "{0: {'train_loss': 1.8821674409721578, 'val_loss': 1.9070368802547455, 'train_acc': 89.46428571428571, 'val_acc': 79.0}, 1: {'train_loss': 2.0256188534200192, 'val_loss': 2.023657767176628, 'train_acc': 23.928571428571427, 'val_acc': 25.0}, 2: {'train_loss': 2.0228484766823907, 'val_loss': 2.0236448496580124, 'train_acc': 25.267857142857142, 'val_acc': 25.0}, 3: {'train_loss': 2.0151021360286645, 'val_loss': 2.02363305747509, 'train_acc': 29.285714285714285, 'val_acc': 25.0}, 4: {'train_loss': 1.8904157172356333, 'val_loss': 1.9084087473154068, 'train_acc': 86.42857142857143, 'val_acc': 79.5}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "\n",
    "dataset = ConcatDataset([X_train, X_test])\n",
    "\n",
    "k_folds = 5\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "results = {}\n",
    "\n",
    "print('--------------------------------')\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "  \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=1, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=1, sampler=test_subsampler)\n",
    "\n",
    "    network = Net()\n",
    "    network.apply(reset_weights)\n",
    "    \n",
    "    optimizer = optim.SGD(network.parameters(), lr =0.001, momentum=0.9)\n",
    "    \n",
    "    result = train_network(network, optimizer, trainloader, testloader)\n",
    "    \n",
    "    results[fold] = result\n",
    "    \n",
    "    print(result)\n",
    "    print('--------------------------------')\n",
    "\n",
    "print('--------------------------------')\n",
    "print('FINAL RESULTS')\n",
    "print('--------------------------------')\n",
    "print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 30\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# loss_results = {\n",
    "#     'train': [],\n",
    "#     'val': []\n",
    "# }\n",
    "\n",
    "# acc_results = {\n",
    "#     'train': [],\n",
    "#     'val': []\n",
    "# }\n",
    "\n",
    "# loaders = {\n",
    "#     'train': train_loader,\n",
    "#     'val': test_loader\n",
    "# }\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = {\n",
    "#         'train': [],\n",
    "#         'val': []\n",
    "#     }\n",
    "    \n",
    "#     total_acc = {\n",
    "#         'train': [],\n",
    "#         'val': []\n",
    "#     }\n",
    "    \n",
    "    \n",
    "#     for phase in ['train', 'val']:\n",
    "#         if phase == 'train':\n",
    "#             network.train(True)\n",
    "#         else:\n",
    "#             network.train(False)\n",
    "        \n",
    "#         correct = 0\n",
    "#         number = 0\n",
    "        \n",
    "#         for batch_idx, (data, target) in enumerate(loaders[phase]):\n",
    "#             optimizer.zero_grad()\n",
    "#             output = network(data)\n",
    "            \n",
    "#             loss = loss_func(output, target)\n",
    "            \n",
    "#             if phase == 'train':\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             total_loss[phase].append(loss.item())\n",
    "\n",
    "#             _, predicted = torch.max(output.data, 1)\n",
    "#             number += target.size(0)\n",
    "#             correct += (predicted == target).sum().item()\n",
    "            \n",
    "#         loss_results[phase].append(sum(total_loss[phase]) / len(total_loss[phase]))\n",
    "#         acc_results[phase].append(100 * correct / number)\n",
    "\n",
    "#     progress = 100.0 * (epoch + 1) / epochs\n",
    "    \n",
    "#     print(f'Training [{int(progress)}%]\\tTraining loss {loss_results[\"train\"][-1]}\\tValidation loss {loss_results[\"val\"][-1]}\\tTraining acc {acc_results[\"train\"][-1]}\\tValidation acc {acc_results[\"val\"][-1]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_results[\"train\"])\n",
    "# plt.title(f'Hybrid NN training convergence for {NUM_QUBITS}-qubits')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_results[\"val\"])\n",
    "# plt.title(f'Hybrid NN training convergence for {NUM_QUBITS}-qubits')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = 0\n",
    "# number = 0\n",
    "# for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#     number += 1\n",
    "#     output = network.predict(data).item()\n",
    "#     accuracy += (output == target[0].item())*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Performance on test data: {accuracy}/{number} = {accuracy/number*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_shape = (8, 6)\n",
    "# count = 0\n",
    "# fig, axes = plt.subplots(nrows=n_sample_shape[0], ncols=n_sample_shape[1], figsize=(10, 2*n_sample_shape[0]))\n",
    "\n",
    "# classes_map = {\n",
    "#     0: 'airplane',\n",
    "#     1: 'automobile',\n",
    "#     2: 'bird',\n",
    "#     3: 'cat',\n",
    "#     4: 'deer',\n",
    "#     5: 'dog',\n",
    "#     6: 'frog',\n",
    "#     7: 'horse',\n",
    "#     8: 'ship',\n",
    "#     9: 'truck'\n",
    "# }\n",
    "\n",
    "# network.eval()\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#         if count == n_sample_shape[0]*n_sample_shape[1]:\n",
    "#             break\n",
    "        \n",
    "#         prediction = network.predict(data).item()\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].imshow(data[0].numpy().squeeze().swapaxes(0,1).swapaxes(1,2))\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_xticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_yticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_title(classes_map[prediction])\n",
    "        \n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
