{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: qiskit in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.23.2)\n",
      "Requirement already satisfied: qiskit-aer==0.7.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.7.2)\n",
      "Requirement already satisfied: qiskit-aqua==0.8.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.8.1)\n",
      "Requirement already satisfied: qiskit-ibmq-provider==0.11.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.11.1)\n",
      "Requirement already satisfied: qiskit-ignis==0.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.5.1)\n",
      "Requirement already satisfied: qiskit-terra==0.16.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.16.1)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (1.5.2)\n",
      "Requirement already satisfied: pybind11>=2.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (2.6.1)\n",
      "Requirement already satisfied: numpy>=1.16.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (1.19.2)\n",
      "Requirement already satisfied: cython>=0.27.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (0.29.21)\n",
      "Requirement already satisfied: sympy>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.7.1)\n",
      "Requirement already satisfied: yfinance in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.1.55)\n",
      "Requirement already satisfied: retworkx>=0.5.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.7.2)\n",
      "Requirement already satisfied: docplex in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.19.202)\n",
      "Requirement already satisfied: quandl in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (3.6.0)\n",
      "Requirement already satisfied: psutil>=5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (5.8.0)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (50.3.0)\n",
      "Requirement already satisfied: pandas in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.1.3)\n",
      "Requirement already satisfied: dlx in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.0.4)\n",
      "Requirement already satisfied: fastdtw in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.3.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.23.2)\n",
      "Requirement already satisfied: h5py in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.10.0)\n",
      "Requirement already satisfied: requests-ntlm>=1.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.1.0)\n",
      "Requirement already satisfied: requests>=2.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (2.24.0)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.25.10)\n",
      "Requirement already satisfied: nest-asyncio!=1.1.0,>=1.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (2.8.1)\n",
      "Requirement already satisfied: websockets>=8 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (8.1)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.5)\n",
      "Requirement already satisfied: python-constraint>=1.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: ply>=3.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.11)\n",
      "Requirement already satisfied: fastjsonschema>=2.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.14.5)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (0.3.3)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from sympy>=1.3->qiskit-aqua==0.8.1->qiskit) (1.1.0)\n",
      "Requirement already satisfied: lxml>=4.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (4.6.2)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (0.0.9)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from docplex->qiskit-aqua==0.8.1->qiskit) (1.15.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (8.6.0)\n",
      "Requirement already satisfied: inflection>=0.3.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from pandas->qiskit-aqua==0.8.1->qiskit) (2020.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.20.0->qiskit-aqua==0.8.1->qiskit) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.20.0->qiskit-aqua==0.8.1->qiskit) (2.1.0)\n",
      "Requirement already satisfied: ntlm-auth>=1.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.5.0)\n",
      "Requirement already satisfied: cryptography>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (3.3.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (3.0.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2020.6.20)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from networkx>=2.2->qiskit-terra==0.16.1->qiskit) (4.4.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (20.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (0.17.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.14.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (2.20)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pylatexenc in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (2.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torchvision in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.8.2)\n",
      "Requirement already satisfied: torch==1.7.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.7.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch==1.7.1->torchvision) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (4.56.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for itertools\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install qiskit\n",
    "!{sys.executable} -m pip install pylatexenc\n",
    "!{sys.executable} -m pip install torchvision\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import execute\n",
    "from qiskit.circuit import Parameter,ControlledGate\n",
    "from qiskit import Aer\n",
    "import qiskit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "\n",
    "NUM_QUBITS = 2\n",
    "NUM_SHOTS = 5000\n",
    "SHIFT = np.pi/4\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "SIMULATOR = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '01', '10', '11']\n"
     ]
    }
   ],
   "source": [
    "# create list of all possible outputs of quantum circuit (2**NUM_QUBITS possible)\n",
    "import itertools\n",
    "\n",
    "def create_QC_OUTPUTS():\n",
    "    measurements = list(itertools.product([0, 1], repeat=NUM_QUBITS))\n",
    "    return [''.join([str(bit) for bit in measurement]) for measurement in measurements]\n",
    "\n",
    "QC_OUTPUTS = create_QC_OUTPUTS()\n",
    "print(QC_OUTPUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contruct QuantumCircuit QFT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumCircuit:\n",
    "    def __init__(self, n_qubits, backend, shots):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.circuit = qiskit.QuantumCircuit(n_qubits)\n",
    "        self.thetas = { k: Parameter('Theta'+str(k)) for k in range(self.n_qubits) }\n",
    "        \n",
    "        all_qubits = [i for i in range(n_qubits)]\n",
    "        \n",
    "        self.circuit.h(all_qubits)\n",
    "        self.circuit.barrier()\n",
    "        for k in range(n_qubits):\n",
    "            self.circuit.ry(self.thetas[k], k)\n",
    "            \n",
    "        self.circuit.measure_all()\n",
    "        \n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "        \n",
    "    def N_qubits_expectation_Z(self, counts, shots, n_qubits):\n",
    "        expects = np.zeros(len(QC_OUTPUTS))\n",
    "        for k in range(len(QC_OUTPUTS)):\n",
    "            key = QC_OUTPUTS[k]\n",
    "            perc = counts.get(key, 0) / shots\n",
    "            expects[k] = perc\n",
    "        return expects\n",
    "    \n",
    "    def run(self, i):\n",
    "        params = i\n",
    "        \n",
    "        job_sim = execute(self.circuit, \n",
    "                          self.backend, \n",
    "                          shots = self.shots, \n",
    "                          parameter_binds = [{ self.thetas[k]: params[k].item() for k in range(self.n_qubits) }])\n",
    "        \n",
    "        result_sim = job_sim.result()\n",
    "        counts = result_sim.get_counts(self.circuit)\n",
    "        \n",
    "        return self.N_qubits_expectation_Z(counts, self.shots, self.n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value for rotation [pi/4]: [0.0208 0.13   0.1256 0.7236]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 478.079x204.68 with 1 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, NUM_SHOTS)\n",
    "print(f'Expected value for rotation [pi/4]: {circuit.run(torch.Tensor([np.pi/4] * len(QC_OUTPUTS)))}')\n",
    "circuit.circuit.draw(output='mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCircuit(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        if not hasattr(ctx, 'QuantumCircuit'):\n",
    "            ctx.QuantumCircuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, shots=NUM_SHOTS)\n",
    "        \n",
    "        exp_value = ctx.QuantumCircuit.run(i)\n",
    "        \n",
    "        result = torch.Tensor([exp_value])\n",
    "        \n",
    "        ctx.save_for_backward(result, i)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        forward_tensor,i = ctx.saved_tensors\n",
    "        \n",
    "        input_numbers = i\n",
    "        gradients = torch.Tensor()\n",
    "        \n",
    "        for k in range(NUM_QUBITS):\n",
    "            shift_right = input_numbers.detach().clone()\n",
    "            shift_right[k] = shift_right[k] + SHIFT\n",
    "            \n",
    "            shift_left = input_numbers.detach().clone()\n",
    "            shift_left[k] = shift_left[k] - SHIFT\n",
    "            \n",
    "            expectation_right = ctx.QuantumCircuit.run(shift_right)\n",
    "            expectation_left = ctx.QuantumCircuit.run(shift_left)\n",
    "            \n",
    "            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])\n",
    "            gradient = gradient / torch.norm(gradient)\n",
    "            \n",
    "            gradients = torch.cat((gradients, gradient.float()))\n",
    "            \n",
    "        result = torch.Tensor(gradients)\n",
    "        \n",
    "        return (result.float() * grad_output.float()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 after quantum layer: tensor([[0.0204, 0.1306, 0.1314, 0.7176]], grad_fn=<TorchCircuitBackward>)\n",
      "x.grad = tensor([-0.1977,  0.1248])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "qc = TorchCircuit.apply\n",
    "\n",
    "y1 = qc(x)\n",
    "print(f'y1 after quantum layer: {y1}')\n",
    "y1 = nn.Linear(2**NUM_QUBITS, 1)(y1.float())\n",
    "\n",
    "y1.backward()\n",
    "print(f'x.grad = {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1422f9580>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qc = TorchCircuit.apply\n",
    "\n",
    "def cost(x):\n",
    "    target = -1\n",
    "    expval = qc(x)[0]\n",
    "    \n",
    "    val = sum([(i + 1) * expval[i] for i in range(2**NUM_QUBITS)]) / 2**NUM_QUBITS\n",
    "    \n",
    "    return torch.abs(target - val) ** 2, expval\n",
    "\n",
    "\n",
    "x = torch.tensor([-np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "opt = optim.Adam([x], lr = 0.1)\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "loss_list = []\n",
    "expval_list = []\n",
    "\n",
    "for i in tqdm(range(num_epoch)):\n",
    "    opt.zero_grad()\n",
    "    loss, expval = cost(x)\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()\n",
    "    loss_list.append(loss.item())\n",
    "    expval_list.append(expval)\n",
    "    \n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "n_samples = 300\n",
    "\n",
    "X_train = datasets.CIFAR10(root='./data', \n",
    "                                train=True,\n",
    "                                download=True,\n",
    "                                transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "\n",
    "idx = np.append(np.where(np.array(X_train.targets) == 0)[0][:n_samples], \n",
    "                np.where(np.array(X_train.targets) == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 3)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 4)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 5)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 6)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 7)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 8)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 9)[0][:n_samples])\n",
    "\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = np.array(X_train.targets)[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "n_samples = 50\n",
    "\n",
    "X_test = datasets.CIFAR10(root='./data', \n",
    "                               train=False,\n",
    "                               download=True,\n",
    "                               transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "\n",
    "idx = np.append(np.where(np.array(X_test.targets) == 0)[0][:n_samples], \n",
    "                np.where(np.array(X_test.targets) == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 3)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 4)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 5)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 6)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 7)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 8)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 9)[0][:n_samples])\n",
    "\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = np.array(X_test.targets)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(X_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 4)\n",
    "        self.fc4 = nn.Linear(4, 4)\n",
    "#         self.fc3 = nn.Linear(84, NUM_QUBITS)\n",
    "#         self.fc4 = nn.Linear(NUM_QUBITS, 4)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "#         x = np.pi*torch.tanh(x)\n",
    "#         x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "#         x = torch.sigmoid(x)\n",
    "#         x = torch.cat((x, 1-x), -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        pred = self.forward(x)\n",
    "        ans = torch.argmax(pred[0]).item()\n",
    "        \n",
    "        return torch.tensor(ans)\n",
    "    \n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, optimizer, train_loader, val_loader):\n",
    "    epochs = 30\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader\n",
    "    }\n",
    "    \n",
    "    loss_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    acc_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': test_loader\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "        total_acc = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                network.train(True)\n",
    "            else:\n",
    "                network.train(False)\n",
    "\n",
    "            correct = 0\n",
    "            number = 0\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(loaders[phase]):\n",
    "                optimizer.zero_grad()\n",
    "                output = network(data)\n",
    "\n",
    "                loss = loss_func(output, target)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                total_loss[phase].append(loss.item())\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                number += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "            loss_results[phase].append(sum(total_loss[phase]) / len(total_loss[phase]))\n",
    "            acc_results[phase].append(100 * correct / number)\n",
    "\n",
    "        progress = 100.0 * (epoch + 1) / epochs\n",
    "\n",
    "        print(f'Training [{int(progress)}%]\\tTraining loss {loss_results[\"train\"][-1]}\\tValidation loss {loss_results[\"val\"][-1]}\\tTraining acc {acc_results[\"train\"][-1]}\\tValidation acc {acc_results[\"val\"][-1]}')\n",
    "        \n",
    "    return {\n",
    "        'train_loss': loss_results[\"train\"][-1],\n",
    "        'val_loss': loss_results[\"val\"][-1],\n",
    "        'train_acc': acc_results[\"train\"][-1],\n",
    "        'val_acc': acc_results[\"val\"][-1]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 1.3871430732309817\tValidation loss 1.3891146385669708\tTraining acc 26.339285714285715\tValidation acc 25.0\n",
      "Training [10%]\tTraining loss 1.3855056861681598\tValidation loss 1.3853813368082046\tTraining acc 26.785714285714285\tValidation acc 25.0\n",
      "Training [15%]\tTraining loss 1.380469376434173\tValidation loss 1.3493784978985786\tTraining acc 27.946428571428573\tValidation acc 35.5\n",
      "Training [20%]\tTraining loss 1.3200188656541285\tValidation loss 1.3473649126291276\tTraining acc 37.94642857142857\tValidation acc 33.0\n",
      "Training [25%]\tTraining loss 1.2462064770624108\tValidation loss 1.1500724793598056\tTraining acc 42.05357142857143\tValidation acc 49.5\n",
      "Training [30%]\tTraining loss 1.1966564581613057\tValidation loss 1.1165565840154885\tTraining acc 42.67857142857143\tValidation acc 48.0\n",
      "Training [35%]\tTraining loss 1.1552400493115231\tValidation loss 1.140219020654913\tTraining acc 46.42857142857143\tValidation acc 53.0\n",
      "Training [40%]\tTraining loss 1.126351611165225\tValidation loss 1.1151108597544954\tTraining acc 48.839285714285715\tValidation acc 52.0\n",
      "Training [45%]\tTraining loss 1.0900241660965335\tValidation loss 1.1085930412932066\tTraining acc 50.982142857142854\tValidation acc 53.0\n",
      "Training [50%]\tTraining loss 1.0165784015171326\tValidation loss 0.9656674198037944\tTraining acc 54.732142857142854\tValidation acc 55.0\n",
      "Training [55%]\tTraining loss 1.0304661954057452\tValidation loss 1.1155005560815334\tTraining acc 54.017857142857146\tValidation acc 47.5\n",
      "Training [60%]\tTraining loss 1.020518546168848\tValidation loss 0.9018245423189364\tTraining acc 52.767857142857146\tValidation acc 63.0\n",
      "Training [65%]\tTraining loss 0.9674472261808241\tValidation loss 0.9044711460966209\tTraining acc 57.94642857142857\tValidation acc 63.5\n",
      "Training [70%]\tTraining loss 1.0111131194383323\tValidation loss 0.8210692123137414\tTraining acc 56.69642857142857\tValidation acc 65.5\n",
      "Training [75%]\tTraining loss 0.9624567593175144\tValidation loss 0.894583088693129\tTraining acc 57.767857142857146\tValidation acc 61.5\n",
      "Training [80%]\tTraining loss 0.8721260781960899\tValidation loss 0.9885274663801465\tTraining acc 60.44642857142857\tValidation acc 57.5\n",
      "Training [85%]\tTraining loss 0.9686731957957562\tValidation loss 0.8807878044078825\tTraining acc 57.857142857142854\tValidation acc 61.5\n",
      "Training [90%]\tTraining loss 0.9250308724559342\tValidation loss 0.8198435128619894\tTraining acc 59.82142857142857\tValidation acc 66.5\n",
      "Training [95%]\tTraining loss 0.9142286926881235\tValidation loss 0.8590668661729433\tTraining acc 59.464285714285715\tValidation acc 64.5\n",
      "Training [100%]\tTraining loss 0.9041693993218995\tValidation loss 0.8163186133321142\tTraining acc 60.267857142857146\tValidation acc 64.0\n",
      "{'train_loss': 0.9041693993218995, 'val_loss': 0.8163186133321142, 'train_acc': 60.267857142857146, 'val_acc': 64.0}\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 1.392535607303892\tValidation loss 1.3815070152282716\tTraining acc 24.107142857142858\tValidation acc 25.0\n",
      "Training [10%]\tTraining loss 1.3536659462111338\tValidation loss 1.2895942815393209\tTraining acc 32.05357142857143\tValidation acc 39.0\n",
      "Training [15%]\tTraining loss 1.2948090047010088\tValidation loss 1.2588784179091455\tTraining acc 36.160714285714285\tValidation acc 41.0\n",
      "Training [20%]\tTraining loss 1.2447223062319348\tValidation loss 1.1944040140509606\tTraining acc 40.267857142857146\tValidation acc 40.0\n",
      "Training [25%]\tTraining loss 1.211594211283539\tValidation loss 1.145147507339716\tTraining acc 39.285714285714285\tValidation acc 43.5\n",
      "Training [30%]\tTraining loss 1.15356194011734\tValidation loss 1.1237283486127854\tTraining acc 41.964285714285715\tValidation acc 43.0\n",
      "Training [35%]\tTraining loss 1.1342213235795497\tValidation loss 1.1068477053195238\tTraining acc 42.767857142857146\tValidation acc 46.5\n",
      "Training [40%]\tTraining loss 1.1157480707092742\tValidation loss 1.0276981009542943\tTraining acc 46.42857142857143\tValidation acc 53.5\n",
      "Training [45%]\tTraining loss 1.057445981128337\tValidation loss 1.0074663086235522\tTraining acc 51.875\tValidation acc 61.0\n",
      "Training [50%]\tTraining loss 1.0237229795670177\tValidation loss 0.9052360084652901\tTraining acc 52.232142857142854\tValidation acc 61.0\n",
      "Training [55%]\tTraining loss 1.0097637159954793\tValidation loss 0.9417517901957035\tTraining acc 53.75\tValidation acc 61.0\n",
      "Training [60%]\tTraining loss 1.0154606281596767\tValidation loss 1.023400367833674\tTraining acc 56.339285714285715\tValidation acc 54.0\n",
      "Training [65%]\tTraining loss 0.9915318032791609\tValidation loss 0.8928462384827435\tTraining acc 54.732142857142854\tValidation acc 61.0\n",
      "Training [70%]\tTraining loss 1.0090142142999057\tValidation loss 0.8749354132823646\tTraining acc 55.80357142857143\tValidation acc 59.5\n",
      "Training [75%]\tTraining loss 0.9664516266967504\tValidation loss 0.8897242366825231\tTraining acc 58.214285714285715\tValidation acc 60.0\n",
      "Training [80%]\tTraining loss 0.9654155526036577\tValidation loss 0.8294341046782211\tTraining acc 57.32142857142857\tValidation acc 61.5\n",
      "Training [85%]\tTraining loss 0.9675772123778253\tValidation loss 0.9028416457609274\tTraining acc 57.589285714285715\tValidation acc 60.5\n",
      "Training [90%]\tTraining loss 0.9473100906793823\tValidation loss 0.8224993439298123\tTraining acc 59.375\tValidation acc 65.0\n",
      "Training [95%]\tTraining loss 0.9335336653419603\tValidation loss 0.9007940236898139\tTraining acc 57.589285714285715\tValidation acc 57.0\n",
      "Training [100%]\tTraining loss 0.9385312726081944\tValidation loss 0.8574562139716\tTraining acc 61.160714285714285\tValidation acc 68.5\n",
      "{'train_loss': 0.9385312726081944, 'val_loss': 0.8574562139716, 'train_acc': 61.160714285714285, 'val_acc': 68.5}\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 1.3961012445390224\tValidation loss 1.3898059296607972\tTraining acc 24.285714285714285\tValidation acc 25.0\n",
      "Training [10%]\tTraining loss 1.3882541401045663\tValidation loss 1.3766179639101028\tTraining acc 24.553571428571427\tValidation acc 26.5\n",
      "Training [15%]\tTraining loss 1.3517842231584447\tValidation loss 1.258826314806938\tTraining acc 33.75\tValidation acc 45.0\n",
      "Training [20%]\tTraining loss 1.2329217546020768\tValidation loss 1.1726965752243996\tTraining acc 39.732142857142854\tValidation acc 41.0\n",
      "Training [25%]\tTraining loss 1.1418513785315945\tValidation loss 1.1636169177293778\tTraining acc 43.125\tValidation acc 39.5\n",
      "Training [30%]\tTraining loss 1.1389078207330645\tValidation loss 1.0801582297682761\tTraining acc 42.142857142857146\tValidation acc 43.0\n",
      "Training [35%]\tTraining loss 1.1049829941509026\tValidation loss 1.0492583724856377\tTraining acc 43.75\tValidation acc 49.0\n",
      "Training [40%]\tTraining loss 1.069929939985741\tValidation loss 0.9649997990578413\tTraining acc 48.482142857142854\tValidation acc 58.0\n",
      "Training [45%]\tTraining loss 1.0534351403464306\tValidation loss 1.0002175938850268\tTraining acc 53.57142857142857\tValidation acc 54.5\n",
      "Training [50%]\tTraining loss 1.0324638023384067\tValidation loss 0.9565879938006401\tTraining acc 51.607142857142854\tValidation acc 55.0\n",
      "Training [55%]\tTraining loss 0.9754987619832848\tValidation loss 0.8682912459690124\tTraining acc 57.232142857142854\tValidation acc 65.0\n",
      "Training [60%]\tTraining loss 0.9579830151604481\tValidation loss 0.8689282926172018\tTraining acc 55.267857142857146\tValidation acc 62.5\n",
      "Training [65%]\tTraining loss 0.9545487372614324\tValidation loss 0.8391696441336535\tTraining acc 57.67857142857143\tValidation acc 67.0\n",
      "Training [70%]\tTraining loss 0.9455059219578772\tValidation loss 0.8625254295766354\tTraining acc 59.55357142857143\tValidation acc 58.5\n",
      "Training [75%]\tTraining loss 0.9272286408154575\tValidation loss 0.8622494861437008\tTraining acc 62.5\tValidation acc 61.5\n",
      "Training [80%]\tTraining loss 0.8889915443963413\tValidation loss 0.8085778697580099\tTraining acc 60.982142857142854\tValidation acc 66.0\n",
      "Training [85%]\tTraining loss 0.8852212939881452\tValidation loss 0.8108650674670934\tTraining acc 62.232142857142854\tValidation acc 64.5\n",
      "Training [90%]\tTraining loss 0.888718725390988\tValidation loss 0.7450767346820794\tTraining acc 61.785714285714285\tValidation acc 63.5\n",
      "Training [95%]\tTraining loss 0.8481058927102465\tValidation loss 0.747058158107102\tTraining acc 63.214285714285715\tValidation acc 65.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [100%]\tTraining loss 0.8709322481023051\tValidation loss 0.7986095838854089\tTraining acc 63.30357142857143\tValidation acc 65.0\n",
      "{'train_loss': 0.8709322481023051, 'val_loss': 0.7986095838854089, 'train_acc': 63.30357142857143, 'val_acc': 65.0}\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 1.4001952765243395\tValidation loss 1.382950974702835\tTraining acc 25.089285714285715\tValidation acc 28.5\n",
      "Training [10%]\tTraining loss 1.3704065850337168\tValidation loss 1.3374360641092062\tTraining acc 30.0\tValidation acc 33.0\n",
      "Training [15%]\tTraining loss 1.3234239830053411\tValidation loss 1.2432892683148384\tTraining acc 35.44642857142857\tValidation acc 45.5\n",
      "Training [20%]\tTraining loss 1.2630069702331508\tValidation loss 1.2368214264512063\tTraining acc 40.982142857142854\tValidation acc 41.5\n",
      "Training [25%]\tTraining loss 1.217497826462412\tValidation loss 1.224061762392521\tTraining acc 43.30357142857143\tValidation acc 47.5\n",
      "Training [30%]\tTraining loss 1.128934732844521\tValidation loss 1.169697434036061\tTraining acc 48.75\tValidation acc 42.0\n",
      "Training [35%]\tTraining loss 1.1212793495688986\tValidation loss 1.0697220633924007\tTraining acc 47.410714285714285\tValidation acc 57.5\n",
      "Training [40%]\tTraining loss 1.0663726739520956\tValidation loss 1.0419604497402906\tTraining acc 50.80357142857143\tValidation acc 51.0\n",
      "Training [45%]\tTraining loss 1.0477634746662197\tValidation loss 0.9568985053803771\tTraining acc 50.80357142857143\tValidation acc 58.0\n",
      "Training [50%]\tTraining loss 1.0116736131029809\tValidation loss 0.9576893871836364\tTraining acc 53.92857142857143\tValidation acc 55.5\n",
      "Training [55%]\tTraining loss 0.9990160036738546\tValidation loss 0.9365960833476856\tTraining acc 54.642857142857146\tValidation acc 61.5\n",
      "Training [60%]\tTraining loss 1.007831179220774\tValidation loss 0.8907355025690049\tTraining acc 56.517857142857146\tValidation acc 63.5\n",
      "Training [65%]\tTraining loss 0.9552961508255197\tValidation loss 0.8615502015687525\tTraining acc 58.30357142857143\tValidation acc 63.0\n",
      "Training [70%]\tTraining loss 0.9465868325583869\tValidation loss 0.907319469107315\tTraining acc 57.232142857142854\tValidation acc 59.5\n",
      "Training [75%]\tTraining loss 0.9756964210515434\tValidation loss 0.8574014870170504\tTraining acc 57.857142857142854\tValidation acc 66.0\n",
      "Training [80%]\tTraining loss 0.9233245548573763\tValidation loss 0.8618335819989443\tTraining acc 59.107142857142854\tValidation acc 66.5\n",
      "Training [85%]\tTraining loss 0.9245524598580752\tValidation loss 0.7864443052606657\tTraining acc 58.660714285714285\tValidation acc 69.0\n",
      "Training [90%]\tTraining loss 0.9278164047580304\tValidation loss 0.7899312931124587\tTraining acc 59.55357142857143\tValidation acc 64.5\n",
      "Training [95%]\tTraining loss 0.9226556292291215\tValidation loss 0.7535189935434028\tTraining acc 58.30357142857143\tValidation acc 68.0\n",
      "Training [100%]\tTraining loss 0.8751828766022052\tValidation loss 0.7630517285131282\tTraining acc 61.875\tValidation acc 67.5\n",
      "{'train_loss': 0.8751828766022052, 'val_loss': 0.7630517285131282, 'train_acc': 61.875, 'val_acc': 67.5}\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 1.3903612835066659\tValidation loss 1.387931843996048\tTraining acc 23.839285714285715\tValidation acc 25.0\n",
      "Training [10%]\tTraining loss 1.3887888729572295\tValidation loss 1.3858594065904617\tTraining acc 24.642857142857142\tValidation acc 27.5\n",
      "Training [15%]\tTraining loss 1.3872603019433363\tValidation loss 1.3839864575862884\tTraining acc 25.982142857142858\tValidation acc 25.0\n",
      "Training [20%]\tTraining loss 1.383358711962189\tValidation loss 1.3670308214426041\tTraining acc 26.428571428571427\tValidation acc 46.0\n",
      "Training [25%]\tTraining loss 1.350210770099823\tValidation loss 1.2777410447597504\tTraining acc 34.375\tValidation acc 43.5\n",
      "Training [30%]\tTraining loss 1.3004714491644076\tValidation loss 1.2241103452444078\tTraining acc 40.267857142857146\tValidation acc 46.5\n",
      "Training [35%]\tTraining loss 1.2358377882844902\tValidation loss 1.0954101169109345\tTraining acc 45.625\tValidation acc 55.0\n",
      "Training [40%]\tTraining loss 1.231240314635579\tValidation loss 1.0858325481414794\tTraining acc 42.32142857142857\tValidation acc 51.5\n",
      "Training [45%]\tTraining loss 1.1312357154193575\tValidation loss 1.0181815545260906\tTraining acc 50.714285714285715\tValidation acc 57.5\n",
      "Training [50%]\tTraining loss 1.0868857724589231\tValidation loss 1.0164163127727806\tTraining acc 51.42857142857143\tValidation acc 58.0\n",
      "Training [55%]\tTraining loss 1.0283242110450326\tValidation loss 1.0603968722373247\tTraining acc 54.910714285714285\tValidation acc 52.5\n",
      "Training [60%]\tTraining loss 1.0352084360131488\tValidation loss 0.9637069210410119\tTraining acc 55.0\tValidation acc 59.5\n",
      "Training [65%]\tTraining loss 0.9955654352307833\tValidation loss 0.8556596331810579\tTraining acc 57.32142857142857\tValidation acc 64.0\n",
      "Training [70%]\tTraining loss 0.9731248721264839\tValidation loss 0.8520306972230901\tTraining acc 58.482142857142854\tValidation acc 61.5\n",
      "Training [75%]\tTraining loss 0.9738538993784195\tValidation loss 0.900992430076003\tTraining acc 56.875\tValidation acc 58.0\n",
      "Training [80%]\tTraining loss 0.9456935009840862\tValidation loss 0.8059783083270304\tTraining acc 59.017857142857146\tValidation acc 65.0\n",
      "Training [85%]\tTraining loss 0.9215715138606506\tValidation loss 0.814005872157868\tTraining acc 59.017857142857146\tValidation acc 67.0\n",
      "Training [90%]\tTraining loss 0.9515652483750947\tValidation loss 0.8336596716527128\tTraining acc 59.732142857142854\tValidation acc 63.0\n",
      "Training [95%]\tTraining loss 0.8716853428116161\tValidation loss 0.7990451723895967\tTraining acc 62.5\tValidation acc 66.5\n",
      "Training [100%]\tTraining loss 0.8606991662911211\tValidation loss 0.7755981028685346\tTraining acc 62.67857142857143\tValidation acc 64.5\n",
      "{'train_loss': 0.8606991662911211, 'val_loss': 0.7755981028685346, 'train_acc': 62.67857142857143, 'val_acc': 64.5}\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "FINAL RESULTS\n",
      "--------------------------------\n",
      "{0: {'train_loss': 0.9041693993218995, 'val_loss': 0.8163186133321142, 'train_acc': 60.267857142857146, 'val_acc': 64.0}, 1: {'train_loss': 0.9385312726081944, 'val_loss': 0.8574562139716, 'train_acc': 61.160714285714285, 'val_acc': 68.5}, 2: {'train_loss': 0.8709322481023051, 'val_loss': 0.7986095838854089, 'train_acc': 63.30357142857143, 'val_acc': 65.0}, 3: {'train_loss': 0.8751828766022052, 'val_loss': 0.7630517285131282, 'train_acc': 61.875, 'val_acc': 67.5}, 4: {'train_loss': 0.8606991662911211, 'val_loss': 0.7755981028685346, 'train_acc': 62.67857142857143, 'val_acc': 64.5}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "\n",
    "dataset = ConcatDataset([X_train, X_test])\n",
    "\n",
    "k_folds = 5\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "results = {}\n",
    "\n",
    "print('--------------------------------')\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "  \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=1, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=1, sampler=test_subsampler)\n",
    "\n",
    "    network = Net()\n",
    "    network.apply(reset_weights)\n",
    "    \n",
    "    optimizer = optim.SGD(network.parameters(), lr =0.001, momentum=0.9)\n",
    "    \n",
    "    result = train_network(network, optimizer, trainloader, testloader)\n",
    "    \n",
    "    results[fold] = result\n",
    "    \n",
    "    print(result)\n",
    "    print('--------------------------------')\n",
    "\n",
    "print('--------------------------------')\n",
    "print('FINAL RESULTS')\n",
    "print('--------------------------------')\n",
    "print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 30\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# loss_results = {\n",
    "#     'train': [],\n",
    "#     'val': []\n",
    "# }\n",
    "\n",
    "# acc_results = {\n",
    "#     'train': [],\n",
    "#     'val': []\n",
    "# }\n",
    "\n",
    "# loaders = {\n",
    "#     'train': train_loader,\n",
    "#     'val': test_loader\n",
    "# }\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = {\n",
    "#         'train': [],\n",
    "#         'val': []\n",
    "#     }\n",
    "    \n",
    "#     total_acc = {\n",
    "#         'train': [],\n",
    "#         'val': []\n",
    "#     }\n",
    "    \n",
    "    \n",
    "#     for phase in ['train', 'val']:\n",
    "#         if phase == 'train':\n",
    "#             network.train(True)\n",
    "#         else:\n",
    "#             network.train(False)\n",
    "        \n",
    "#         correct = 0\n",
    "#         number = 0\n",
    "        \n",
    "#         for batch_idx, (data, target) in enumerate(loaders[phase]):\n",
    "#             optimizer.zero_grad()\n",
    "#             output = network(data)\n",
    "            \n",
    "#             loss = loss_func(output, target)\n",
    "            \n",
    "#             if phase == 'train':\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             total_loss[phase].append(loss.item())\n",
    "\n",
    "#             _, predicted = torch.max(output.data, 1)\n",
    "#             number += target.size(0)\n",
    "#             correct += (predicted == target).sum().item()\n",
    "            \n",
    "#         loss_results[phase].append(sum(total_loss[phase]) / len(total_loss[phase]))\n",
    "#         acc_results[phase].append(100 * correct / number)\n",
    "\n",
    "#     progress = 100.0 * (epoch + 1) / epochs\n",
    "    \n",
    "#     print(f'Training [{int(progress)}%]\\tTraining loss {loss_results[\"train\"][-1]}\\tValidation loss {loss_results[\"val\"][-1]}\\tTraining acc {acc_results[\"train\"][-1]}\\tValidation acc {acc_results[\"val\"][-1]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_results[\"train\"])\n",
    "# plt.title(f'Hybrid NN training convergence for {NUM_QUBITS}-qubits')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_results[\"val\"])\n",
    "# plt.title(f'Hybrid NN training convergence for {NUM_QUBITS}-qubits')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = 0\n",
    "# number = 0\n",
    "# for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#     number += 1\n",
    "#     output = network.predict(data).item()\n",
    "#     accuracy += (output == target[0].item())*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Performance on test data: {accuracy}/{number} = {accuracy/number*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_shape = (8, 6)\n",
    "# count = 0\n",
    "# fig, axes = plt.subplots(nrows=n_sample_shape[0], ncols=n_sample_shape[1], figsize=(10, 2*n_sample_shape[0]))\n",
    "\n",
    "# classes_map = {\n",
    "#     0: 'airplane',\n",
    "#     1: 'automobile',\n",
    "#     2: 'bird',\n",
    "#     3: 'cat',\n",
    "#     4: 'deer',\n",
    "#     5: 'dog',\n",
    "#     6: 'frog',\n",
    "#     7: 'horse',\n",
    "#     8: 'ship',\n",
    "#     9: 'truck'\n",
    "# }\n",
    "\n",
    "# network.eval()\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#         if count == n_sample_shape[0]*n_sample_shape[1]:\n",
    "#             break\n",
    "        \n",
    "#         prediction = network.predict(data).item()\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].imshow(data[0].numpy().squeeze().swapaxes(0,1).swapaxes(1,2))\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_xticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_yticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_title(classes_map[prediction])\n",
    "        \n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
