{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: qiskit in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.23.2)\n",
      "Requirement already satisfied: qiskit-ignis==0.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.5.1)\n",
      "Requirement already satisfied: qiskit-ibmq-provider==0.11.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.11.1)\n",
      "Requirement already satisfied: qiskit-aer==0.7.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.7.2)\n",
      "Requirement already satisfied: qiskit-terra==0.16.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.16.1)\n",
      "Requirement already satisfied: qiskit-aqua==0.8.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ignis==0.5.1->qiskit) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.13 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ignis==0.5.1->qiskit) (1.19.2)\n",
      "Requirement already satisfied: scipy!=0.19.1,>=0.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ignis==0.5.1->qiskit) (1.5.2)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ignis==0.5.1->qiskit) (50.3.0)\n",
      "Requirement already satisfied: requests-ntlm>=1.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.1.0)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.25.10)\n",
      "Requirement already satisfied: websockets>=8 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (2.8.1)\n",
      "Requirement already satisfied: requests>=2.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (2.24.0)\n",
      "Requirement already satisfied: nest-asyncio!=1.1.0,>=1.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: cython>=0.27.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (0.29.21)\n",
      "Requirement already satisfied: pybind11>=2.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (2.6.1)\n",
      "Requirement already satisfied: retworkx>=0.5.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (0.7.2)\n",
      "Requirement already satisfied: python-constraint>=1.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.2.0)\n",
      "Requirement already satisfied: psutil>=5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (5.8.0)\n",
      "Requirement already satisfied: sympy>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.7.1)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (0.3.3)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.5)\n",
      "Requirement already satisfied: fastjsonschema>=2.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.14.5)\n",
      "Requirement already satisfied: ply>=3.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.11)\n",
      "Requirement already satisfied: yfinance in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.1.55)\n",
      "Requirement already satisfied: fastdtw in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.3.4)\n",
      "Requirement already satisfied: docplex in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.19.202)\n",
      "Requirement already satisfied: pandas in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.1.3)\n",
      "Requirement already satisfied: h5py in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.10.0)\n",
      "Requirement already satisfied: dlx in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.0.4)\n",
      "Requirement already satisfied: quandl in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (3.6.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.17->qiskit-ignis==0.5.1->qiskit) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.17->qiskit-ignis==0.5.1->qiskit) (2.1.0)\n",
      "Requirement already satisfied: cryptography>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (3.3.1)\n",
      "Requirement already satisfied: ntlm-auth>=1.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from python-dateutil>=2.8.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.15.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2020.6.20)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (20.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from sympy>=1.3->qiskit-terra==0.16.1->qiskit) (1.1.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from networkx>=2.2->qiskit-terra==0.16.1->qiskit) (4.4.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml>=4.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (4.6.2)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (0.0.9)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from pandas->qiskit-aqua==0.8.1->qiskit) (2020.1)\n",
      "Requirement already satisfied: more-itertools in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (8.6.0)\n",
      "Requirement already satisfied: inflection>=0.3.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (0.5.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.14.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (2.20)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pylatexenc in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (2.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torchvision in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.8.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: torch==1.7.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch==1.7.1->torchvision) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (4.56.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for itertools\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install qiskit\n",
    "!{sys.executable} -m pip install pylatexenc\n",
    "!{sys.executable} -m pip install torchvision\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import execute\n",
    "from qiskit.circuit import Parameter,ControlledGate\n",
    "from qiskit import Aer\n",
    "import qiskit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "\n",
    "NUM_QUBITS = 4\n",
    "NUM_SHOTS = 5000\n",
    "SHIFT = np.pi/4\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "SIMULATOR = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000', '0001', '0010', '0011', '0100', '0101', '0110', '0111', '1000', '1001', '1010', '1011', '1100', '1101', '1110', '1111']\n"
     ]
    }
   ],
   "source": [
    "# create list of all possible outputs of quantum circuit (2**NUM_QUBITS possible)\n",
    "import itertools\n",
    "\n",
    "def create_QC_OUTPUTS():\n",
    "    measurements = list(itertools.product([0, 1], repeat=NUM_QUBITS))\n",
    "    return [''.join([str(bit) for bit in measurement]) for measurement in measurements]\n",
    "\n",
    "QC_OUTPUTS = create_QC_OUTPUTS()\n",
    "print(QC_OUTPUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contruct QuantumCircuit QFT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumCircuit:\n",
    "    def __init__(self, n_qubits, backend, shots):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.circuit = qiskit.QuantumCircuit(n_qubits)\n",
    "        self.thetas = { k: Parameter('Theta'+str(k)) for k in range(self.n_qubits) }\n",
    "        \n",
    "        all_qubits = [i for i in range(n_qubits)]\n",
    "        \n",
    "        self.circuit.h(all_qubits)\n",
    "        self.circuit.barrier()\n",
    "        for k in range(n_qubits):\n",
    "            self.circuit.ry(self.thetas[k], k)\n",
    "            \n",
    "        self.circuit.measure_all()\n",
    "        \n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "        \n",
    "    def N_qubits_expectation_Z(self, counts, shots, n_qubits):\n",
    "        expects = np.zeros(len(QC_OUTPUTS))\n",
    "        for k in range(len(QC_OUTPUTS)):\n",
    "            key = QC_OUTPUTS[k]\n",
    "            perc = counts.get(key, 0) / shots\n",
    "            expects[k] = perc\n",
    "        return expects\n",
    "    \n",
    "    def run(self, i):\n",
    "        params = i\n",
    "        \n",
    "        job_sim = execute(self.circuit, \n",
    "                          self.backend, \n",
    "                          shots = self.shots, \n",
    "                          parameter_binds = [{ self.thetas[k]: params[k].item() for k in range(self.n_qubits) }])\n",
    "        \n",
    "        result_sim = job_sim.result()\n",
    "        counts = result_sim.get_counts(self.circuit)\n",
    "        \n",
    "        return self.N_qubits_expectation_Z(counts, self.shots, self.n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value for rotation [pi/4]: [4.000e-04 1.400e-03 3.200e-03 1.540e-02 3.400e-03 1.420e-02 1.660e-02\n",
      " 9.020e-02 3.000e-03 1.540e-02 1.920e-02 8.860e-02 1.440e-02 9.180e-02\n",
      " 9.120e-02 5.316e-01]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 598.479x325.08 with 1 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, NUM_SHOTS)\n",
    "print(f'Expected value for rotation [pi/4]: {circuit.run(torch.Tensor([np.pi/4] * len(QC_OUTPUTS)))}')\n",
    "circuit.circuit.draw(output='mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCircuit(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        if not hasattr(ctx, 'QuantumCircuit'):\n",
    "            ctx.QuantumCircuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, shots=NUM_SHOTS)\n",
    "        \n",
    "        exp_value = ctx.QuantumCircuit.run(i)\n",
    "        \n",
    "        result = torch.Tensor([exp_value])\n",
    "        \n",
    "        ctx.save_for_backward(result, i)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        forward_tensor,i = ctx.saved_tensors\n",
    "        \n",
    "        input_numbers = i\n",
    "        gradients = torch.Tensor()\n",
    "        \n",
    "        for k in range(NUM_QUBITS):\n",
    "            shift_right = input_numbers.detach().clone()\n",
    "            shift_right[k] = shift_right[k] + SHIFT\n",
    "            \n",
    "            shift_left = input_numbers.detach().clone()\n",
    "            shift_left[k] = shift_left[k] - SHIFT\n",
    "            \n",
    "            expectation_right = ctx.QuantumCircuit.run(shift_right)\n",
    "            expectation_left = ctx.QuantumCircuit.run(shift_left)\n",
    "            \n",
    "            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])\n",
    "            gradient = gradient / torch.norm(gradient)\n",
    "            \n",
    "            gradients = torch.cat((gradients, gradient.float()))\n",
    "            \n",
    "        result = torch.Tensor(gradients)\n",
    "        \n",
    "        return (result.float() * grad_output.float()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 after quantum layer: tensor([[0.0000, 0.0010, 0.0022, 0.0144, 0.0038, 0.0168, 0.0122, 0.0854, 0.0024,\n",
      "         0.0160, 0.0166, 0.0976, 0.0152, 0.0884, 0.0920, 0.5360]],\n",
      "       grad_fn=<TorchCircuitBackward>)\n",
      "x.grad = tensor([-0.1798,  0.0370, -0.1861, -0.0964])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "qc = TorchCircuit.apply\n",
    "\n",
    "y1 = qc(x)\n",
    "print(f'y1 after quantum layer: {y1}')\n",
    "y1 = nn.Linear(2**NUM_QUBITS, 1)(y1.float())\n",
    "\n",
    "y1.backward()\n",
    "print(f'x.grad = {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:35<00:00,  2.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13f0946a0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qc = TorchCircuit.apply\n",
    "\n",
    "def cost(x):\n",
    "    target = -1\n",
    "    expval = qc(x)[0]\n",
    "    \n",
    "    val = sum([(i + 1) * expval[i] for i in range(2**NUM_QUBITS)]) / 2**NUM_QUBITS\n",
    "    \n",
    "    return torch.abs(target - val) ** 2, expval\n",
    "\n",
    "\n",
    "x = torch.tensor([-np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "opt = optim.Adam([x], lr = 0.1)\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "loss_list = []\n",
    "expval_list = []\n",
    "\n",
    "for i in tqdm(range(num_epoch)):\n",
    "    opt.zero_grad()\n",
    "    loss, expval = cost(x)\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()\n",
    "    loss_list.append(loss.item())\n",
    "    expval_list.append(expval)\n",
    "    \n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "n_samples = 300\n",
    "\n",
    "X_train = datasets.CIFAR10(root='./data', \n",
    "                                train=True,\n",
    "                                download=True,\n",
    "                                transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "\n",
    "idx = np.append(np.where(np.array(X_train.targets) == 0)[0][:n_samples], \n",
    "                np.where(np.array(X_train.targets) == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 3)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 4)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 5)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 6)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 7)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 8)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 9)[0][:n_samples])\n",
    "\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = np.array(X_train.targets)[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "n_samples = 50\n",
    "\n",
    "X_test = datasets.CIFAR10(root='./data', \n",
    "                               train=False,\n",
    "                               download=True,\n",
    "                               transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "\n",
    "idx = np.append(np.where(np.array(X_test.targets) == 0)[0][:n_samples], \n",
    "                np.where(np.array(X_test.targets) == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 3)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 4)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 5)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 6)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 7)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 8)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 9)[0][:n_samples])\n",
    "\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = np.array(X_test.targets)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(X_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.fc4 = nn.Linear(10, 10)\n",
    "#         self.fc3 = nn.Linear(84, NUM_QUBITS)\n",
    "#         self.fc4 = nn.Linear(NUM_QUBITS, 10)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "#         x = np.pi*torch.tanh(x)\n",
    "#         x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "#         x = torch.sigmoid(x)\n",
    "#         x = torch.cat((x, 1-x), -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        pred = self.forward(x)\n",
    "        ans = torch.argmax(pred[0]).item()\n",
    "        \n",
    "        return torch.tensor(ans)\n",
    "    \n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, optimizer, train_loader, val_loader):\n",
    "    epochs = 30\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader\n",
    "    }\n",
    "    \n",
    "    loss_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    acc_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': test_loader\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "        total_acc = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                network.train(True)\n",
    "            else:\n",
    "                network.train(False)\n",
    "\n",
    "            correct = 0\n",
    "            number = 0\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(loaders[phase]):\n",
    "                optimizer.zero_grad()\n",
    "                output = network(data)\n",
    "\n",
    "                loss = loss_func(output, target)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                total_loss[phase].append(loss.item())\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                number += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "            loss_results[phase].append(sum(total_loss[phase]) / len(total_loss[phase]))\n",
    "            acc_results[phase].append(100 * correct / number)\n",
    "\n",
    "        progress = 100.0 * (epoch + 1) / epochs\n",
    "\n",
    "        print(f'Training [{int(progress)}%]\\tTraining loss {loss_results[\"train\"][-1]}\\tValidation loss {loss_results[\"val\"][-1]}\\tTraining acc {acc_results[\"train\"][-1]}\\tValidation acc {acc_results[\"val\"][-1]}')\n",
    "        \n",
    "    return {\n",
    "        'train_loss': loss_results[\"train\"][-1],\n",
    "        'val_loss': loss_results[\"val\"][-1],\n",
    "        'train_acc': acc_results[\"train\"][-1],\n",
    "        'val_acc': acc_results[\"val\"][-1]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Training [3%]\tTraining loss 2.2734614028675217\tValidation loss 2.1939626202583313\tTraining acc 11.821428571428571\tValidation acc 17.4\n",
      "Training [6%]\tTraining loss 2.074107958291258\tValidation loss 1.9345608102977276\tTraining acc 20.321428571428573\tValidation acc 26.8\n",
      "Training [10%]\tTraining loss 1.924936352428049\tValidation loss 1.8552465957850217\tTraining acc 26.214285714285715\tValidation acc 30.4\n",
      "Training [13%]\tTraining loss 1.8263987253912326\tValidation loss 1.738277000427246\tTraining acc 31.107142857142858\tValidation acc 35.4\n",
      "Training [16%]\tTraining loss 1.7404086493710722\tValidation loss 1.6372038019001485\tTraining acc 35.07142857142857\tValidation acc 39.8\n",
      "Training [20%]\tTraining loss 1.6604210603150673\tValidation loss 1.6410088605731725\tTraining acc 36.92857142857143\tValidation acc 36.2\n",
      "Training [23%]\tTraining loss 1.5959476590169859\tValidation loss 1.59308435510844\tTraining acc 41.035714285714285\tValidation acc 39.8\n",
      "Training [26%]\tTraining loss 1.5198582711393829\tValidation loss 1.4270042289681732\tTraining acc 42.42857142857143\tValidation acc 46.6\n",
      "Training [30%]\tTraining loss 1.4640244969539968\tValidation loss 1.4973403911494534\tTraining acc 46.142857142857146\tValidation acc 44.8\n",
      "Training [33%]\tTraining loss 1.3858826777268223\tValidation loss 1.3175630400590599\tTraining acc 47.214285714285715\tValidation acc 50.4\n",
      "Training [36%]\tTraining loss 1.302168720659962\tValidation loss 1.2825967708844692\tTraining acc 50.32142857142857\tValidation acc 54.0\n",
      "Training [40%]\tTraining loss 1.2330114061662738\tValidation loss 1.186994065480074\tTraining acc 53.607142857142854\tValidation acc 56.8\n",
      "Training [43%]\tTraining loss 1.150344375566077\tValidation loss 1.1444100345091355\tTraining acc 57.785714285714285\tValidation acc 58.0\n",
      "Training [46%]\tTraining loss 1.138217685230149\tValidation loss 1.1351949868814555\tTraining acc 59.285714285714285\tValidation acc 60.0\n",
      "Training [50%]\tTraining loss 1.038261062873966\tValidation loss 1.1805995426947764\tTraining acc 62.25\tValidation acc 62.2\n",
      "Training [53%]\tTraining loss 1.032816254720932\tValidation loss 1.188253674312653\tTraining acc 64.42857142857143\tValidation acc 60.6\n",
      "Training [56%]\tTraining loss 0.966462048673623\tValidation loss 1.0797673990399486\tTraining acc 65.71428571428571\tValidation acc 65.2\n",
      "Training [60%]\tTraining loss 0.9686202296248189\tValidation loss 1.1633115023651626\tTraining acc 66.35714285714286\tValidation acc 63.8\n",
      "Training [63%]\tTraining loss 0.8682773285943965\tValidation loss 1.2802927746092436\tTraining acc 69.82142857142857\tValidation acc 64.0\n",
      "Training [66%]\tTraining loss 0.8621447323497036\tValidation loss 1.1387163301118655\tTraining acc 70.21428571428571\tValidation acc 68.4\n",
      "Training [70%]\tTraining loss 0.8915945832008273\tValidation loss 1.0824819827326413\tTraining acc 71.10714285714286\tValidation acc 68.0\n",
      "Training [73%]\tTraining loss 1.004792598191641\tValidation loss 1.2885646965910624\tTraining acc 66.14285714285714\tValidation acc 60.4\n",
      "Training [76%]\tTraining loss 0.8333236540225492\tValidation loss 1.4870276280028196\tTraining acc 71.85714285714286\tValidation acc 61.6\n",
      "Training [80%]\tTraining loss 0.7714583970844249\tValidation loss 1.1250339821406816\tTraining acc 74.53571428571429\tValidation acc 68.8\n",
      "Training [83%]\tTraining loss 0.8395269002338832\tValidation loss 1.0960794633410755\tTraining acc 74.39285714285714\tValidation acc 69.2\n",
      "Training [86%]\tTraining loss 0.8628383587758953\tValidation loss 1.159840481186766\tTraining acc 73.07142857142857\tValidation acc 69.0\n",
      "Training [90%]\tTraining loss 0.7931549069826549\tValidation loss 1.2372438777647208\tTraining acc 74.60714285714286\tValidation acc 68.4\n",
      "Training [93%]\tTraining loss 0.8936481672914643\tValidation loss 1.2925725433849113\tTraining acc 71.14285714285714\tValidation acc 67.8\n",
      "Training [96%]\tTraining loss 0.7846585600338121\tValidation loss 1.2914920624743123\tTraining acc 75.5\tValidation acc 66.4\n",
      "Training [100%]\tTraining loss 0.7860698715967753\tValidation loss 1.442564193239048\tTraining acc 76.57142857142857\tValidation acc 68.0\n",
      "{'train_loss': 0.7860698715967753, 'val_loss': 1.442564193239048, 'train_acc': 76.57142857142857, 'val_acc': 68.0}\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Training [3%]\tTraining loss 2.307120269536972\tValidation loss 2.3032982892990113\tTraining acc 9.714285714285714\tValidation acc 16.0\n",
      "Training [6%]\tTraining loss 2.267199376246759\tValidation loss 2.202770471453667\tTraining acc 14.214285714285714\tValidation acc 16.6\n",
      "Training [10%]\tTraining loss 2.1083882025203535\tValidation loss 2.0739383730888368\tTraining acc 18.25\tValidation acc 18.0\n",
      "Training [13%]\tTraining loss 2.0033039615409716\tValidation loss 1.9632594207525254\tTraining acc 19.678571428571427\tValidation acc 23.2\n",
      "Training [16%]\tTraining loss 1.9432333089171776\tValidation loss 1.8854306793808937\tTraining acc 22.821428571428573\tValidation acc 24.4\n",
      "Training [20%]\tTraining loss 1.8687146515785051\tValidation loss 1.7875489227175712\tTraining acc 27.857142857142858\tValidation acc 32.2\n",
      "Training [23%]\tTraining loss 1.7968663386441768\tValidation loss 2.213588302746415\tTraining acc 29.821428571428573\tValidation acc 23.2\n",
      "Training [26%]\tTraining loss 1.73653229725148\tValidation loss 1.6652572447657585\tTraining acc 31.964285714285715\tValidation acc 35.2\n",
      "Training [30%]\tTraining loss 1.634374941691224\tValidation loss 1.5159681365489959\tTraining acc 37.964285714285715\tValidation acc 39.4\n",
      "Training [33%]\tTraining loss 1.5759734756725707\tValidation loss 1.4567089473307133\tTraining acc 38.67857142857143\tValidation acc 42.0\n",
      "Training [36%]\tTraining loss 1.5118680980222832\tValidation loss 1.520436646461487\tTraining acc 41.57142857142857\tValidation acc 43.8\n",
      "Training [40%]\tTraining loss 1.4431708502579879\tValidation loss 1.3813196427971124\tTraining acc 44.035714285714285\tValidation acc 45.6\n",
      "Training [43%]\tTraining loss 1.4035863125793855\tValidation loss 1.3100006610043347\tTraining acc 44.857142857142854\tValidation acc 47.2\n",
      "Training [46%]\tTraining loss 1.3481504407494815\tValidation loss 1.2932380950404332\tTraining acc 49.32142857142857\tValidation acc 49.0\n",
      "Training [50%]\tTraining loss 1.3037042118910669\tValidation loss 1.248956004270076\tTraining acc 49.92857142857143\tValidation acc 50.2\n",
      "Training [53%]\tTraining loss 1.275012478307738\tValidation loss 1.2650419114393008\tTraining acc 53.07142857142857\tValidation acc 52.6\n",
      "Training [56%]\tTraining loss 1.1829477723740949\tValidation loss 1.2092904364983041\tTraining acc 55.214285714285715\tValidation acc 55.6\n",
      "Training [60%]\tTraining loss 1.1270915154653898\tValidation loss 1.375350575812161\tTraining acc 57.67857142857143\tValidation acc 54.6\n",
      "Training [63%]\tTraining loss 1.1271755459143127\tValidation loss 1.3114848884652304\tTraining acc 58.464285714285715\tValidation acc 57.4\n",
      "Training [66%]\tTraining loss 1.039456008372863\tValidation loss 1.1584870800141835\tTraining acc 61.214285714285715\tValidation acc 58.0\n",
      "Training [70%]\tTraining loss 1.047323425547211\tValidation loss 1.2544453678048266\tTraining acc 61.0\tValidation acc 56.6\n",
      "Training [73%]\tTraining loss 1.0559559555288882\tValidation loss 1.3578661966639847\tTraining acc 62.42857142857143\tValidation acc 54.8\n",
      "Training [76%]\tTraining loss 0.949986388951773\tValidation loss 1.1994596961438975\tTraining acc 66.35714285714286\tValidation acc 63.0\n",
      "Training [80%]\tTraining loss 1.0611023963744162\tValidation loss 1.1509815729958355\tTraining acc 64.10714285714286\tValidation acc 60.0\n",
      "Training [83%]\tTraining loss 0.9227109632730641\tValidation loss 1.0055961375810036\tTraining acc 68.17857142857143\tValidation acc 68.8\n",
      "Training [86%]\tTraining loss 0.8444119977189409\tValidation loss 1.2423136907319707\tTraining acc 71.53571428571429\tValidation acc 61.4\n",
      "Training [90%]\tTraining loss 0.8509965953978108\tValidation loss 0.9968532292597329\tTraining acc 71.21428571428571\tValidation acc 69.2\n",
      "Training [93%]\tTraining loss 0.8390320515086563\tValidation loss 1.7986060800487602\tTraining acc 72.96428571428571\tValidation acc 57.0\n",
      "Training [96%]\tTraining loss 0.8820542887662959\tValidation loss 0.9749716787271334\tTraining acc 71.92857142857143\tValidation acc 70.4\n",
      "Training [100%]\tTraining loss 0.8264970329639336\tValidation loss 0.9819843961291063\tTraining acc 73.53571428571429\tValidation acc 69.8\n",
      "{'train_loss': 0.8264970329639336, 'val_loss': 0.9819843961291063, 'train_acc': 73.53571428571429, 'val_acc': 69.8}\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [3%]\tTraining loss 2.305178659047399\tValidation loss 2.298376004219055\tTraining acc 10.857142857142858\tValidation acc 15.8\n",
      "Training [6%]\tTraining loss 2.1849917121018683\tValidation loss 2.050266006350517\tTraining acc 15.964285714285714\tValidation acc 17.0\n",
      "Training [10%]\tTraining loss 2.0171129928742135\tValidation loss 1.9555228182077409\tTraining acc 21.5\tValidation acc 22.8\n",
      "Training [13%]\tTraining loss 1.932702493076878\tValidation loss 1.8438485100865365\tTraining acc 24.428571428571427\tValidation acc 27.6\n",
      "Training [16%]\tTraining loss 1.847771948293916\tValidation loss 1.7419115746915341\tTraining acc 26.785714285714285\tValidation acc 28.8\n",
      "Training [20%]\tTraining loss 1.7605206552227692\tValidation loss 1.7055442801713943\tTraining acc 31.464285714285715\tValidation acc 36.0\n",
      "Training [23%]\tTraining loss 1.699194586854428\tValidation loss 1.6300737471282483\tTraining acc 33.607142857142854\tValidation acc 36.2\n",
      "Training [26%]\tTraining loss 1.6247044500149785\tValidation loss 1.7348165496885777\tTraining acc 38.5\tValidation acc 35.0\n",
      "Training [30%]\tTraining loss 1.562020328895721\tValidation loss 1.5450639878362418\tTraining acc 41.357142857142854\tValidation acc 41.0\n",
      "Training [33%]\tTraining loss 1.4891935285559572\tValidation loss 1.5855946840513497\tTraining acc 43.92857142857143\tValidation acc 42.6\n",
      "Training [36%]\tTraining loss 1.420554021716837\tValidation loss 1.3711758790891617\tTraining acc 47.892857142857146\tValidation acc 52.2\n",
      "Training [40%]\tTraining loss 1.367598213551568\tValidation loss 1.2136138833388685\tTraining acc 48.142857142857146\tValidation acc 54.8\n",
      "Training [43%]\tTraining loss 1.3353092517121552\tValidation loss 1.6699061332251877\tTraining acc 50.607142857142854\tValidation acc 43.0\n",
      "Training [46%]\tTraining loss 1.2676793253921226\tValidation loss 1.2221115025670732\tTraining acc 53.107142857142854\tValidation acc 55.4\n",
      "Training [50%]\tTraining loss 1.1709095056290848\tValidation loss 1.184165266528871\tTraining acc 55.67857142857143\tValidation acc 57.4\n",
      "Training [53%]\tTraining loss 1.1405680122734212\tValidation loss 1.1447638524325447\tTraining acc 57.285714285714285\tValidation acc 55.6\n",
      "Training [56%]\tTraining loss 1.1258891334184742\tValidation loss 1.199217277278046\tTraining acc 59.857142857142854\tValidation acc 58.8\n",
      "Training [60%]\tTraining loss 1.074068343807538\tValidation loss 1.1242184890674836\tTraining acc 60.535714285714285\tValidation acc 60.4\n",
      "Training [63%]\tTraining loss 1.0561227890568543\tValidation loss 0.96574838896652\tTraining acc 62.75\tValidation acc 66.2\n",
      "Training [66%]\tTraining loss 1.0423565275911821\tValidation loss 1.1104997756919688\tTraining acc 63.142857142857146\tValidation acc 60.6\n",
      "Training [70%]\tTraining loss 0.9857813017727243\tValidation loss 0.9999338202372637\tTraining acc 65.53571428571429\tValidation acc 67.4\n",
      "Training [73%]\tTraining loss 0.952857713077453\tValidation loss 1.0024039026323353\tTraining acc 66.75\tValidation acc 67.6\n",
      "Training [76%]\tTraining loss 0.8582100015942882\tValidation loss 0.9973240517715513\tTraining acc 70.32142857142857\tValidation acc 67.2\n",
      "Training [80%]\tTraining loss 0.9104871079914806\tValidation loss 1.0634706898449642\tTraining acc 69.71428571428571\tValidation acc 66.6\n",
      "Training [83%]\tTraining loss 0.8877753059819892\tValidation loss 1.2276447789034624\tTraining acc 69.67857142857143\tValidation acc 63.0\n",
      "Training [86%]\tTraining loss 0.8769068669944594\tValidation loss 1.225365475883406\tTraining acc 69.53571428571429\tValidation acc 69.0\n",
      "Training [90%]\tTraining loss 0.9179172942473518\tValidation loss 1.1533251659298756\tTraining acc 69.78571428571429\tValidation acc 64.8\n",
      "Training [93%]\tTraining loss 0.7855023476219516\tValidation loss 0.9638123106059568\tTraining acc 73.67857142857143\tValidation acc 69.6\n",
      "Training [96%]\tTraining loss 0.7470610959316015\tValidation loss 0.8974402748155285\tTraining acc 76.28571428571429\tValidation acc 72.4\n",
      "Training [100%]\tTraining loss 0.7317955577465419\tValidation loss 0.9251587593056035\tTraining acc 76.10714285714286\tValidation acc 74.2\n",
      "{'train_loss': 0.7317955577465419, 'val_loss': 0.9251587593056035, 'train_acc': 76.10714285714286, 'val_acc': 74.2}\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Training [3%]\tTraining loss 2.243577731315579\tValidation loss 2.058234117984772\tTraining acc 14.464285714285714\tValidation acc 23.0\n",
      "Training [6%]\tTraining loss 2.0407093206952727\tValidation loss 1.9074768367409707\tTraining acc 22.785714285714285\tValidation acc 26.2\n",
      "Training [10%]\tTraining loss 1.92961501661422\tValidation loss 1.8351128885149957\tTraining acc 24.964285714285715\tValidation acc 28.8\n",
      "Training [13%]\tTraining loss 1.835086612602962\tValidation loss 1.772446343548596\tTraining acc 30.571428571428573\tValidation acc 31.4\n",
      "Training [16%]\tTraining loss 1.731513891555369\tValidation loss 1.7747271184623241\tTraining acc 35.214285714285715\tValidation acc 34.6\n",
      "Training [20%]\tTraining loss 1.649128057926095\tValidation loss 1.676990676805377\tTraining acc 38.714285714285715\tValidation acc 40.4\n",
      "Training [23%]\tTraining loss 1.5727041240322952\tValidation loss 1.6141803216990083\tTraining acc 41.5\tValidation acc 41.0\n",
      "Training [26%]\tTraining loss 1.5082506682741588\tValidation loss 1.5141149761956185\tTraining acc 44.107142857142854\tValidation acc 41.4\n",
      "Training [30%]\tTraining loss 1.450636267780397\tValidation loss 1.5846884016729892\tTraining acc 46.107142857142854\tValidation acc 42.4\n",
      "Training [33%]\tTraining loss 1.3855664694053742\tValidation loss 1.358724330344936\tTraining acc 47.785714285714285\tValidation acc 50.0\n",
      "Training [36%]\tTraining loss 1.3180169083027964\tValidation loss 1.314887240276672\tTraining acc 50.17857142857143\tValidation acc 48.6\n",
      "Training [40%]\tTraining loss 1.2413283843580134\tValidation loss 1.238454071429558\tTraining acc 52.92857142857143\tValidation acc 54.0\n",
      "Training [43%]\tTraining loss 1.2016649970431017\tValidation loss 1.414519412110135\tTraining acc 54.035714285714285\tValidation acc 48.6\n",
      "Training [46%]\tTraining loss 1.1530434979804103\tValidation loss 1.1277943539931439\tTraining acc 57.714285714285715\tValidation acc 56.2\n",
      "Training [50%]\tTraining loss 1.048464531962524\tValidation loss 1.0814080487070896\tTraining acc 62.32142857142857\tValidation acc 61.0\n",
      "Training [53%]\tTraining loss 1.0548919108820374\tValidation loss 1.1915809932184567\tTraining acc 60.857142857142854\tValidation acc 62.6\n",
      "Training [56%]\tTraining loss 1.0147911118848727\tValidation loss 1.1035509295731025\tTraining acc 63.07142857142857\tValidation acc 60.2\n",
      "Training [60%]\tTraining loss 0.9553549793557605\tValidation loss 1.1450818880796851\tTraining acc 66.0\tValidation acc 63.2\n",
      "Training [63%]\tTraining loss 0.9571412021750526\tValidation loss 1.5315167806914247\tTraining acc 66.32142857142857\tValidation acc 50.4\n",
      "Training [66%]\tTraining loss 0.9171652718720071\tValidation loss 1.1291074973053183\tTraining acc 68.42857142857143\tValidation acc 63.4\n",
      "Training [70%]\tTraining loss 0.8622783643220145\tValidation loss 1.0173845818659337\tTraining acc 69.10714285714286\tValidation acc 65.6\n",
      "Training [73%]\tTraining loss 0.8602390266227264\tValidation loss 1.056975975987355\tTraining acc 71.14285714285714\tValidation acc 68.4\n",
      "Training [76%]\tTraining loss 0.8413469345522255\tValidation loss 1.0962219714491148\tTraining acc 69.07142857142857\tValidation acc 68.2\n",
      "Training [80%]\tTraining loss 0.8990623606453318\tValidation loss 0.9700900445280618\tTraining acc 70.03571428571429\tValidation acc 71.0\n",
      "Training [83%]\tTraining loss 0.7548309813849853\tValidation loss 1.1400336439775096\tTraining acc 74.32142857142857\tValidation acc 68.4\n",
      "Training [86%]\tTraining loss 0.8761163467123764\tValidation loss 1.0553006548805361\tTraining acc 72.17857142857143\tValidation acc 68.6\n",
      "Training [90%]\tTraining loss 0.8143955026870445\tValidation loss 1.1246909602860877\tTraining acc 72.28571428571429\tValidation acc 67.8\n",
      "Training [93%]\tTraining loss 0.690826338455165\tValidation loss 1.2342346235079338\tTraining acc 77.57142857142857\tValidation acc 65.6\n",
      "Training [96%]\tTraining loss 0.834083774642291\tValidation loss 1.4219554296747288\tTraining acc 72.92857142857143\tValidation acc 57.0\n",
      "Training [100%]\tTraining loss 0.9328974618435385\tValidation loss 1.151287755007669\tTraining acc 69.60714285714286\tValidation acc 71.0\n",
      "{'train_loss': 0.9328974618435385, 'val_loss': 1.151287755007669, 'train_acc': 69.60714285714286, 'val_acc': 71.0}\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [3%]\tTraining loss 2.2429642180672715\tValidation loss 2.1323945751190188\tTraining acc 14.214285714285714\tValidation acc 16.2\n",
      "Training [6%]\tTraining loss 2.041735059203846\tValidation loss 1.9861845537424088\tTraining acc 21.857142857142858\tValidation acc 26.6\n",
      "Training [10%]\tTraining loss 1.9298251427390745\tValidation loss 1.89425942581892\tTraining acc 27.571428571428573\tValidation acc 28.6\n",
      "Training [13%]\tTraining loss 1.8365538463635105\tValidation loss 1.751103399068117\tTraining acc 30.642857142857142\tValidation acc 35.4\n",
      "Training [16%]\tTraining loss 1.743724019789536\tValidation loss 1.6380770611166955\tTraining acc 32.82142857142857\tValidation acc 39.0\n",
      "Training [20%]\tTraining loss 1.6772011740640405\tValidation loss 1.6352805384248494\tTraining acc 38.214285714285715\tValidation acc 41.8\n",
      "Training [23%]\tTraining loss 1.6157041843875777\tValidation loss 1.5211905511319637\tTraining acc 39.142857142857146\tValidation acc 44.0\n",
      "Training [26%]\tTraining loss 1.5476791402256536\tValidation loss 1.4714952782932669\tTraining acc 42.25\tValidation acc 44.8\n",
      "Training [30%]\tTraining loss 1.4865294086047185\tValidation loss 1.461601343705901\tTraining acc 44.035714285714285\tValidation acc 46.2\n",
      "Training [33%]\tTraining loss 1.4499021207023854\tValidation loss 1.3550662882788456\tTraining acc 47.035714285714285\tValidation acc 47.8\n",
      "Training [36%]\tTraining loss 1.377485592639874\tValidation loss 1.3034804042058532\tTraining acc 48.642857142857146\tValidation acc 53.6\n",
      "Training [40%]\tTraining loss 1.3035233132754829\tValidation loss 1.423962887403839\tTraining acc 52.32142857142857\tValidation acc 48.8\n",
      "Training [43%]\tTraining loss 1.2289336554552361\tValidation loss 1.295021735112241\tTraining acc 54.607142857142854\tValidation acc 54.8\n",
      "Training [46%]\tTraining loss 1.2112146920612188\tValidation loss 1.2132232537306553\tTraining acc 56.214285714285715\tValidation acc 57.0\n",
      "Training [50%]\tTraining loss 1.145404208877215\tValidation loss 1.177372569298539\tTraining acc 59.214285714285715\tValidation acc 58.4\n",
      "Training [53%]\tTraining loss 1.1212203398487146\tValidation loss 1.3372228993860753\tTraining acc 60.607142857142854\tValidation acc 55.0\n",
      "Training [56%]\tTraining loss 1.0551112117120007\tValidation loss 1.1723065241541477\tTraining acc 62.357142857142854\tValidation acc 61.4\n",
      "Training [60%]\tTraining loss 1.1030281073083035\tValidation loss 1.144524732803643\tTraining acc 61.25\tValidation acc 60.8\n",
      "Training [63%]\tTraining loss 0.982733889772886\tValidation loss 1.1705236459725947\tTraining acc 64.82142857142857\tValidation acc 62.4\n",
      "Training [66%]\tTraining loss 0.937579618959415\tValidation loss 1.1416907888202958\tTraining acc 67.53571428571429\tValidation acc 61.6\n",
      "Training [70%]\tTraining loss 0.944631633304122\tValidation loss 1.088819272623767\tTraining acc 67.25\tValidation acc 66.4\n",
      "Training [73%]\tTraining loss 0.9246758817413905\tValidation loss 1.260468636053538\tTraining acc 68.96428571428571\tValidation acc 62.6\n",
      "Training [76%]\tTraining loss 0.9601959092331395\tValidation loss 1.1457108436538037\tTraining acc 67.64285714285714\tValidation acc 66.4\n",
      "Training [80%]\tTraining loss 0.8851764501587847\tValidation loss 1.108315249766046\tTraining acc 70.10714285714286\tValidation acc 67.2\n",
      "Training [83%]\tTraining loss 0.8961368365771146\tValidation loss 1.0848450156769125\tTraining acc 70.60714285714286\tValidation acc 65.4\n",
      "Training [86%]\tTraining loss 0.8478896867217461\tValidation loss 1.0869188500828872\tTraining acc 72.71428571428571\tValidation acc 67.8\n",
      "Training [90%]\tTraining loss 0.8415035604337582\tValidation loss 1.4517005220575108\tTraining acc 73.17857142857143\tValidation acc 58.6\n",
      "Training [93%]\tTraining loss 0.944811455200467\tValidation loss 1.1944782563806007\tTraining acc 70.57142857142857\tValidation acc 66.6\n",
      "Training [96%]\tTraining loss 0.8792054183499788\tValidation loss 1.630028995395286\tTraining acc 72.46428571428571\tValidation acc 58.4\n",
      "Training [100%]\tTraining loss 0.9676881640782267\tValidation loss 1.1105692062918429\tTraining acc 69.46428571428571\tValidation acc 67.6\n",
      "{'train_loss': 0.9676881640782267, 'val_loss': 1.1105692062918429, 'train_acc': 69.46428571428571, 'val_acc': 67.6}\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "FINAL RESULTS\n",
      "--------------------------------\n",
      "{0: {'train_loss': 0.7860698715967753, 'val_loss': 1.442564193239048, 'train_acc': 76.57142857142857, 'val_acc': 68.0}, 1: {'train_loss': 0.8264970329639336, 'val_loss': 0.9819843961291063, 'train_acc': 73.53571428571429, 'val_acc': 69.8}, 2: {'train_loss': 0.7317955577465419, 'val_loss': 0.9251587593056035, 'train_acc': 76.10714285714286, 'val_acc': 74.2}, 3: {'train_loss': 0.9328974618435385, 'val_loss': 1.151287755007669, 'train_acc': 69.60714285714286, 'val_acc': 71.0}, 4: {'train_loss': 0.9676881640782267, 'val_loss': 1.1105692062918429, 'train_acc': 69.46428571428571, 'val_acc': 67.6}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "\n",
    "dataset = ConcatDataset([X_train, X_test])\n",
    "\n",
    "k_folds = 5\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "results = {}\n",
    "\n",
    "print('--------------------------------')\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "  \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=1, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=1, sampler=test_subsampler)\n",
    "\n",
    "    network = Net()\n",
    "    network.apply(reset_weights)\n",
    "    \n",
    "    optimizer = optim.SGD(network.parameters(), lr =0.001, momentum=0.9)\n",
    "    \n",
    "    result = train_network(network, optimizer, trainloader, testloader)\n",
    "    \n",
    "    results[fold] = result\n",
    "    \n",
    "    print(result)\n",
    "    print('--------------------------------')\n",
    "\n",
    "print('--------------------------------')\n",
    "print('FINAL RESULTS')\n",
    "print('--------------------------------')\n",
    "print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 30\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# loss_results = {\n",
    "#     'train': [],\n",
    "#     'val': []\n",
    "# }\n",
    "\n",
    "# acc_results = {\n",
    "#     'train': [],\n",
    "#     'val': []\n",
    "# }\n",
    "\n",
    "# loaders = {\n",
    "#     'train': train_loader,\n",
    "#     'val': test_loader\n",
    "# }\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = {\n",
    "#         'train': [],\n",
    "#         'val': []\n",
    "#     }\n",
    "    \n",
    "#     total_acc = {\n",
    "#         'train': [],\n",
    "#         'val': []\n",
    "#     }\n",
    "    \n",
    "    \n",
    "#     for phase in ['train', 'val']:\n",
    "#         if phase == 'train':\n",
    "#             network.train(True)\n",
    "#         else:\n",
    "#             network.train(False)\n",
    "        \n",
    "#         correct = 0\n",
    "#         number = 0\n",
    "        \n",
    "#         for batch_idx, (data, target) in enumerate(loaders[phase]):\n",
    "#             optimizer.zero_grad()\n",
    "#             output = network(data)\n",
    "            \n",
    "#             loss = loss_func(output, target)\n",
    "            \n",
    "#             if phase == 'train':\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             total_loss[phase].append(loss.item())\n",
    "\n",
    "#             _, predicted = torch.max(output.data, 1)\n",
    "#             number += target.size(0)\n",
    "#             correct += (predicted == target).sum().item()\n",
    "            \n",
    "#         loss_results[phase].append(sum(total_loss[phase]) / len(total_loss[phase]))\n",
    "#         acc_results[phase].append(100 * correct / number)\n",
    "\n",
    "#     progress = 100.0 * (epoch + 1) / epochs\n",
    "    \n",
    "#     print(f'Training [{int(progress)}%]\\tTraining loss {loss_results[\"train\"][-1]}\\tValidation loss {loss_results[\"val\"][-1]}\\tTraining acc {acc_results[\"train\"][-1]}\\tValidation acc {acc_results[\"val\"][-1]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_results[\"train\"])\n",
    "# plt.title(f'Hybrid NN training convergence for {NUM_QUBITS}-qubits')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_results[\"val\"])\n",
    "# plt.title(f'Hybrid NN training convergence for {NUM_QUBITS}-qubits')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = 0\n",
    "# number = 0\n",
    "# for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#     number += 1\n",
    "#     output = network.predict(data).item()\n",
    "#     accuracy += (output == target[0].item())*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Performance on test data: {accuracy}/{number} = {accuracy/number*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_shape = (8, 6)\n",
    "# count = 0\n",
    "# fig, axes = plt.subplots(nrows=n_sample_shape[0], ncols=n_sample_shape[1], figsize=(10, 2*n_sample_shape[0]))\n",
    "\n",
    "# classes_map = {\n",
    "#     0: 'airplane',\n",
    "#     1: 'automobile',\n",
    "#     2: 'bird',\n",
    "#     3: 'cat',\n",
    "#     4: 'deer',\n",
    "#     5: 'dog',\n",
    "#     6: 'frog',\n",
    "#     7: 'horse',\n",
    "#     8: 'ship',\n",
    "#     9: 'truck'\n",
    "# }\n",
    "\n",
    "# network.eval()\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#         if count == n_sample_shape[0]*n_sample_shape[1]:\n",
    "#             break\n",
    "        \n",
    "#         prediction = network.predict(data).item()\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].imshow(data[0].numpy().squeeze().swapaxes(0,1).swapaxes(1,2))\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_xticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_yticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_title(classes_map[prediction])\n",
    "        \n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
