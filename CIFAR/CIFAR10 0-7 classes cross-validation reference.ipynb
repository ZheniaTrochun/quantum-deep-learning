{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: qiskit in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.23.2)\n",
      "Requirement already satisfied: qiskit-aer==0.7.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.7.2)\n",
      "Requirement already satisfied: qiskit-ignis==0.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.5.1)\n",
      "Requirement already satisfied: qiskit-aqua==0.8.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.8.1)\n",
      "Requirement already satisfied: qiskit-terra==0.16.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.16.1)\n",
      "Requirement already satisfied: qiskit-ibmq-provider==0.11.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.11.1)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (1.5.2)\n",
      "Requirement already satisfied: cython>=0.27.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (0.29.21)\n",
      "Requirement already satisfied: numpy>=1.16.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (1.19.2)\n",
      "Requirement already satisfied: pybind11>=2.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (2.6.1)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ignis==0.5.1->qiskit) (50.3.0)\n",
      "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ignis==0.5.1->qiskit) (0.23.2)\n",
      "Requirement already satisfied: yfinance in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.1.55)\n",
      "Requirement already satisfied: docplex in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.19.202)\n",
      "Requirement already satisfied: dlx in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.0.4)\n",
      "Requirement already satisfied: retworkx>=0.5.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.7.2)\n",
      "Requirement already satisfied: h5py in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.10.0)\n",
      "Requirement already satisfied: sympy>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.7.1)\n",
      "Requirement already satisfied: pandas in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.1.3)\n",
      "Requirement already satisfied: quandl in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (3.6.0)\n",
      "Requirement already satisfied: fastdtw in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.3.4)\n",
      "Requirement already satisfied: psutil>=5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (5.8.0)\n",
      "Requirement already satisfied: ply>=3.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.11)\n",
      "Requirement already satisfied: python-constraint>=1.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.14.5)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (0.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.8.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.2.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.5)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.25.10)\n",
      "Requirement already satisfied: websockets>=8 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (8.1)\n",
      "Requirement already satisfied: requests-ntlm>=1.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.1.0)\n",
      "Requirement already satisfied: nest-asyncio!=1.1.0,>=1.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: requests>=2.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (2.24.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.17->qiskit-ignis==0.5.1->qiskit) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.17->qiskit-ignis==0.5.1->qiskit) (2.1.0)\n",
      "Requirement already satisfied: lxml>=4.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (4.6.2)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (0.0.9)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from docplex->qiskit-aqua==0.8.1->qiskit) (1.15.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from sympy>=1.3->qiskit-aqua==0.8.1->qiskit) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from pandas->qiskit-aqua==0.8.1->qiskit) (2020.1)\n",
      "Requirement already satisfied: inflection>=0.3.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (0.5.1)\n",
      "Requirement already satisfied: more-itertools in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (8.6.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (20.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (0.17.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from networkx>=2.2->qiskit-terra==0.16.1->qiskit) (4.4.2)\n",
      "Requirement already satisfied: ntlm-auth>=1.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.5.0)\n",
      "Requirement already satisfied: cryptography>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (3.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2020.6.20)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.14.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (2.20)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pylatexenc in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (2.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torchvision in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.8.2)\n",
      "Requirement already satisfied: torch==1.7.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch==1.7.1->torchvision) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (4.56.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for itertools\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install qiskit\n",
    "!{sys.executable} -m pip install pylatexenc\n",
    "!{sys.executable} -m pip install torchvision\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import execute\n",
    "from qiskit.circuit import Parameter,ControlledGate\n",
    "from qiskit import Aer\n",
    "import qiskit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "\n",
    "NUM_QUBITS = 3\n",
    "NUM_SHOTS = 5000\n",
    "SHIFT = np.pi/4\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "SIMULATOR = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '001', '010', '011', '100', '101', '110', '111']\n"
     ]
    }
   ],
   "source": [
    "# create list of all possible outputs of quantum circuit (2**NUM_QUBITS possible)\n",
    "import itertools\n",
    "\n",
    "def create_QC_OUTPUTS():\n",
    "    measurements = list(itertools.product([0, 1], repeat=NUM_QUBITS))\n",
    "    return [''.join([str(bit) for bit in measurement]) for measurement in measurements]\n",
    "\n",
    "QC_OUTPUTS = create_QC_OUTPUTS()\n",
    "print(QC_OUTPUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contruct QuantumCircuit QFT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumCircuit:\n",
    "    def __init__(self, n_qubits, backend, shots):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.circuit = qiskit.QuantumCircuit(n_qubits)\n",
    "        self.thetas = { k: Parameter('Theta'+str(k)) for k in range(self.n_qubits) }\n",
    "        \n",
    "        all_qubits = [i for i in range(n_qubits)]\n",
    "        \n",
    "        self.circuit.h(all_qubits)\n",
    "        self.circuit.barrier()\n",
    "        for k in range(n_qubits):\n",
    "            self.circuit.ry(self.thetas[k], k)\n",
    "            \n",
    "        self.circuit.measure_all()\n",
    "        \n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "        \n",
    "    def N_qubits_expectation_Z(self, counts, shots, n_qubits):\n",
    "        expects = np.zeros(len(QC_OUTPUTS))\n",
    "        for k in range(len(QC_OUTPUTS)):\n",
    "            key = QC_OUTPUTS[k]\n",
    "            perc = counts.get(key, 0) / shots\n",
    "            expects[k] = perc\n",
    "        return expects\n",
    "    \n",
    "    def run(self, i):\n",
    "        params = i\n",
    "        \n",
    "        job_sim = execute(self.circuit, \n",
    "                          self.backend, \n",
    "                          shots = self.shots, \n",
    "                          parameter_binds = [{ self.thetas[k]: params[k].item() for k in range(self.n_qubits) }])\n",
    "        \n",
    "        result_sim = job_sim.result()\n",
    "        counts = result_sim.get_counts(self.circuit)\n",
    "        \n",
    "        return self.N_qubits_expectation_Z(counts, self.shots, self.n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value for rotation [pi/4]: [0.0046 0.0166 0.019  0.1072 0.0178 0.1046 0.107  0.6232]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 538.279x264.88 with 1 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, NUM_SHOTS)\n",
    "print(f'Expected value for rotation [pi/4]: {circuit.run(torch.Tensor([np.pi/4] * len(QC_OUTPUTS)))}')\n",
    "circuit.circuit.draw(output='mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCircuit(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        if not hasattr(ctx, 'QuantumCircuit'):\n",
    "            ctx.QuantumCircuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, shots=NUM_SHOTS)\n",
    "        \n",
    "        exp_value = ctx.QuantumCircuit.run(i)\n",
    "        \n",
    "        result = torch.Tensor([exp_value])\n",
    "        \n",
    "        ctx.save_for_backward(result, i)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        forward_tensor,i = ctx.saved_tensors\n",
    "        \n",
    "        input_numbers = i\n",
    "        gradients = torch.Tensor()\n",
    "        \n",
    "        for k in range(NUM_QUBITS):\n",
    "            shift_right = input_numbers.detach().clone()\n",
    "            shift_right[k] = shift_right[k] + SHIFT\n",
    "            \n",
    "            shift_left = input_numbers.detach().clone()\n",
    "            shift_left[k] = shift_left[k] - SHIFT\n",
    "            \n",
    "            expectation_right = ctx.QuantumCircuit.run(shift_right)\n",
    "            expectation_left = ctx.QuantumCircuit.run(shift_left)\n",
    "            \n",
    "            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])\n",
    "            gradient = gradient / torch.norm(gradient)\n",
    "            \n",
    "            gradients = torch.cat((gradients, gradient.float()))\n",
    "            \n",
    "        result = torch.Tensor(gradients)\n",
    "        \n",
    "        return (result.float() * grad_output.float()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 after quantum layer: tensor([[0.0044, 0.0172, 0.0178, 0.1038, 0.0190, 0.1074, 0.1078, 0.6226]],\n",
      "       grad_fn=<TorchCircuitBackward>)\n",
      "x.grad = tensor([ 0.0044, -0.2280, -0.0816])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "qc = TorchCircuit.apply\n",
    "\n",
    "y1 = qc(x)\n",
    "print(f'y1 after quantum layer: {y1}')\n",
    "y1 = nn.Linear(2**NUM_QUBITS, 1)(y1.float())\n",
    "\n",
    "y1.backward()\n",
    "print(f'x.grad = {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:24<00:00,  4.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13c7a0cd0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qc = TorchCircuit.apply\n",
    "\n",
    "def cost(x):\n",
    "    target = -1\n",
    "    expval = qc(x)[0]\n",
    "    \n",
    "    val = sum([(i + 1) * expval[i] for i in range(2**NUM_QUBITS)]) / 2**NUM_QUBITS\n",
    "    \n",
    "    return torch.abs(target - val) ** 2, expval\n",
    "\n",
    "\n",
    "x = torch.tensor([-np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "opt = optim.Adam([x], lr = 0.1)\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "loss_list = []\n",
    "expval_list = []\n",
    "\n",
    "for i in tqdm(range(num_epoch)):\n",
    "    opt.zero_grad()\n",
    "    loss, expval = cost(x)\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()\n",
    "    loss_list.append(loss.item())\n",
    "    expval_list.append(expval)\n",
    "    \n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "n_samples = 300\n",
    "\n",
    "X_train = datasets.CIFAR10(root='./data', \n",
    "                                train=True,\n",
    "                                download=True,\n",
    "                                transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "\n",
    "idx = np.append(np.where(np.array(X_train.targets) == 0)[0][:n_samples], \n",
    "                np.where(np.array(X_train.targets) == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 3)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 4)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 5)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 6)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 7)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 8)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 9)[0][:n_samples])\n",
    "\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = np.array(X_train.targets)[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "n_samples = 50\n",
    "\n",
    "X_test = datasets.CIFAR10(root='./data', \n",
    "                               train=False,\n",
    "                               download=True,\n",
    "                               transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "\n",
    "idx = np.append(np.where(np.array(X_test.targets) == 0)[0][:n_samples], \n",
    "                np.where(np.array(X_test.targets) == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 3)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 4)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 5)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 6)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 7)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 8)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 9)[0][:n_samples])\n",
    "\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = np.array(X_test.targets)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(X_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 8)\n",
    "        self.fc4 = nn.Linear(8, 8)\n",
    "#         self.fc3 = nn.Linear(84, NUM_QUBITS)\n",
    "#         self.fc4 = nn.Linear(NUM_QUBITS, 8)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "#         x = np.pi*torch.tanh(x)\n",
    "#         x = self.fc4(x)\n",
    "\n",
    "        x = self.fc4(F.relu(x))\n",
    "        \n",
    "#         x = torch.sigmoid(x)\n",
    "#         x = torch.cat((x, 1-x), -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        pred = self.forward(x)\n",
    "        ans = torch.argmax(pred[0]).item()\n",
    "        \n",
    "        return torch.tensor(ans)\n",
    "    \n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, optimizer, train_loader, val_loader):\n",
    "    epochs = 30\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader\n",
    "    }\n",
    "    \n",
    "    loss_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    acc_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': test_loader\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "        total_acc = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                network.train(True)\n",
    "            else:\n",
    "                network.train(False)\n",
    "\n",
    "            correct = 0\n",
    "            number = 0\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(loaders[phase]):\n",
    "                optimizer.zero_grad()\n",
    "                output = network(data)\n",
    "                \n",
    "                loss = loss_func(output, target)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                total_loss[phase].append(loss.item())\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                number += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "            loss_results[phase].append(sum(total_loss[phase]) / len(total_loss[phase]))\n",
    "            acc_results[phase].append(100 * correct / number)\n",
    "\n",
    "        progress = 100.0 * (epoch + 1) / epochs\n",
    "\n",
    "        print(f'Training [{int(progress)}%]\\tTraining loss {loss_results[\"train\"][-1]}\\tValidation loss {loss_results[\"val\"][-1]}\\tTraining acc {acc_results[\"train\"][-1]}\\tValidation acc {acc_results[\"val\"][-1]}')\n",
    "        \n",
    "    return {\n",
    "        'train_loss': loss_results[\"train\"][-1],\n",
    "        'val_loss': loss_results[\"val\"][-1],\n",
    "        'train_acc': acc_results[\"train\"][-1],\n",
    "        'val_acc': acc_results[\"val\"][-1]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Training [3%]\tTraining loss 2.0853109509817194\tValidation loss 2.080156192481518\tTraining acc 12.589285714285714\tValidation acc 12.5\n",
      "Training [6%]\tTraining loss 2.0772403206676247\tValidation loss 2.05731922596693\tTraining acc 13.348214285714286\tValidation acc 18.0\n",
      "Training [10%]\tTraining loss 2.0357065804568784\tValidation loss 1.993460884988308\tTraining acc 21.919642857142858\tValidation acc 21.25\n",
      "Training [13%]\tTraining loss 1.9952272239274211\tValidation loss 2.0311073487997056\tTraining acc 21.696428571428573\tValidation acc 13.25\n",
      "Training [16%]\tTraining loss 1.98731821245913\tValidation loss 1.947784857749939\tTraining acc 21.473214285714285\tValidation acc 19.0\n",
      "Training [20%]\tTraining loss 1.94710073380598\tValidation loss 1.9109753388166428\tTraining acc 22.366071428571427\tValidation acc 27.25\n",
      "Training [23%]\tTraining loss 1.8953797593047577\tValidation loss 1.9067696878314018\tTraining acc 23.794642857142858\tValidation acc 23.75\n",
      "Training [26%]\tTraining loss 1.8537455857864449\tValidation loss 1.836705638319254\tTraining acc 25.446428571428573\tValidation acc 25.5\n",
      "Training [30%]\tTraining loss 1.8211483920924365\tValidation loss 1.867816544920206\tTraining acc 23.839285714285715\tValidation acc 27.75\n",
      "Training [33%]\tTraining loss 1.738266213866882\tValidation loss 1.7085884816385806\tTraining acc 31.830357142857142\tValidation acc 35.5\n",
      "Training [36%]\tTraining loss 1.6627991302117673\tValidation loss 1.6876039719732945\tTraining acc 35.848214285714285\tValidation acc 35.25\n",
      "Training [40%]\tTraining loss 1.595774835849144\tValidation loss 1.494260179332923\tTraining acc 40.223214285714285\tValidation acc 47.0\n",
      "Training [43%]\tTraining loss 1.4979171002559983\tValidation loss 1.6264186748676002\tTraining acc 44.955357142857146\tValidation acc 39.25\n",
      "Training [46%]\tTraining loss 1.4681393690284363\tValidation loss 1.463051286592381\tTraining acc 44.464285714285715\tValidation acc 45.25\n",
      "Training [50%]\tTraining loss 1.3732436144129938\tValidation loss 1.4798031236918177\tTraining acc 47.232142857142854\tValidation acc 45.0\n",
      "Training [53%]\tTraining loss 1.3166779881026482\tValidation loss 1.3887755968398414\tTraining acc 49.017857142857146\tValidation acc 50.75\n",
      "Training [56%]\tTraining loss 1.251112088080576\tValidation loss 1.2215627137242882\tTraining acc 53.4375\tValidation acc 57.25\n",
      "Training [60%]\tTraining loss 1.2039990428444027\tValidation loss 1.3182670814555604\tTraining acc 57.05357142857143\tValidation acc 54.5\n",
      "Training [63%]\tTraining loss 1.1440158915847665\tValidation loss 1.4122567890734945\tTraining acc 56.517857142857146\tValidation acc 50.0\n",
      "Training [66%]\tTraining loss 1.0881260194737206\tValidation loss 1.1924931035589998\tTraining acc 60.089285714285715\tValidation acc 59.25\n",
      "Training [70%]\tTraining loss 1.0097194674334826\tValidation loss 1.2369479894645685\tTraining acc 62.94642857142857\tValidation acc 58.5\n",
      "Training [73%]\tTraining loss 0.9727477958647597\tValidation loss 1.111711941928761\tTraining acc 64.24107142857143\tValidation acc 64.0\n",
      "Training [76%]\tTraining loss 0.9635743568400431\tValidation loss 1.2515082014173264\tTraining acc 64.46428571428571\tValidation acc 57.5\n",
      "Training [80%]\tTraining loss 0.9215370277319244\tValidation loss 1.0546080473357526\tTraining acc 66.60714285714286\tValidation acc 61.0\n",
      "Training [83%]\tTraining loss 0.8523126182312435\tValidation loss 1.1125199480616386\tTraining acc 71.07142857142857\tValidation acc 63.75\n",
      "Training [86%]\tTraining loss 0.8319146851458592\tValidation loss 1.2060972442388214\tTraining acc 70.0\tValidation acc 63.0\n",
      "Training [90%]\tTraining loss 0.8313081587767945\tValidation loss 1.0244483663497588\tTraining acc 70.98214285714286\tValidation acc 69.5\n",
      "Training [93%]\tTraining loss 0.778129635299005\tValidation loss 1.0048004521379752\tTraining acc 72.8125\tValidation acc 66.75\n",
      "Training [96%]\tTraining loss 0.8585630853872812\tValidation loss 1.9823488839903292\tTraining acc 70.53571428571429\tValidation acc 48.5\n",
      "Training [100%]\tTraining loss 0.9231558479962757\tValidation loss 1.1006043499892388\tTraining acc 67.85714285714286\tValidation acc 69.5\n",
      "{'train_loss': 0.9231558479962757, 'val_loss': 1.1006043499892388, 'train_acc': 67.85714285714286, 'val_acc': 69.5}\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Training [3%]\tTraining loss 2.0779840495437383\tValidation loss 2.016713260114193\tTraining acc 14.196428571428571\tValidation acc 22.5\n",
      "Training [6%]\tTraining loss 1.9491809747048787\tValidation loss 1.9014308954030275\tTraining acc 22.098214285714285\tValidation acc 21.75\n",
      "Training [10%]\tTraining loss 1.8639356224298744\tValidation loss 1.8712539000809192\tTraining acc 25.9375\tValidation acc 25.75\n",
      "Training [13%]\tTraining loss 1.7745073342735072\tValidation loss 1.738295030752197\tTraining acc 32.5\tValidation acc 33.25\n",
      "Training [16%]\tTraining loss 1.6950138393816994\tValidation loss 1.6452077246410772\tTraining acc 36.42857142857143\tValidation acc 37.0\n",
      "Training [20%]\tTraining loss 1.6149198890177088\tValidation loss 1.690780897499244\tTraining acc 38.705357142857146\tValidation acc 34.75\n",
      "Training [23%]\tTraining loss 1.5560461579093743\tValidation loss 1.5546842241566627\tTraining acc 41.160714285714285\tValidation acc 42.75\n",
      "Training [26%]\tTraining loss 1.496234427503011\tValidation loss 1.5125673061622182\tTraining acc 43.526785714285715\tValidation acc 47.0\n",
      "Training [30%]\tTraining loss 1.4492841587958751\tValidation loss 1.471060321040859\tTraining acc 47.098214285714285\tValidation acc 47.25\n",
      "Training [33%]\tTraining loss 1.3390599913229881\tValidation loss 1.3474579687893493\tTraining acc 50.625\tValidation acc 51.0\n",
      "Training [36%]\tTraining loss 1.2831860682187277\tValidation loss 1.3728048916877014\tTraining acc 53.214285714285715\tValidation acc 50.75\n",
      "Training [40%]\tTraining loss 1.1906969539380714\tValidation loss 1.4384951893886324\tTraining acc 56.160714285714285\tValidation acc 51.75\n",
      "Training [43%]\tTraining loss 1.1318355211626951\tValidation loss 1.3056923134148293\tTraining acc 59.24107142857143\tValidation acc 56.0\n",
      "Training [46%]\tTraining loss 1.1085558909090014\tValidation loss 1.1708267549566882\tTraining acc 59.24107142857143\tValidation acc 57.0\n",
      "Training [50%]\tTraining loss 1.0068882964828885\tValidation loss 1.083720138399558\tTraining acc 63.839285714285715\tValidation acc 63.5\n",
      "Training [53%]\tTraining loss 0.9485229122146951\tValidation loss 1.0748524956974983\tTraining acc 66.02678571428571\tValidation acc 61.25\n",
      "Training [56%]\tTraining loss 0.929796053120374\tValidation loss 1.2132947634012543\tTraining acc 67.00892857142857\tValidation acc 56.5\n",
      "Training [60%]\tTraining loss 0.891990714621155\tValidation loss 1.1392218818165225\tTraining acc 69.19642857142857\tValidation acc 63.25\n",
      "Training [63%]\tTraining loss 0.8388504456077294\tValidation loss 0.8985416651784363\tTraining acc 70.58035714285714\tValidation acc 70.0\n",
      "Training [66%]\tTraining loss 0.8435174746778396\tValidation loss 1.115278797750529\tTraining acc 71.25\tValidation acc 62.75\n",
      "Training [70%]\tTraining loss 0.7946990563862375\tValidation loss 1.0867820415311666\tTraining acc 72.99107142857143\tValidation acc 64.25\n",
      "Training [73%]\tTraining loss 0.7310673265313403\tValidation loss 0.9547248739055176\tTraining acc 74.95535714285714\tValidation acc 71.5\n",
      "Training [76%]\tTraining loss 0.7111151034175369\tValidation loss 0.8401092558145464\tTraining acc 77.09821428571429\tValidation acc 73.75\n",
      "Training [80%]\tTraining loss 0.5905790618708024\tValidation loss 1.2839141121938848\tTraining acc 80.80357142857143\tValidation acc 62.0\n",
      "Training [83%]\tTraining loss 0.6681740346014193\tValidation loss 0.9082869644988015\tTraining acc 78.61607142857143\tValidation acc 73.5\n",
      "Training [86%]\tTraining loss 0.6412921980356997\tValidation loss 1.1242036702828144\tTraining acc 79.28571428571429\tValidation acc 71.0\n",
      "Training [90%]\tTraining loss 0.5919163552727097\tValidation loss 0.9255112914889853\tTraining acc 80.71428571428571\tValidation acc 77.0\n",
      "Training [93%]\tTraining loss 0.5979430929686782\tValidation loss 1.0854476588985627\tTraining acc 81.91964285714286\tValidation acc 72.0\n",
      "Training [96%]\tTraining loss 0.5769753478513839\tValidation loss 1.0673404602564396\tTraining acc 82.32142857142857\tValidation acc 72.0\n",
      "Training [100%]\tTraining loss 0.5241106551290577\tValidation loss 1.1424112308027004\tTraining acc 84.01785714285714\tValidation acc 73.75\n",
      "{'train_loss': 0.5241106551290577, 'val_loss': 1.1424112308027004, 'train_acc': 84.01785714285714, 'val_acc': 73.75}\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [3%]\tTraining loss 2.0843372867575716\tValidation loss 2.0804042467474937\tTraining acc 11.875\tValidation acc 12.5\n",
      "Training [6%]\tTraining loss 2.080693032273224\tValidation loss 2.0767480516433716\tTraining acc 12.1875\tValidation acc 14.5\n",
      "Training [10%]\tTraining loss 2.04121975446386\tValidation loss 1.9975828747078777\tTraining acc 18.616071428571427\tValidation acc 23.75\n",
      "Training [13%]\tTraining loss 1.9287509000743739\tValidation loss 1.944629943780601\tTraining acc 23.973214285714285\tValidation acc 20.75\n",
      "Training [16%]\tTraining loss 1.8437216924736277\tValidation loss 1.8102979103417602\tTraining acc 29.553571428571427\tValidation acc 29.5\n",
      "Training [20%]\tTraining loss 1.7431344971418314\tValidation loss 1.7118518004752696\tTraining acc 33.75\tValidation acc 35.75\n",
      "Training [23%]\tTraining loss 1.6690031178849754\tValidation loss 1.597247381049092\tTraining acc 37.32142857142857\tValidation acc 39.5\n",
      "Training [26%]\tTraining loss 1.6208471514002667\tValidation loss 1.5415513770701363\tTraining acc 40.49107142857143\tValidation acc 42.5\n",
      "Training [30%]\tTraining loss 1.5344912758625144\tValidation loss 1.4890486401645466\tTraining acc 42.410714285714285\tValidation acc 46.0\n",
      "Training [33%]\tTraining loss 1.4608968766125183\tValidation loss 1.4816253470117227\tTraining acc 46.026785714285715\tValidation acc 45.25\n",
      "Training [36%]\tTraining loss 1.3979878388314\tValidation loss 1.3269876072933038\tTraining acc 46.964285714285715\tValidation acc 50.25\n",
      "Training [40%]\tTraining loss 1.3570322550034144\tValidation loss 1.262386365039729\tTraining acc 48.75\tValidation acc 53.25\n",
      "Training [43%]\tTraining loss 1.2761812321248212\tValidation loss 1.241677886871621\tTraining acc 52.767857142857146\tValidation acc 54.25\n",
      "Training [46%]\tTraining loss 1.2265226994216407\tValidation loss 1.1554285243879712\tTraining acc 54.19642857142857\tValidation acc 57.25\n",
      "Training [50%]\tTraining loss 1.1359170397077165\tValidation loss 1.179083163197161\tTraining acc 58.482142857142854\tValidation acc 55.5\n",
      "Training [53%]\tTraining loss 1.1074467861676147\tValidation loss 1.1399012227815046\tTraining acc 59.955357142857146\tValidation acc 63.25\n",
      "Training [56%]\tTraining loss 1.038222614991097\tValidation loss 1.246023774485982\tTraining acc 62.589285714285715\tValidation acc 57.0\n",
      "Training [60%]\tTraining loss 0.9591695980715192\tValidation loss 1.171983851060911\tTraining acc 65.44642857142857\tValidation acc 60.0\n",
      "Training [63%]\tTraining loss 0.9261294492677746\tValidation loss 1.1220198618810742\tTraining acc 66.60714285714286\tValidation acc 63.25\n",
      "Training [66%]\tTraining loss 0.9955595072019617\tValidation loss 1.0963649785227627\tTraining acc 66.60714285714286\tValidation acc 66.25\n",
      "Training [70%]\tTraining loss 0.8339928547288924\tValidation loss 0.9411523173601108\tTraining acc 71.25\tValidation acc 69.5\n",
      "Training [73%]\tTraining loss 0.8265669085484507\tValidation loss 1.2888496573262274\tTraining acc 70.40178571428571\tValidation acc 60.75\n",
      "Training [76%]\tTraining loss 0.795458656326967\tValidation loss 0.9582930636275924\tTraining acc 73.30357142857143\tValidation acc 72.25\n",
      "Training [80%]\tTraining loss 0.794752329222299\tValidation loss 1.1342720838674996\tTraining acc 73.08035714285714\tValidation acc 64.0\n",
      "Training [83%]\tTraining loss 0.7127825782373883\tValidation loss 0.9980111852956207\tTraining acc 75.98214285714286\tValidation acc 71.5\n",
      "Training [86%]\tTraining loss 0.6561795530347508\tValidation loss 0.998620794970393\tTraining acc 78.66071428571429\tValidation acc 72.75\n",
      "Training [90%]\tTraining loss 0.7566525587886503\tValidation loss 0.815185916787234\tTraining acc 75.0\tValidation acc 73.5\n",
      "Training [93%]\tTraining loss 0.6192334720130755\tValidation loss 0.7380947779868764\tTraining acc 79.0625\tValidation acc 77.25\n",
      "Training [96%]\tTraining loss 0.6245395822395104\tValidation loss 0.9257132911323325\tTraining acc 81.02678571428571\tValidation acc 73.0\n",
      "Training [100%]\tTraining loss 0.7023383718109875\tValidation loss 0.9318156110446489\tTraining acc 78.97321428571429\tValidation acc 69.75\n",
      "{'train_loss': 0.7023383718109875, 'val_loss': 0.9318156110446489, 'train_acc': 78.97321428571429, 'val_acc': 69.75}\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Training [3%]\tTraining loss 2.081011873377221\tValidation loss 2.0703142178058624\tTraining acc 12.991071428571429\tValidation acc 16.75\n",
      "Training [6%]\tTraining loss 2.0538080973284587\tValidation loss 1.9977067133784294\tTraining acc 18.526785714285715\tValidation acc 20.0\n",
      "Training [10%]\tTraining loss 1.9643138008325227\tValidation loss 1.9053137593716383\tTraining acc 20.892857142857142\tValidation acc 27.25\n",
      "Training [13%]\tTraining loss 1.8869408303705444\tValidation loss 1.7855734996497632\tTraining acc 24.955357142857142\tValidation acc 30.25\n",
      "Training [16%]\tTraining loss 1.788377670623179\tValidation loss 1.7659656566567719\tTraining acc 28.928571428571427\tValidation acc 29.25\n",
      "Training [20%]\tTraining loss 1.71302169506736\tValidation loss 1.696950963694253\tTraining acc 32.410714285714285\tValidation acc 31.75\n",
      "Training [23%]\tTraining loss 1.668629681757815\tValidation loss 1.672339708181098\tTraining acc 35.44642857142857\tValidation acc 33.75\n",
      "Training [26%]\tTraining loss 1.5977828321312408\tValidation loss 1.6662191313983932\tTraining acc 37.589285714285715\tValidation acc 32.0\n",
      "Training [30%]\tTraining loss 1.5595883273039863\tValidation loss 1.5532454462281458\tTraining acc 39.86607142857143\tValidation acc 39.0\n",
      "Training [33%]\tTraining loss 1.518076835628647\tValidation loss 1.5983926032431737\tTraining acc 41.25\tValidation acc 42.5\n",
      "Training [36%]\tTraining loss 1.4291330780368734\tValidation loss 1.560623445644132\tTraining acc 44.24107142857143\tValidation acc 39.0\n",
      "Training [40%]\tTraining loss 1.397245688305144\tValidation loss 1.3626180630654892\tTraining acc 44.776785714285715\tValidation acc 48.75\n",
      "Training [43%]\tTraining loss 1.2906991875345124\tValidation loss 1.3689165749225958\tTraining acc 48.660714285714285\tValidation acc 51.0\n",
      "Training [46%]\tTraining loss 1.2328493805083116\tValidation loss 1.2332218516568176\tTraining acc 52.767857142857146\tValidation acc 54.75\n",
      "Training [50%]\tTraining loss 1.1993035820506102\tValidation loss 1.2275482564224092\tTraining acc 54.732142857142854\tValidation acc 55.0\n",
      "Training [53%]\tTraining loss 1.073613271624401\tValidation loss 1.4067081886320374\tTraining acc 58.61607142857143\tValidation acc 50.5\n",
      "Training [56%]\tTraining loss 1.0849460473492538\tValidation loss 1.1602637904640734\tTraining acc 59.642857142857146\tValidation acc 58.25\n",
      "Training [60%]\tTraining loss 1.0727665964488289\tValidation loss 1.0594015266183876\tTraining acc 60.357142857142854\tValidation acc 64.75\n",
      "Training [63%]\tTraining loss 0.8952124860740759\tValidation loss 1.013639300517252\tTraining acc 66.42857142857143\tValidation acc 66.75\n",
      "Training [66%]\tTraining loss 0.8581365756038027\tValidation loss 1.0384863359274812\tTraining acc 69.15178571428571\tValidation acc 66.0\n",
      "Training [70%]\tTraining loss 0.8300601582729438\tValidation loss 1.0275526244749122\tTraining acc 70.625\tValidation acc 65.25\n",
      "Training [73%]\tTraining loss 0.7906977744314139\tValidation loss 1.0220415152946467\tTraining acc 72.63392857142857\tValidation acc 66.25\n",
      "Training [76%]\tTraining loss 0.7504176288992193\tValidation loss 1.0448491602271155\tTraining acc 73.66071428571429\tValidation acc 69.0\n",
      "Training [80%]\tTraining loss 0.7610767252978895\tValidation loss 1.001104600033613\tTraining acc 73.57142857142857\tValidation acc 70.75\n",
      "Training [83%]\tTraining loss 0.8124727917801164\tValidation loss 0.9413451847244851\tTraining acc 72.63392857142857\tValidation acc 71.25\n",
      "Training [86%]\tTraining loss 0.8288233317868433\tValidation loss 1.0885009372053118\tTraining acc 72.32142857142857\tValidation acc 69.5\n",
      "Training [90%]\tTraining loss 0.6916754520368333\tValidation loss 1.1401245933397939\tTraining acc 77.14285714285714\tValidation acc 66.5\n",
      "Training [93%]\tTraining loss 0.6450764516316008\tValidation loss 1.057511552068469\tTraining acc 78.21428571428571\tValidation acc 69.75\n",
      "Training [96%]\tTraining loss 0.6470238865832317\tValidation loss 1.1722469486944065\tTraining acc 79.33035714285714\tValidation acc 72.25\n",
      "Training [100%]\tTraining loss 0.6099626374353898\tValidation loss 1.1613479007880785\tTraining acc 81.42857142857143\tValidation acc 71.25\n",
      "{'train_loss': 0.6099626374353898, 'val_loss': 1.1613479007880785, 'train_acc': 81.42857142857143, 'val_acc': 71.25}\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [3%]\tTraining loss 2.0855200373700686\tValidation loss 2.0784250330924987\tTraining acc 11.785714285714286\tValidation acc 12.75\n",
      "Training [6%]\tTraining loss 2.0790286765566894\tValidation loss 2.0678444546461106\tTraining acc 13.258928571428571\tValidation acc 12.75\n",
      "Training [10%]\tTraining loss 2.0149801738693247\tValidation loss 1.8715756798535585\tTraining acc 20.758928571428573\tValidation acc 27.5\n",
      "Training [13%]\tTraining loss 1.868750465566492\tValidation loss 1.7691125445440412\tTraining acc 26.875\tValidation acc 33.5\n",
      "Training [16%]\tTraining loss 1.768846960159447\tValidation loss 1.7051160215679557\tTraining acc 31.830357142857142\tValidation acc 35.25\n",
      "Training [20%]\tTraining loss 1.6485032332633962\tValidation loss 2.212514467285364\tTraining acc 37.276785714285715\tValidation acc 31.25\n",
      "Training [23%]\tTraining loss 1.5650083855889274\tValidation loss 1.547675010266539\tTraining acc 40.401785714285715\tValidation acc 42.25\n",
      "Training [26%]\tTraining loss 1.501855203198446\tValidation loss 1.5146820632909384\tTraining acc 43.61607142857143\tValidation acc 42.25\n",
      "Training [30%]\tTraining loss 1.4185915385655465\tValidation loss 1.3329891575478905\tTraining acc 47.410714285714285\tValidation acc 52.75\n",
      "Training [33%]\tTraining loss 1.348962751592673\tValidation loss 1.3887230672637179\tTraining acc 50.3125\tValidation acc 50.0\n",
      "Training [36%]\tTraining loss 1.2839913417134607\tValidation loss 1.237075765530899\tTraining acc 51.205357142857146\tValidation acc 52.0\n",
      "Training [40%]\tTraining loss 1.2253864760706945\tValidation loss 1.3748593978583812\tTraining acc 54.6875\tValidation acc 50.5\n",
      "Training [43%]\tTraining loss 1.1319395065592435\tValidation loss 1.0897929438887421\tTraining acc 57.05357142857143\tValidation acc 57.0\n",
      "Training [46%]\tTraining loss 1.108244715964478\tValidation loss 1.301085511162136\tTraining acc 58.705357142857146\tValidation acc 56.0\n",
      "Training [50%]\tTraining loss 1.0282613468879365\tValidation loss 1.008460132713533\tTraining acc 59.86607142857143\tValidation acc 68.25\n",
      "Training [53%]\tTraining loss 0.9493894263228136\tValidation loss 1.027271818819716\tTraining acc 64.24107142857143\tValidation acc 66.0\n",
      "Training [56%]\tTraining loss 0.9482402365600349\tValidation loss 1.058270334730691\tTraining acc 64.41964285714286\tValidation acc 63.5\n",
      "Training [60%]\tTraining loss 0.8895116242067328\tValidation loss 1.0023382991301104\tTraining acc 66.875\tValidation acc 69.0\n",
      "Training [63%]\tTraining loss 0.899328525410758\tValidation loss 1.029548332071418\tTraining acc 67.27678571428571\tValidation acc 65.5\n",
      "Training [66%]\tTraining loss 0.8460507697238181\tValidation loss 1.2261704613946045\tTraining acc 69.33035714285714\tValidation acc 60.75\n",
      "Training [70%]\tTraining loss 0.7658016343827991\tValidation loss 0.8049235771225159\tTraining acc 72.1875\tValidation acc 73.75\n",
      "Training [73%]\tTraining loss 0.7138951802821208\tValidation loss 1.1731845851333875\tTraining acc 74.86607142857143\tValidation acc 71.5\n",
      "Training [76%]\tTraining loss 0.6933988485149656\tValidation loss 0.7785833029680146\tTraining acc 75.35714285714286\tValidation acc 78.5\n",
      "Training [80%]\tTraining loss 0.6630231910894235\tValidation loss 0.8492161878155678\tTraining acc 78.25892857142857\tValidation acc 77.75\n",
      "Training [83%]\tTraining loss 0.6754104825128794\tValidation loss 0.971045519192275\tTraining acc 77.27678571428571\tValidation acc 68.25\n",
      "Training [86%]\tTraining loss 0.6595082118565097\tValidation loss 0.9658444274471392\tTraining acc 78.25892857142857\tValidation acc 75.5\n",
      "Training [90%]\tTraining loss 0.665457588027615\tValidation loss 0.800025304890226\tTraining acc 78.25892857142857\tValidation acc 80.5\n",
      "Training [93%]\tTraining loss 0.7822250023625845\tValidation loss 1.094450907306918\tTraining acc 76.20535714285714\tValidation acc 67.0\n",
      "Training [96%]\tTraining loss 0.6706015956920736\tValidation loss 0.8351753393209722\tTraining acc 78.57142857142857\tValidation acc 75.5\n",
      "Training [100%]\tTraining loss 0.5487743791829116\tValidation loss 0.8964178576958107\tTraining acc 82.90178571428571\tValidation acc 71.75\n",
      "{'train_loss': 0.5487743791829116, 'val_loss': 0.8964178576958107, 'train_acc': 82.90178571428571, 'val_acc': 71.75}\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "FINAL RESULTS\n",
      "--------------------------------\n",
      "{0: {'train_loss': 0.9231558479962757, 'val_loss': 1.1006043499892388, 'train_acc': 67.85714285714286, 'val_acc': 69.5}, 1: {'train_loss': 0.5241106551290577, 'val_loss': 1.1424112308027004, 'train_acc': 84.01785714285714, 'val_acc': 73.75}, 2: {'train_loss': 0.7023383718109875, 'val_loss': 0.9318156110446489, 'train_acc': 78.97321428571429, 'val_acc': 69.75}, 3: {'train_loss': 0.6099626374353898, 'val_loss': 1.1613479007880785, 'train_acc': 81.42857142857143, 'val_acc': 71.25}, 4: {'train_loss': 0.5487743791829116, 'val_loss': 0.8964178576958107, 'train_acc': 82.90178571428571, 'val_acc': 71.75}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "\n",
    "dataset = ConcatDataset([X_train, X_test])\n",
    "\n",
    "k_folds = 5\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "results = {}\n",
    "\n",
    "print('--------------------------------')\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "  \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=1, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=1, sampler=test_subsampler)\n",
    "\n",
    "    network = Net()\n",
    "    network.apply(reset_weights)\n",
    "    \n",
    "    optimizer = optim.SGD(network.parameters(), lr =0.001, momentum=0.9)\n",
    "    \n",
    "    result = train_network(network, optimizer, trainloader, testloader)\n",
    "    \n",
    "    results[fold] = result\n",
    "    \n",
    "    print(result)\n",
    "    print('--------------------------------')\n",
    "\n",
    "print('--------------------------------')\n",
    "print('FINAL RESULTS')\n",
    "print('--------------------------------')\n",
    "print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 30\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# loss_results = {\n",
    "#     'train': [],\n",
    "#     'val': []\n",
    "# }\n",
    "\n",
    "# acc_results = {\n",
    "#     'train': [],\n",
    "#     'val': []\n",
    "# }\n",
    "\n",
    "# loaders = {\n",
    "#     'train': train_loader,\n",
    "#     'val': test_loader\n",
    "# }\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = {\n",
    "#         'train': [],\n",
    "#         'val': []\n",
    "#     }\n",
    "    \n",
    "#     total_acc = {\n",
    "#         'train': [],\n",
    "#         'val': []\n",
    "#     }\n",
    "    \n",
    "    \n",
    "#     for phase in ['train', 'val']:\n",
    "#         if phase == 'train':\n",
    "#             network.train(True)\n",
    "#         else:\n",
    "#             network.train(False)\n",
    "        \n",
    "#         correct = 0\n",
    "#         number = 0\n",
    "        \n",
    "#         for batch_idx, (data, target) in enumerate(loaders[phase]):\n",
    "#             optimizer.zero_grad()\n",
    "#             output = network(data)\n",
    "            \n",
    "#             loss = loss_func(output, target)\n",
    "            \n",
    "#             if phase == 'train':\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             total_loss[phase].append(loss.item())\n",
    "\n",
    "#             _, predicted = torch.max(output.data, 1)\n",
    "#             number += target.size(0)\n",
    "#             correct += (predicted == target).sum().item()\n",
    "            \n",
    "#         loss_results[phase].append(sum(total_loss[phase]) / len(total_loss[phase]))\n",
    "#         acc_results[phase].append(100 * correct / number)\n",
    "\n",
    "#     progress = 100.0 * (epoch + 1) / epochs\n",
    "    \n",
    "#     print(f'Training [{int(progress)}%]\\tTraining loss {loss_results[\"train\"][-1]}\\tValidation loss {loss_results[\"val\"][-1]}\\tTraining acc {acc_results[\"train\"][-1]}\\tValidation acc {acc_results[\"val\"][-1]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_results[\"train\"])\n",
    "# plt.title(f'Hybrid NN training convergence for {NUM_QUBITS}-qubits')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_results[\"val\"])\n",
    "# plt.title(f'Hybrid NN training convergence for {NUM_QUBITS}-qubits')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = 0\n",
    "# number = 0\n",
    "# for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#     number += 1\n",
    "#     output = network.predict(data).item()\n",
    "#     accuracy += (output == target[0].item())*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Performance on test data: {accuracy}/{number} = {accuracy/number*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_shape = (8, 6)\n",
    "# count = 0\n",
    "# fig, axes = plt.subplots(nrows=n_sample_shape[0], ncols=n_sample_shape[1], figsize=(10, 2*n_sample_shape[0]))\n",
    "\n",
    "# classes_map = {\n",
    "#     0: 'airplane',\n",
    "#     1: 'automobile',\n",
    "#     2: 'bird',\n",
    "#     3: 'cat',\n",
    "#     4: 'deer',\n",
    "#     5: 'dog',\n",
    "#     6: 'frog',\n",
    "#     7: 'horse',\n",
    "#     8: 'ship',\n",
    "#     9: 'truck'\n",
    "# }\n",
    "\n",
    "# network.eval()\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#         if count == n_sample_shape[0]*n_sample_shape[1]:\n",
    "#             break\n",
    "        \n",
    "#         prediction = network.predict(data).item()\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].imshow(data[0].numpy().squeeze().swapaxes(0,1).swapaxes(1,2))\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_xticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_yticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_title(classes_map[prediction])\n",
    "        \n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
