{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: qiskit in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.23.2)\n",
      "Requirement already satisfied: qiskit-terra==0.16.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.16.1)\n",
      "Requirement already satisfied: qiskit-aer==0.7.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.7.2)\n",
      "Requirement already satisfied: qiskit-aqua==0.8.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.8.1)\n",
      "Requirement already satisfied: qiskit-ignis==0.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.5.1)\n",
      "Requirement already satisfied: qiskit-ibmq-provider==0.11.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.11.1)\n",
      "Requirement already satisfied: scipy>=1.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.5.2)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (0.3.3)\n",
      "Requirement already satisfied: sympy>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.19.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.2.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.5)\n",
      "Requirement already satisfied: psutil>=5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (5.8.0)\n",
      "Requirement already satisfied: python-constraint>=1.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: ply>=3.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.11)\n",
      "Requirement already satisfied: fastjsonschema>=2.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.14.5)\n",
      "Requirement already satisfied: retworkx>=0.5.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (0.7.2)\n",
      "Requirement already satisfied: cython>=0.27.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (0.29.21)\n",
      "Requirement already satisfied: pybind11>=2.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (2.6.1)\n",
      "Requirement already satisfied: fastdtw in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.3.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.23.2)\n",
      "Requirement already satisfied: docplex in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.19.202)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (50.3.0)\n",
      "Requirement already satisfied: dlx in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.0.4)\n",
      "Requirement already satisfied: h5py in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.10.0)\n",
      "Requirement already satisfied: pandas in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.1.3)\n",
      "Requirement already satisfied: yfinance in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.1.55)\n",
      "Requirement already satisfied: quandl in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (3.6.0)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.25.10)\n",
      "Requirement already satisfied: websockets>=8 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (8.1)\n",
      "Requirement already satisfied: requests-ntlm>=1.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.1.0)\n",
      "Requirement already satisfied: nest-asyncio!=1.1.0,>=1.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: requests>=2.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (2.24.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from sympy>=1.3->qiskit-terra==0.16.1->qiskit) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from python-dateutil>=2.8.0->qiskit-terra==0.16.1->qiskit) (1.15.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (20.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (0.17.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from networkx>=2.2->qiskit-terra==0.16.1->qiskit) (4.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.20.0->qiskit-aqua==0.8.1->qiskit) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.20.0->qiskit-aqua==0.8.1->qiskit) (0.17.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from pandas->qiskit-aqua==0.8.1->qiskit) (2020.1)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (0.0.9)\n",
      "Requirement already satisfied: lxml>=4.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (4.6.2)\n",
      "Requirement already satisfied: inflection>=0.3.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (0.5.1)\n",
      "Requirement already satisfied: more-itertools in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (8.6.0)\n",
      "Requirement already satisfied: ntlm-auth>=1.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.5.0)\n",
      "Requirement already satisfied: cryptography>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (3.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2020.6.20)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.14.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (2.20)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pylatexenc in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (2.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torchvision in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.8.2)\n",
      "Requirement already satisfied: torch==1.7.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch==1.7.1->torchvision) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (4.56.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for itertools\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install qiskit\n",
    "!{sys.executable} -m pip install pylatexenc\n",
    "!{sys.executable} -m pip install torchvision\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import execute\n",
    "from qiskit.circuit import Parameter,ControlledGate\n",
    "from qiskit import Aer\n",
    "import qiskit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "\n",
    "NUM_QUBITS = 3\n",
    "NUM_SHOTS = 5000\n",
    "SHIFT = np.pi/4\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "SIMULATOR = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '001', '010', '011', '100', '101', '110', '111']\n"
     ]
    }
   ],
   "source": [
    "# create list of all possible outputs of quantum circuit (2**NUM_QUBITS possible)\n",
    "import itertools\n",
    "\n",
    "def create_QC_OUTPUTS():\n",
    "    measurements = list(itertools.product([0, 1], repeat=NUM_QUBITS))\n",
    "    return [''.join([str(bit) for bit in measurement]) for measurement in measurements]\n",
    "\n",
    "QC_OUTPUTS = create_QC_OUTPUTS()\n",
    "print(QC_OUTPUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contruct QuantumCircuit QFT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumCircuit:\n",
    "    def __init__(self, n_qubits, backend, shots):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.circuit = qiskit.QuantumCircuit(n_qubits)\n",
    "        self.thetas = { k: Parameter('Theta'+str(k)) for k in range(self.n_qubits) }\n",
    "        \n",
    "        all_qubits = [i for i in range(n_qubits)]\n",
    "        \n",
    "        self.circuit.h(all_qubits)\n",
    "        self.circuit.barrier()\n",
    "        for k in range(n_qubits):\n",
    "            self.circuit.ry(self.thetas[k], k)\n",
    "            \n",
    "        self.circuit.measure_all()\n",
    "        \n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "        \n",
    "    def N_qubits_expectation_Z(self, counts, shots, n_qubits):\n",
    "        expects = np.zeros(len(QC_OUTPUTS))\n",
    "        for k in range(len(QC_OUTPUTS)):\n",
    "            key = QC_OUTPUTS[k]\n",
    "            perc = counts.get(key, 0) / shots\n",
    "            expects[k] = perc\n",
    "        return expects\n",
    "    \n",
    "    def run(self, i):\n",
    "        params = i\n",
    "        \n",
    "        backend = Aer.get_backend('qasm_simulator')\n",
    "        \n",
    "        job_sim = execute(self.circuit, \n",
    "                          self.backend, \n",
    "                          shots = self.shots, \n",
    "                          parameter_binds = [{ self.thetas[k]: params[k].item() for k in range(self.n_qubits) }])\n",
    "        \n",
    "        result_sim = job_sim.result()\n",
    "        counts = result_sim.get_counts(self.circuit)\n",
    "        \n",
    "        return self.N_qubits_expectation_Z(counts, self.shots, self.n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value for rotation [pi/4]: [0.0036 0.0194 0.019  0.1038 0.0172 0.1056 0.1052 0.6262]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 538.279x264.88 with 1 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, NUM_SHOTS)\n",
    "print(f'Expected value for rotation [pi/4]: {circuit.run(torch.Tensor([np.pi/4] * len(QC_OUTPUTS)))}')\n",
    "circuit.circuit.draw(output='mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCircuit(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        if not hasattr(ctx, 'QuantumCircuit'):\n",
    "            ctx.QuantumCircuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, shots=NUM_SHOTS)\n",
    "        \n",
    "        exp_value = ctx.QuantumCircuit.run(i)\n",
    "        \n",
    "        result = torch.Tensor([exp_value])\n",
    "        \n",
    "        ctx.save_for_backward(result, i)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        forward_tensor,i = ctx.saved_tensors\n",
    "        \n",
    "        input_numbers = i\n",
    "        gradients = torch.Tensor()\n",
    "        \n",
    "        for k in range(NUM_QUBITS):\n",
    "            shift_right = input_numbers.detach().clone()\n",
    "            shift_right[k] = shift_right[k] + SHIFT\n",
    "            \n",
    "            shift_left = input_numbers.detach().clone()\n",
    "            shift_left[k] = shift_left[k] - SHIFT\n",
    "            \n",
    "            expectation_right = ctx.QuantumCircuit.run(shift_right)\n",
    "            expectation_left = ctx.QuantumCircuit.run(shift_left)\n",
    "            \n",
    "            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])\n",
    "            gradient = gradient / torch.norm(gradient)\n",
    "            \n",
    "            gradients = torch.cat((gradients, gradient.float()))\n",
    "            \n",
    "        result = torch.Tensor(gradients)\n",
    "        \n",
    "        return (result.float() * grad_output.float()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 after quantum layer: tensor([[0.0030, 0.0190, 0.0160, 0.1038, 0.0204, 0.1158, 0.1028, 0.6192]],\n",
      "       grad_fn=<TorchCircuitBackward>)\n",
      "x.grad = tensor([ 0.0870, -0.3515, -0.2639])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "qc = TorchCircuit.apply\n",
    "\n",
    "y1 = qc(x)\n",
    "print(f'y1 after quantum layer: {y1}')\n",
    "y1 = nn.Linear(2**NUM_QUBITS, 1)(y1.float())\n",
    "\n",
    "y1.backward()\n",
    "print(f'x.grad = {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:21<00:00,  4.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x140097fd0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qc = TorchCircuit.apply\n",
    "\n",
    "def cost(x):\n",
    "    target = -1\n",
    "    expval = qc(x)[0]\n",
    "    \n",
    "    val = sum([(i + 1) * expval[i] for i in range(2**NUM_QUBITS)]) / 2**NUM_QUBITS\n",
    "    \n",
    "    return torch.abs(target - val) ** 2, expval\n",
    "\n",
    "\n",
    "x = torch.tensor([-np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "opt = optim.Adam([x], lr = 0.1)\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "loss_list = []\n",
    "expval_list = []\n",
    "\n",
    "for i in tqdm(range(num_epoch)):\n",
    "    opt.zero_grad()\n",
    "    loss, expval = cost(x)\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()\n",
    "    loss_list.append(loss.item())\n",
    "    expval_list.append(expval)\n",
    "    \n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "X_train = datasets.FashionMNIST(root='./data', \n",
    "                                train=True,\n",
    "                                download=True,\n",
    "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "idx = np.append(np.where(X_train.targets == 0)[0][:n_samples], \n",
    "                np.where(X_train.targets == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_train.targets == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_train.targets == 3)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_train.targets == 4)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_train.targets == 5)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_train.targets == 6)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_train.targets == 7)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(X_train.targets == 8)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(X_train.targets == 9)[0][:n_samples])\n",
    "\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = X_train.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "\n",
    "X_test = datasets.FashionMNIST(root='./data', \n",
    "                               train=False,\n",
    "                               download=True,\n",
    "                               transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "idx = np.append(np.where(X_test.targets == 0)[0][:n_samples], \n",
    "                np.where(X_test.targets == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_test.targets == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_test.targets == 3)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_test.targets == 4)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_test.targets == 5)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_test.targets == 6)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_test.targets == 7)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(X_test.targets == 8)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(X_test.targets == 9)[0][:n_samples])\n",
    "\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = X_test.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(X_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, NUM_QUBITS)\n",
    "        self.qc = TorchCircuit.apply\n",
    "        self.qcsim = nn.Linear(NUM_QUBITS, 2**NUM_QUBITS)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = np.pi*torch.tanh(x)\n",
    "        \n",
    "        MODE = 'QC'\n",
    "        \n",
    "        if MODE == 'QC':\n",
    "            x = self.qc(x[0])\n",
    "        else:\n",
    "            x = self.qcsim(x)\n",
    "            \n",
    "        x = torch.sigmoid(x)\n",
    "        x = torch.cat((x, 1-x), -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        pred = self.forward(x)\n",
    "        ans = torch.argmax(pred[0]).item()\n",
    "        \n",
    "        return torch.tensor(ans)\n",
    "    \n",
    "network = Net()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, optimizer, train_loader, val_loader):\n",
    "    epochs = 15\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader\n",
    "    }\n",
    "    \n",
    "    loss_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    acc_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': test_loader\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "        total_acc = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                network.train(True)\n",
    "            else:\n",
    "                network.train(False)\n",
    "\n",
    "            correct = 0\n",
    "            number = 0\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(loaders[phase]):\n",
    "                optimizer.zero_grad()\n",
    "                output = network(data)\n",
    "\n",
    "                loss = loss_func(output, target)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                total_loss[phase].append(loss.item())\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                number += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "            loss_results[phase].append(sum(total_loss[phase]) / len(total_loss[phase]))\n",
    "            acc_results[phase].append(100 * correct / number)\n",
    "\n",
    "        progress = 100.0 * (epoch + 1) / epochs\n",
    "\n",
    "        print(f'Training [{int(progress)}%]\\tTraining loss {loss_results[\"train\"][-1]}\\tValidation loss {loss_results[\"val\"][-1]}\\tTraining acc {acc_results[\"train\"][-1]}\\tValidation acc {acc_results[\"val\"][-1]}')\n",
    "        \n",
    "    return {\n",
    "        'train_loss': loss_results[\"train\"][-1],\n",
    "        'val_loss': loss_results[\"val\"][-1],\n",
    "        'train_acc': acc_results[\"train\"][-1],\n",
    "        'val_acc': acc_results[\"val\"][-1]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Training [6%]\tTraining loss 2.7211852125823497\tValidation loss 2.678909217119217\tTraining acc 25.833333333333332\tValidation acc 50.0\n",
      "Training [13%]\tTraining loss 2.6940269018212954\tValidation loss 2.673760243058205\tTraining acc 43.125\tValidation acc 51.5\n",
      "Training [20%]\tTraining loss 2.6819198687871295\tValidation loss 2.656518900990486\tTraining acc 50.3125\tValidation acc 53.0\n",
      "Training [26%]\tTraining loss 2.6746313182016213\tValidation loss 2.6588625675439834\tTraining acc 53.4375\tValidation acc 58.75\n",
      "Training [33%]\tTraining loss 2.6711808492739997\tValidation loss 2.6502295875549318\tTraining acc 55.0\tValidation acc 59.5\n",
      "Training [40%]\tTraining loss 2.663199011236429\tValidation loss 2.6332385206222533\tTraining acc 59.6875\tValidation acc 66.75\n",
      "Training [46%]\tTraining loss 2.6579526806871097\tValidation loss 2.6264267808198927\tTraining acc 61.770833333333336\tValidation acc 68.75\n",
      "Training [53%]\tTraining loss 2.6538390782972177\tValidation loss 2.6232000482082367\tTraining acc 63.4375\tValidation acc 70.0\n",
      "Training [60%]\tTraining loss 2.6501386207838853\tValidation loss 2.6241477054357527\tTraining acc 64.16666666666667\tValidation acc 70.25\n",
      "Training [66%]\tTraining loss 2.6537811835606893\tValidation loss 2.628103604912758\tTraining acc 61.666666666666664\tValidation acc 69.25\n",
      "Training [73%]\tTraining loss 2.648658702770869\tValidation loss 2.6200391417741775\tTraining acc 64.89583333333333\tValidation acc 70.5\n",
      "Training [80%]\tTraining loss 2.6477426096796988\tValidation loss 2.6202807796001433\tTraining acc 63.541666666666664\tValidation acc 70.0\n",
      "Training [86%]\tTraining loss 2.64832290460666\tValidation loss 2.6176925748586655\tTraining acc 63.645833333333336\tValidation acc 70.25\n",
      "Training [93%]\tTraining loss 2.6446943091849486\tValidation loss 2.615940358042717\tTraining acc 64.58333333333333\tValidation acc 71.25\n",
      "Training [100%]\tTraining loss 2.6475127555429934\tValidation loss 2.6184422475099565\tTraining acc 63.4375\tValidation acc 70.75\n",
      "{'train_loss': 2.6475127555429934, 'val_loss': 2.6184422475099565, 'train_acc': 63.4375, 'val_acc': 70.75}\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Training [6%]\tTraining loss 2.7413332380354403\tValidation loss 2.74331529378891\tTraining acc 14.0625\tValidation acc 15.0\n",
      "Training [13%]\tTraining loss 2.742066064973672\tValidation loss 2.7418538945913316\tTraining acc 11.666666666666666\tValidation acc 13.25\n",
      "Training [20%]\tTraining loss 2.741679332902034\tValidation loss 2.7418882316350937\tTraining acc 12.5\tValidation acc 15.0\n",
      "Training [26%]\tTraining loss 2.741901007046302\tValidation loss 2.741778246164322\tTraining acc 10.9375\tValidation acc 14.25\n",
      "Training [33%]\tTraining loss 2.7418654814362524\tValidation loss 2.741974751353264\tTraining acc 12.8125\tValidation acc 10.25\n",
      "Training [40%]\tTraining loss 2.7386673500140506\tValidation loss 2.741888265013695\tTraining acc 14.0625\tValidation acc 11.0\n",
      "Training [46%]\tTraining loss 2.735668978343407\tValidation loss 2.7418606728315353\tTraining acc 15.520833333333334\tValidation acc 11.25\n",
      "Training [53%]\tTraining loss 2.7357894701262313\tValidation loss 2.741882458925247\tTraining acc 15.104166666666666\tValidation acc 12.25\n",
      "Training [60%]\tTraining loss 2.73582133303086\tValidation loss 2.741832901239395\tTraining acc 17.916666666666668\tValidation acc 13.25\n",
      "Training [66%]\tTraining loss 2.7329481579363346\tValidation loss 2.7419062238931655\tTraining acc 15.3125\tValidation acc 12.0\n",
      "Training [73%]\tTraining loss 2.733252188563347\tValidation loss 2.7419434851408004\tTraining acc 16.770833333333332\tValidation acc 9.75\n",
      "Training [80%]\tTraining loss 2.7331874541938306\tValidation loss 2.7419483107328415\tTraining acc 17.8125\tValidation acc 10.25\n",
      "Training [86%]\tTraining loss 2.732987220833699\tValidation loss 2.7418124681711196\tTraining acc 16.770833333333332\tValidation acc 14.0\n",
      "Training [93%]\tTraining loss 2.732930074383815\tValidation loss 2.7417908024787905\tTraining acc 15.9375\tValidation acc 11.75\n",
      "Training [100%]\tTraining loss 2.731627604116996\tValidation loss 2.7418748468160627\tTraining acc 17.395833333333332\tValidation acc 12.0\n",
      "{'train_loss': 2.731627604116996, 'val_loss': 2.7418748468160627, 'train_acc': 17.395833333333332, 'val_acc': 12.0}\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Training [6%]\tTraining loss 2.7158022257188956\tValidation loss 2.671338585019112\tTraining acc 30.625\tValidation acc 51.5\n",
      "Training [13%]\tTraining loss 2.690469030290842\tValidation loss 2.660546284914017\tTraining acc 45.520833333333336\tValidation acc 54.25\n",
      "Training [20%]\tTraining loss 2.6777020297944545\tValidation loss 2.649200548529625\tTraining acc 51.666666666666664\tValidation acc 59.25\n",
      "Training [26%]\tTraining loss 2.6700943460067115\tValidation loss 2.6416024416685104\tTraining acc 54.479166666666664\tValidation acc 59.75\n",
      "Training [33%]\tTraining loss 2.660752636939287\tValidation loss 2.625579931139946\tTraining acc 59.375\tValidation acc 70.5\n",
      "Training [40%]\tTraining loss 2.655172862112522\tValidation loss 2.6153074204921722\tTraining acc 61.875\tValidation acc 74.5\n",
      "Training [46%]\tTraining loss 2.6497885167598723\tValidation loss 2.617902681827545\tTraining acc 65.3125\tValidation acc 76.5\n",
      "Training [53%]\tTraining loss 2.6479398670295873\tValidation loss 2.61453844666481\tTraining acc 66.14583333333333\tValidation acc 75.75\n",
      "Training [60%]\tTraining loss 2.645262503127257\tValidation loss 2.6113243865966798\tTraining acc 66.66666666666667\tValidation acc 78.0\n",
      "Training [66%]\tTraining loss 2.6433838946123918\tValidation loss 2.620843337774277\tTraining acc 68.85416666666667\tValidation acc 72.75\n",
      "Training [73%]\tTraining loss 2.643543847401937\tValidation loss 2.607683287858963\tTraining acc 68.64583333333333\tValidation acc 79.0\n",
      "Training [80%]\tTraining loss 2.6402217676242192\tValidation loss 2.6066891461610795\tTraining acc 67.8125\tValidation acc 79.25\n",
      "Training [86%]\tTraining loss 2.6380278492967286\tValidation loss 2.6084569549560546\tTraining acc 68.85416666666667\tValidation acc 79.0\n",
      "Training [93%]\tTraining loss 2.6401386310656867\tValidation loss 2.6097910702228546\tTraining acc 68.75\tValidation acc 77.5\n",
      "Training [100%]\tTraining loss 2.640544408808152\tValidation loss 2.6045947599411012\tTraining acc 67.39583333333333\tValidation acc 80.25\n",
      "{'train_loss': 2.640544408808152, 'val_loss': 2.6045947599411012, 'train_acc': 67.39583333333333, 'val_acc': 80.25}\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Training [6%]\tTraining loss 2.7176885810991127\tValidation loss 2.6685496777296067\tTraining acc 29.375\tValidation acc 51.25\n",
      "Training [13%]\tTraining loss 2.6884511791169645\tValidation loss 2.6556559765338896\tTraining acc 45.0\tValidation acc 52.75\n",
      "Training [20%]\tTraining loss 2.6769136101007462\tValidation loss 2.6443184459209443\tTraining acc 51.458333333333336\tValidation acc 62.75\n",
      "Training [26%]\tTraining loss 2.670325438429912\tValidation loss 2.6318335169553757\tTraining acc 55.416666666666664\tValidation acc 72.75\n",
      "Training [33%]\tTraining loss 2.660360349714756\tValidation loss 2.6334135973453523\tTraining acc 60.3125\tValidation acc 71.75\n",
      "Training [40%]\tTraining loss 2.6518194963534674\tValidation loss 2.6157348835468293\tTraining acc 64.47916666666667\tValidation acc 78.5\n",
      "Training [46%]\tTraining loss 2.6482543316980203\tValidation loss 2.6208276838064193\tTraining acc 67.29166666666667\tValidation acc 74.5\n",
      "Training [53%]\tTraining loss 2.6431265830993653\tValidation loss 2.61153782248497\tTraining acc 69.375\tValidation acc 76.25\n",
      "Training [60%]\tTraining loss 2.6402225668231645\tValidation loss 2.6123891454935073\tTraining acc 70.83333333333333\tValidation acc 75.5\n",
      "Training [66%]\tTraining loss 2.640749481568734\tValidation loss 2.6119238358736037\tTraining acc 69.16666666666667\tValidation acc 76.75\n",
      "Training [73%]\tTraining loss 2.636989080905914\tValidation loss 2.604287332892418\tTraining acc 72.60416666666667\tValidation acc 78.5\n",
      "Training [80%]\tTraining loss 2.637003217637539\tValidation loss 2.607043008208275\tTraining acc 71.5625\tValidation acc 77.5\n",
      "Training [86%]\tTraining loss 2.6373575190703074\tValidation loss 2.60770758330822\tTraining acc 70.0\tValidation acc 76.5\n",
      "Training [93%]\tTraining loss 2.637503576527039\tValidation loss 2.6069182896614076\tTraining acc 69.6875\tValidation acc 78.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [100%]\tTraining loss 2.6339104287326336\tValidation loss 2.605886313319206\tTraining acc 71.04166666666667\tValidation acc 76.75\n",
      "{'train_loss': 2.6339104287326336, 'val_loss': 2.605886313319206, 'train_acc': 71.04166666666667, 'val_acc': 76.75}\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Training [6%]\tTraining loss 2.7061199131111304\tValidation loss 2.664207091331482\tTraining acc 36.25\tValidation acc 49.25\n",
      "Training [13%]\tTraining loss 2.686493228872617\tValidation loss 2.6626594257354737\tTraining acc 46.770833333333336\tValidation acc 50.75\n",
      "Training [20%]\tTraining loss 2.6765168964862824\tValidation loss 2.663351666927338\tTraining acc 51.979166666666664\tValidation acc 55.5\n",
      "Training [26%]\tTraining loss 2.672567979991436\tValidation loss 2.6434445625543592\tTraining acc 53.125\tValidation acc 61.5\n",
      "Training [33%]\tTraining loss 2.662454345325629\tValidation loss 2.6380312532186507\tTraining acc 59.0625\tValidation acc 68.75\n",
      "Training [40%]\tTraining loss 2.656994072596232\tValidation loss 2.6352252519130706\tTraining acc 62.1875\tValidation acc 66.5\n",
      "Training [46%]\tTraining loss 2.6531186242898306\tValidation loss 2.6243258941173555\tTraining acc 63.958333333333336\tValidation acc 70.5\n",
      "Training [53%]\tTraining loss 2.6505653580029804\tValidation loss 2.623433517217636\tTraining acc 64.89583333333333\tValidation acc 69.5\n",
      "Training [60%]\tTraining loss 2.6470201681057612\tValidation loss 2.6221958845853806\tTraining acc 63.854166666666664\tValidation acc 68.5\n",
      "Training [66%]\tTraining loss 2.6441666396955648\tValidation loss 2.6194839787483217\tTraining acc 66.5625\tValidation acc 74.0\n",
      "Training [73%]\tTraining loss 2.6433045357465743\tValidation loss 2.619260524511337\tTraining acc 67.60416666666667\tValidation acc 74.0\n",
      "Training [80%]\tTraining loss 2.643795605748892\tValidation loss 2.6203900849819184\tTraining acc 65.625\tValidation acc 74.5\n",
      "Training [86%]\tTraining loss 2.6440528584023317\tValidation loss 2.616136553287506\tTraining acc 66.14583333333333\tValidation acc 75.75\n",
      "Training [93%]\tTraining loss 2.640024175494909\tValidation loss 2.6116959911584856\tTraining acc 69.27083333333333\tValidation acc 75.75\n",
      "Training [100%]\tTraining loss 2.6413236632943153\tValidation loss 2.6117574417591096\tTraining acc 67.08333333333333\tValidation acc 74.25\n",
      "{'train_loss': 2.6413236632943153, 'val_loss': 2.6117574417591096, 'train_acc': 67.08333333333333, 'val_acc': 74.25}\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "FINAL RESULTS\n",
      "--------------------------------\n",
      "{0: {'train_loss': 2.6475127555429934, 'val_loss': 2.6184422475099565, 'train_acc': 63.4375, 'val_acc': 70.75}, 1: {'train_loss': 2.731627604116996, 'val_loss': 2.7418748468160627, 'train_acc': 17.395833333333332, 'val_acc': 12.0}, 2: {'train_loss': 2.640544408808152, 'val_loss': 2.6045947599411012, 'train_acc': 67.39583333333333, 'val_acc': 80.25}, 3: {'train_loss': 2.6339104287326336, 'val_loss': 2.605886313319206, 'train_acc': 71.04166666666667, 'val_acc': 76.75}, 4: {'train_loss': 2.6413236632943153, 'val_loss': 2.6117574417591096, 'train_acc': 67.08333333333333, 'val_acc': 74.25}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "\n",
    "dataset = ConcatDataset([X_train, X_test])\n",
    "\n",
    "k_folds = 5\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "results = {}\n",
    "\n",
    "print('--------------------------------')\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "  \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=1, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=1, sampler=test_subsampler)\n",
    "\n",
    "    network = Net()\n",
    "    network.apply(reset_weights)\n",
    "    \n",
    "    optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "    \n",
    "    result = train_network(network, optimizer, trainloader, testloader)\n",
    "    \n",
    "    results[fold] = result\n",
    "    \n",
    "    print(result)\n",
    "    print('--------------------------------')\n",
    "\n",
    "print('--------------------------------')\n",
    "print('FINAL RESULTS')\n",
    "print('--------------------------------')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 15\n",
    "# loss_list = []\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = []\n",
    "    \n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         output = network(data)\n",
    "#         loss = loss_func(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss.append(loss.item())\n",
    "        \n",
    "#     loss_list.append(sum(total_loss) / len(total_loss))\n",
    "#     progress = 100.0 * (epoch + 1) / epochs\n",
    "    \n",
    "#     print(f'Training [{int(progress)}%]\\tLoss {loss_list[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_list)\n",
    "# plt.title(f'Hybrid NN training convergence for {NUM_QUBITS}-qubits')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = 0\n",
    "# number = 0\n",
    "# for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#     number += 1\n",
    "#     output = network.predict(data).item()\n",
    "#     accuracy += (output == target[0].item())*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Performance on test data: {accuracy}/{number} = {accuracy/number*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_sample_shape = (8, 6)\n",
    "# count = 0\n",
    "# fig, axes = plt.subplots(nrows=n_sample_shape[0], ncols=n_sample_shape[1], figsize=(10, 2*n_sample_shape[0]))\n",
    "\n",
    "# classes_map = {\n",
    "#     0: 'T-shirt/top',\n",
    "#     1: 'Trouser',\n",
    "#     2: 'Pullover',\n",
    "#     3: 'Dress',\n",
    "#     4: 'Coat',\n",
    "#     5: 'Sandal',\n",
    "#     6: 'Shirt',\n",
    "#     7: 'Sneaker',\n",
    "#     8: 'Bag',\n",
    "#     9: 'Ankle boot'\n",
    "# }\n",
    "\n",
    "# network.eval()\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#         if count == n_sample_shape[0]*n_sample_shape[1]:\n",
    "#             break\n",
    "        \n",
    "#         prediction = network.predict(data).item()\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].imshow(data[0].numpy().squeeze(), cmap='gray')\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_xticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_yticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_title(classes_map[prediction])\n",
    "        \n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
