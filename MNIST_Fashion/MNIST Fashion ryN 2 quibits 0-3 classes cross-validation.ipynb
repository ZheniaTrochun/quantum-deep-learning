{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: qiskit in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.23.2)\n",
      "Requirement already satisfied: qiskit-aer==0.7.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.7.2)\n",
      "Requirement already satisfied: qiskit-ibmq-provider==0.11.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.11.1)\n",
      "Requirement already satisfied: qiskit-aqua==0.8.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.8.1)\n",
      "Requirement already satisfied: qiskit-terra==0.16.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.16.1)\n",
      "Requirement already satisfied: qiskit-ignis==0.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.5.1)\n",
      "Requirement already satisfied: pybind11>=2.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (2.6.1)\n",
      "Requirement already satisfied: numpy>=1.16.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (1.19.2)\n",
      "Requirement already satisfied: cython>=0.27.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (0.29.21)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (1.5.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.25.10)\n",
      "Requirement already satisfied: requests>=2.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (2.24.0)\n",
      "Requirement already satisfied: nest-asyncio!=1.1.0,>=1.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (2.8.1)\n",
      "Requirement already satisfied: websockets>=8 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (8.1)\n",
      "Requirement already satisfied: requests-ntlm>=1.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.1.0)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (50.3.0)\n",
      "Requirement already satisfied: docplex in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.19.202)\n",
      "Requirement already satisfied: fastdtw in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.3.4)\n",
      "Requirement already satisfied: h5py in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.10.0)\n",
      "Requirement already satisfied: quandl in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (3.6.0)\n",
      "Requirement already satisfied: dlx in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.0.4)\n",
      "Requirement already satisfied: sympy>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.7.1)\n",
      "Requirement already satisfied: psutil>=5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (5.8.0)\n",
      "Requirement already satisfied: pandas in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.1.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.23.2)\n",
      "Requirement already satisfied: yfinance in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.1.55)\n",
      "Requirement already satisfied: retworkx>=0.5.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.7.2)\n",
      "Requirement already satisfied: fastjsonschema>=2.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.14.5)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.5)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (0.3.3)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.2.0)\n",
      "Requirement already satisfied: python-constraint>=1.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: ply>=3.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2020.6.20)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from python-dateutil>=2.8.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.15.0)\n",
      "Requirement already satisfied: cryptography>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (3.3.1)\n",
      "Requirement already satisfied: ntlm-auth>=1.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.5.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (8.6.0)\n",
      "Requirement already satisfied: inflection>=0.3.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (0.5.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from sympy>=1.3->qiskit-aqua==0.8.1->qiskit) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from pandas->qiskit-aqua==0.8.1->qiskit) (2020.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.20.0->qiskit-aqua==0.8.1->qiskit) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.20.0->qiskit-aqua==0.8.1->qiskit) (0.17.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (0.0.9)\n",
      "Requirement already satisfied: lxml>=4.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (4.6.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from networkx>=2.2->qiskit-terra==0.16.1->qiskit) (4.4.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (20.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (0.17.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.14.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (2.20)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pylatexenc in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (2.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torchvision in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.8.2)\n",
      "Requirement already satisfied: torch==1.7.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch==1.7.1->torchvision) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (4.56.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for itertools\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install qiskit\n",
    "!{sys.executable} -m pip install pylatexenc\n",
    "!{sys.executable} -m pip install torchvision\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import execute\n",
    "from qiskit.circuit import Parameter,ControlledGate\n",
    "from qiskit import Aer\n",
    "import qiskit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "\n",
    "NUM_QUBITS = 2\n",
    "NUM_SHOTS = 5000\n",
    "SHIFT = np.pi/4\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "SIMULATOR = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '01', '10', '11']\n"
     ]
    }
   ],
   "source": [
    "# create list of all possible outputs of quantum circuit (2**NUM_QUBITS possible)\n",
    "import itertools\n",
    "\n",
    "def create_QC_OUTPUTS():\n",
    "    measurements = list(itertools.product([0, 1], repeat=NUM_QUBITS))\n",
    "    return [''.join([str(bit) for bit in measurement]) for measurement in measurements]\n",
    "\n",
    "QC_OUTPUTS = create_QC_OUTPUTS()\n",
    "print(QC_OUTPUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contruct QuantumCircuit QFT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumCircuit:\n",
    "    def __init__(self, n_qubits, backend, shots):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.circuit = qiskit.QuantumCircuit(n_qubits)\n",
    "        self.thetas = { k: Parameter('Theta'+str(k)) for k in range(self.n_qubits) }\n",
    "        \n",
    "        all_qubits = [i for i in range(n_qubits)]\n",
    "        \n",
    "        self.circuit.h(all_qubits)\n",
    "        self.circuit.barrier()\n",
    "        for k in range(n_qubits):\n",
    "            self.circuit.ry(self.thetas[k], k)\n",
    "            \n",
    "        self.circuit.measure_all()\n",
    "        \n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "        \n",
    "    def N_qubits_expectation_Z(self, counts, shots, n_qubits):\n",
    "        expects = np.zeros(len(QC_OUTPUTS))\n",
    "        for k in range(len(QC_OUTPUTS)):\n",
    "            key = QC_OUTPUTS[k]\n",
    "            perc = counts.get(key, 0) / shots\n",
    "            expects[k] = perc\n",
    "        return expects\n",
    "    \n",
    "    def run(self, i):\n",
    "        params = i\n",
    "        \n",
    "        job_sim = execute(self.circuit, \n",
    "                          self.backend, \n",
    "                          shots = self.shots, \n",
    "                          parameter_binds = [{ self.thetas[k]: params[k].item() for k in range(self.n_qubits) }])\n",
    "        \n",
    "        result_sim = job_sim.result()\n",
    "        counts = result_sim.get_counts(self.circuit)\n",
    "        \n",
    "        return self.N_qubits_expectation_Z(counts, self.shots, self.n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value for rotation [pi/4]: [0.024  0.1342 0.1268 0.715 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 478.079x204.68 with 1 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, NUM_SHOTS)\n",
    "print(f'Expected value for rotation [pi/4]: {circuit.run(torch.Tensor([np.pi/4] * len(QC_OUTPUTS)))}')\n",
    "circuit.circuit.draw(output='mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCircuit(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        if not hasattr(ctx, 'QuantumCircuit'):\n",
    "            ctx.QuantumCircuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, shots=NUM_SHOTS)\n",
    "        \n",
    "        exp_value = ctx.QuantumCircuit.run(i)\n",
    "        \n",
    "        result = torch.Tensor([exp_value])\n",
    "        \n",
    "        ctx.save_for_backward(result, i)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        forward_tensor,i = ctx.saved_tensors\n",
    "        \n",
    "        input_numbers = i\n",
    "        gradients = torch.Tensor()\n",
    "        \n",
    "        for k in range(NUM_QUBITS):\n",
    "            shift_right = input_numbers.detach().clone()\n",
    "            shift_right[k] = shift_right[k] + SHIFT\n",
    "            \n",
    "            shift_left = input_numbers.detach().clone()\n",
    "            shift_left[k] = shift_left[k] - SHIFT\n",
    "            \n",
    "            expectation_right = ctx.QuantumCircuit.run(shift_right)\n",
    "            expectation_left = ctx.QuantumCircuit.run(shift_left)\n",
    "            \n",
    "            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])\n",
    "            gradient = gradient / torch.norm(gradient)\n",
    "            \n",
    "            gradients = torch.cat((gradients, gradient.float()))\n",
    "            \n",
    "        result = torch.Tensor(gradients)\n",
    "        \n",
    "        return (result.float() * grad_output.float()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 after quantum layer: tensor([[0.0206, 0.1254, 0.1168, 0.7372]], grad_fn=<TorchCircuitBackward>)\n",
      "x.grad = tensor([0.2000, 0.5079])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "qc = TorchCircuit.apply\n",
    "\n",
    "y1 = qc(x)\n",
    "print(f'y1 after quantum layer: {y1}')\n",
    "y1 = nn.Linear(2**NUM_QUBITS, 1)(y1.float())\n",
    "\n",
    "y1.backward()\n",
    "print(f'x.grad = {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13ef598e0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAfyUlEQVR4nO3de5hcdZ3n8fe3bl1d1dd0dy4ESAjhDnKLgAYxyArx7ujMrGFmRR9m8ujijLMzuuqzz8qszj7rDM7OjoOIrGQz6hJ3ERB0lYuARJFbQAKBcAm5QK59S9L3qq6q7/5Rpzud0Lck1Sk85/N6nn66+5zT53xPn+5P/epXv/Mrc3dERCS8YtUuQEREZpaCXkQk5BT0IiIhp6AXEQk5Bb2ISMglql3AeFpbW33hwoXVLkNE5PfG008/3enubeOte0sG/cKFC1m3bl21yxAR+b1hZtsmWqeuGxGRkFPQi4iE3JRBb2arzKzdzDZMsL7RzH5qZuvN7AUz+/SYdUUzezb4uKeShYuIyPRMp0W/Glg+yfrrgBfd/VxgGfCPZpYK1g26+3nBx4ePqlIRETkiUwa9u68FuifbBKg3MwPqgm0LlSlPRESOViX66G8EzgB2As8Dn3f3UrAubWbrzOxxM/toBY4lIiKHqRLDK68CngXeA5wMPGBmv3b3HmCBu+8ws0XAQ2b2vLu/Nt5OzGwlsBLgxBNPrEBZIiIClWnRfxq408s2AVuA0wHcfUfweTPwK+D8iXbi7re4+xJ3X9LWNu6Y/0m5O9968FXWvtJxBKcgIhJelQj614ErAMxsDnAasNnMms2sJljeCiwFXqzA8cZlZtyydjMPv9w+U4cQEfm9NGXXjZmtoTyaptXMtgPXA0kAd78Z+Dqw2syeBwz4krt3mtk7ge+aWYnyA8o33H3Ggh6gKZNk38DwTB5CROT3zpRB7+4rpli/E7hynOW/Bc458tIOX3Mmxd6B/LE8pIjIW16o7oxtyiTZqxa9iMhBQhX0zZkU+9SiFxE5SMiCPsnefgW9iMhYoQr6pkyKnqEChWJp6o1FRCIiVEE/K1ueYmffoPrpRURGhCromzJJAPXTi4iMEaqgb86UW/QaeSMickA4g14vyIqIjApV0B/oulGLXkRkRKiCvjk70nWjFr2IyIhQBX02FScZN/XRi4iMEaqgNzOadHesiMhBQhX0ENwdq6AXERkVuqBvyqTUdSMiMkbogr45k1TXjYjIGCEMerXoRUTGCl3Qj7wY6+7VLkVE5C0hdEHfnEkyXHT688VqlyIi8pYQwqDXNAgiImOFLug1DYKIyMFCF/SaBkFE5GDhC/qgRa+gFxEpC2HQq49eRGSs0AV9Y+1Ii1599CIiEMKgT8RjNKQTujtWRCQQuqCH8guyatGLiJRNK+jNbJWZtZvZhgnWN5rZT81svZm9YGafHrPuGjN7Nfi4plKFT6Y8sZla9CIiMP0W/Wpg+STrrwNedPdzgWXAP5pZysxmAdcDFwMXAdebWfORlzs95YnN1KIXEYFpBr27rwW6J9sEqDczA+qCbQvAVcAD7t7t7nuBB5j8AaMimtWiFxEZVak++huBM4CdwPPA5929BMwH3hiz3fZg2ZuY2UozW2dm6zo6Oo6qmCa16EVERlUq6K8CngWOA84DbjSzhsPZgbvf4u5L3H1JW1vbURXTnEnRlyuQL5SOaj8iImFQqaD/NHCnl20CtgCnAzuAE8Zsd3ywbEaN3B27b1DdNyIilQr614ErAMxsDnAasBm4D7jSzJqDF2GvDJbNqKbg7lh134iIQGI6G5nZGsqjaVrNbDvlkTRJAHe/Gfg6sNrMngcM+JK7dwY/+3XgqWBXX3P3yV7UrQhNgyAicsC0gt7dV0yxfifl1vp461YBqw6/tCPXlNE0CCIiI0J7ZyygaRBERAhr0KtFLyIyKpRBX5uMk0rE1KIXESGkQW9mNGeSujtWRISQBj2MTIOgrhsRkdAGfXkaBLXoRURCG/TNmZRumBIRIcRB35RJqutGRIRQB32KfQN53L3apYiIVFVog745k6RQcvrzxWqXIiJSVaEN+qZazXcjIgJhDvqRqYrVTy8iERfaoB+d70Zz0otIxIU26JtqNd+NiAiEOegzmsFSRARCHfRBi75fLXoRibbQBn0yHqOuJqE+ehGJvNAGPYzMd6MWvYhEW6iDvjyDpVr0IhJtoQ56tehFREIf9CmNuhGRyAt10DdrBksRkXAHfVMmRc/QMMWSZrAUkegKd9DXJnGHnkG16kUkukId9M3ZkWkQ1E8vItEV6qAfnQZBLXoRibDEVBuY2Srgg0C7u589zvovAn8yZn9nAG3u3m1mW4FeoAgU3H1JpQqfjpGJzTTyRkSibDot+tXA8olWuvsN7n6eu58HfAV4xN27x2xyebD+mIY8lG+YAs13IyLRNmXQu/taoHuq7QIrgDVHVVEFNavrRkSkcn30Zpah3PK/Y8xiB+43s6fNbOUUP7/SzNaZ2bqOjo6K1FSfThAzdd2ISLRV8sXYDwGPHtJtc6m7XwC8D7jOzC6b6Ifd/RZ3X+LuS9ra2ipSUCxmNNYmNepGRCKtkkH/CQ7ptnH3HcHnduAu4KIKHm9amjMpzXcjIpFWkaA3s0bg3cDdY5Zlzax+5GvgSmBDJY53OBo1sZmIRNx0hleuAZYBrWa2HbgeSAK4+83BZn8A3O/u/WN+dA5wl5mNHOc2d7+3cqVPT3MmxZ6eoWN9WBGRt4wpg97dV0xjm9WUh2GOXbYZOPdIC6uUpkySl3f3VrsMEZGqCfWdsQBNtZqqWESiLfRB35xJ0p8vki+Uql2KiEhVhD7om7LBTVNq1YtIRIU/6Efmu9HdsSISUaEP+gPz3ahFLyLRFPqgb8qMzEmvFr2IRFNkgn7/oFr0IhJNoQ/60a4btehFJKJCH/SZVJxUPKaJzUQkskIf9GZGUybJfrXoRSSiQh/0UO6nV4teRKIqEkHfnEnp7QRFJLIiEfQtdSm61aIXkYiKRNA3Z1J064YpEYmoSAT9rGx5BstiyatdiojIMReZoC859Gi+GxGJoMgEPaB+ehGJpEgE/cjdseqnF5EoikTQj7boFfQiEkGRCnpNVSwiURSpoO9S0ItIBEUi6NPJOJlUXC16EYmkSAQ9BDdNadSNiERQZIJ+VlZ3x4pINEUq6NV1IyJRFKmgV9eNiETRlEFvZqvMrN3MNkyw/otm9mzwscHMimY2K1i33MxeNrNNZvblShd/OJozKbr7FPQiEj3TadGvBpZPtNLdb3D389z9POArwCPu3m1mceDbwPuAM4EVZnbm0Zd8ZFrqUvTniwwNF6tVgohIVUwZ9O6+Fuie5v5WAGuCry8CNrn7ZnfPAz8CPnJEVVbAyDQI+/SWgiISMRXrozezDOWW/x3BovnAG2M22R4sm+jnV5rZOjNb19HRUamyRs3KJgHo6s9VfN8iIm9llXwx9kPAo+4+3db/Qdz9Fndf4u5L2traKlhW2UiLXm8pKCJRU8mg/wQHum0AdgAnjPn++GBZVbTUaapiEYmmigS9mTUC7wbuHrP4KeAUMzvJzFKUHwjuqcTxjsSBFr2CXkSiJTHVBma2BlgGtJrZduB6IAng7jcHm/0BcL+794/8nLsXzOxzwH1AHFjl7i9Utvzpa8qkMNPEZiISPVMGvbuvmMY2qykPwzx0+c+Bnx9JYZUWjxlNtUm16EUkciJzZyxAs+6OFZEIilTQz9LdsSISQdEK+myKvWrRi0jERC7oNVWxiERNpIK+OWjRu3u1SxEROWYiFfQt2RTDRac3V6h2KSIix0ykgl43TYlIFEUq6Gdly0Gvm6ZEJEoiGfRq0YtIlEQy6DXyRkSiJFJB36ygF5EIilTQZ1NxUomYpkEQkUiJVNCbGbMyKfXRi0ikRCroIZjYTEEvIhESuaBvyabo1MRmIhIhkQv62Q01dPTqDcJFJDoiF/RzG9Ls6RmiVNJ8NyISDZEL+jkNaQol18gbEYmMCAZ9DQC79w9VuRIRkWMjgkGfBqC9V0EvItEQ2aDf06MXZEUkGiIX9G31NZip60ZEoiNyQZ+Mx2jJ1qjrRkQiI3JBDzC3sUYtehGJjEgG/Zz6tProRSQyIhn0sxvS6roRkciYMujNbJWZtZvZhkm2WWZmz5rZC2b2yJjlW83s+WDdukoVfbTmNqTp7MuTL5SqXYqIyIybTot+NbB8opVm1gTcBHzY3c8C/uiQTS539/PcfcmRFllpIzdNdfSp+0ZEwm/KoHf3tUD3JJtcDdzp7q8H27dXqLYZM6exPJZeL8iKSBRUoo/+VKDZzH5lZk+b2SfHrHPg/mD5ysl2YmYrzWydma3r6OioQFkTm1Mf3B3bo6AXkfBLVGgfFwJXALXAY2b2uLu/Alzq7jvMbDbwgJm9FDxDeBN3vwW4BWDJkiUzOrXk6Hw3CnoRiYBKtOi3A/e5e7+7dwJrgXMB3H1H8LkduAu4qALHO2qzsimScdMQSxGJhEoE/d3ApWaWMLMMcDGw0cyyZlYPYGZZ4EpgwpE7x5KZMbs+ra4bEYmEKbtuzGwNsAxoNbPtwPVAEsDdb3b3jWZ2L/AcUAK+5+4bzGwRcJeZjRznNne/d2ZO4/DNbUyr60ZEImHKoHf3FdPY5gbghkOWbSbownkrmtNQw8u7e6tdhojIjIvknbEAszUNgohERGSDfm5jmr5cgb5codqliIjMqMgG/cgQS70gKyJhF+GgD+6OVdCLSMhFPujb1U8vIiEX+aBXi15Ewi6yQV9Xk6CuJsEeBb2IhFxkgx5gdkONgl5EQi/SQT+3QWPpRST8Ih308xpr2bF3sNpliIjMqEgH/cKWDLt7hhjMF6tdiojIjIl20LdmAdjW3V/lSkREZk60g76lHPRbOxX0IhJe0Q761gwAWzoHqlyJiMjMiXTQ16eTtNal1KIXkVCLdNBDuftmS5eCXkTCS0HfmlWLXkRCLfJBf1JrlvbeHP2al15EQiryQT868kbdNyISUpEP+gUt5ZE3WzXyRkRCKvJBP3LTlFr0IhJWkQ/6upoEbfU1ekFWREIr8kEPcFJLVi16EQktBT3lO2R1d6yIhJWCnnI/fWdfjt6h4WqXIiJScQp6yl03ANu61KoXkfCZMujNbJWZtZvZhkm2WWZmz5rZC2b2yJjly83sZTPbZGZfrlTRlTYy8maLXpAVkRCaTot+NbB8opVm1gTcBHzY3c8C/ihYHge+DbwPOBNYYWZnHmW9M+LAWHoFvYiEz5RB7+5rge5JNrkauNPdXw+2bw+WXwRscvfN7p4HfgR85CjrnRGZVII5DTWa3ExEQqkSffSnAs1m9isze9rMPhksnw+8MWa77cGycZnZSjNbZ2brOjo6KlDW4VnYosnNRCScKhH0CeBC4APAVcB/NrNTD3cn7n6Luy9x9yVtbW0VKOvwLGqrY1N7H6WSH/Nji4jMpEoE/XbgPnfvd/dOYC1wLrADOGHMdscHy96S3r6wmZ6hAht391S7FBGRiqpE0N8NXGpmCTPLABcDG4GngFPM7CQzSwGfAO6pwPFmxNLFrQA8uqmzypWIiFTWdIZXrgEeA04zs+1mdq2ZfcbMPgPg7huBe4HngCeB77n7BncvAJ8D7qMc/P/X3V+YqRM5WnMa0pzcluXRTV3VLkVEpKISU23g7iumsc0NwA3jLP858PMjK+3YW7q4ldvXbSdfKJFK6F4yEQkHpdkYSxe3Mjhc5Hev7612KSIiFaOgH+OSRS3EDB59bea7b/YPDDNcLM34cUREFPRjNNYmOWd+I7+doRdkh4aL3LN+J9esepLzv34/n/3hM7hrOKeIzKwp++ijZuniVm5Zu5m+XIG6mqP79bzeNcB/+8VGNnf009Wfo7s/T8nhuMY0V545l3tf2M2tv9nCn71rUYWqFxF5MwX9IZYubuWmX73Gk1u6eM/pc45oH+7OXb/bwVfvfgEzeMeiFi5Y0ExrXYpLFrXwjkUtmMFnfvg03/jFSyxZOIvzTmiq7ImIiAQU9Ie4cEEzqUSMRzcdWdDnCyW+cPt67lm/k4tOmsU//dvzmN9UO+62//Dxc/nAv/yaz932DP/vL99FY23yaMsXEXkT9dEfIp2M8/aFzUd849Q3fvES96zfyd+891TW/PklE4Y8QGMmyb+sOJ/d+4f4d7c+wat7eo+0bBGRCSnox7F0cSsv7e5l9/6hw/q5+1/YzapHt/Cpdy7kL644hXjMpvyZ809s5sarz+eN7gE+8K3fcONDr2o0johUlIJ+HFcEXTYPv9w+xZYHbN87wBduX8858xv5yvtPP6zjLT97Hg/89bt571lz+Ob9r/DJW58kVyge1j5ERCaioB/HqXPqmN9Uy4Mbpxf0w8USf7Hmd7jDjVefT00iftjHbK2r4dtXX8A//OHbeGxzF1+4/TnNpCkiFaGgH4eZccUZs/nNpg6GhqduWd/5zHZ+9/o+/uvHzmFB8P6zR+qPl5zAl5afzk/X7+Tv73vpqPYlIgIK+gldccYchoZLPDbFXbLFknPzI5s5e34DH3rbvIoc+zPvXsSfXnIi331kM7f+ZktF9iki0aWgn8DFJ80ik4rz4Et7Jt3uFxt2saWzn+uWLcZs6hdfp8PM+NsPncV7z5zD13/2Il++47lpPbMQERmPgn4C6WScSxe38tDG9gmnKXB3vv3wayxqy3LVWXMrevxEPMbNf3oh111+Mj966g0+/p3fsk3vaSsiR0BBP4krzpjNzv1DbNw1/vj2X73SwcZdPXz23ScTm8ZQysMVjxlfvOp0br1mCW90D/Cxm37LFr2vrYgcJgX9JC4/fTYAD03QfXPTw5uY31TLR8+f8D3PK+KKM+Zw13VLceCTq56gvffwxvcXiiWe3NLNvoH8zBQoIm9pCvpJzK5Pc+7xjfxynGGWdz+7g6e27mXlZYtIxmf+13hyWx2rPvV2uvryfGrVU/QODU/r517c2cNHb3qUP/7uY1z4d7/k6v/5OKt+s4UnNnexp2dIs2eKRIC9Ff/RlyxZ4uvWrat2GQDc+NCrfPP+V/jClafy75ctJhYz7t2wm+tue4YLTmziB9deTDp5+OPmj9Qjr3Rw7eqnOGFWhhNmZUjFY8zKJlm6uJV3ndLGrGyKfKHE1q5+7nl2Jzc/8hpNmRR//d5T2b53gPtf3MOm9r7R/WVTcd5/zjz+/LJFnDqn/pidh1RGrlBkz/4cHX05oPy/XHIYLpTIFUqU3Dl7fiNzGtLVLVRmnJk97e5Lxl2noJ/cYL7Il+54jnvW7+Ty09r40LnH8aU7nuOc+Y18/9qLj3oq4yNx74ZdrHp0K7lCiXyhxO79g+wdGMYMTmjOsHPfIIXgZquPXTCfr37wTJoyqdGf3753gM0d/Wzr6mfDjh7uXr+DoeESl53aRmtdil37htjTM0QyHqOlLsWsbIraMQ9mdekExzXWMq8pTbHkbOsaYGtnPz1Dw6QSMVLxGE2ZFGcd18A5xzeyuK2OxDjPenKFIt39ebr68uwbGMaDoCqUnPaeIXbuG6KzL8fxzRnOmd/Imcc10DdUYGtXP693D1CfTrCgJcvClgyJeIy+oQK9Q8PsHRimuz9HZ1/+oNFKA/ny8Tr7cgwXS7TU1dCaTVGTjNPVl6erP0eh5Jx7fCMXLmjmrOMaZ+RB3N3pGSzQ1Z+jqz9PV1/5cyJmwflkmdNQM+4orlyhyK9f6eRnz+3k0de66OjNTeuYi1qzXLyohXmNaerTCerTSRa0ZDh1dj2NmQOT6bk7nX15du0fZOe+IXbtH2TX/iF27huksTbJZae2sXRxK3U1CQrFEnt6c+QLJeY1pif9Xbn7Qefj7uwbGKarP8esbA3NmeSbzrdQLDdYNrX3k4gZLXUpWutqaKlLkUmN/3/n7vTlChSKTl06MeGz7ZHcm2yk3HCxxN7+PD3B39VgvkhDbZKWuhTNwf9TrlBiuFgibkYqESMZj7FvIM/O/UPs3j/I0PCB6UwaM0kWtmQ5vrmWRMzoyxXo7s/jDnOn+P1Nh4L+KLk7P3x8G1/72YsMF8tB8IM/u5iG9FtjtsliyXl+x34eebmDV/b0sqAlw2lz6znruAYWz566lb63P88PH9/GbU++DsC8xjTzGmsZLpbKQdyfJxcEpgM9g8P05w8e7jmvMU1zJkW+WH7w6ezLMRBsYwZ1qQR16QQ1iRh9uSK9Q8PkCpPP6WMGDekk+wen1001HQ3pBC11NSTjRnd/fvQ9ArKpOC11NZTc2b53cHT7bCpOfTpJtiZOLAgFpzxLaa5QZLjoB3V/JeIxahIxUokY8THbF4ql0QfmnqFhhouT/9/FDOpqyoFcm4ozEke7e4boHSrQlEnyntNms6Aly7ymNG31NaPHM4NUvFxDyZ1ntu3jsc1dPLW1m96hwpuONbu+hmQ8Ru/QMH25AofekJ1KxJjXmKazN0d/vkgybrRka2jvHTpo2+ZMklnZ1OjvqVjy0ZDMFUqjv8tUIkZ779BBIZhOxpjTkCYVBHOxVL4O+QnmfapNxmmpO9AAcWAgV6CzP0++cPB+08kDv79iyUf/Rp3y32V9OkFt6sD1LZSc7v58Rf/uxorHjHjMDqoTYFY2xcltWW7/zDuPaL8K+gpZ/8Y+fvLsDv7qilMPagVFjXv5H3jX/kEMY0FL5k2tkWLJ2dLZx4YdPWzu6KM3V6BvqMBQoRQEWIL6mnLottSlaKpNjk4CZ2bMrq8p/+MnYuztz7Nh535e2tVLY225JXpiS4a+oQJbOvvZ1jVAyZ36dJK6dILmTJKWbA2tdSlqUwfqqknE3/Sm78WSM1wsHVR/R2+OZ17fy8ZdPfQMFujLDdOfK44+4xjdVzxGMmEHHgAcCqUSueESuWLp4AeA2IEHgIbaJC3ZVPBsqYaWbLmlmi+U2Nbdz9auAfbsH6IvV6BnaPigZyWNtUmuOmsuSxe3HtFrQ/lCib5cgf2Dw2zt7OflPb1sau/DnaCln6C1rmb0wX5eU5qWbAqzcjA9vW0vj7zSQWdfjuMa08xtrKUmEWN3T7nVv3fMC/4xM+rTSerTCdKJGP35Aw/wbXU1zGuqpbUuRVfwDGJ3T45iqRx+hnH8rFpOm1PP4tl1uENXf47O3vxBz4LGzglVm0zQWlf+vSZiMfpyBfpyhYN+f7Gg5V2TiGFAb65A71CBgXzhoG3K16eG5myKhuD3UptM0DM0TFdfnr0D+YMeUIslJx88kDdmkuXfXWOabPCM37384LE1ePabL5bKtWZrcGDXvkF27i+/ZvaNj7/tsK8rKOhFREJvsqDXqBsRkZBT0IuIhJyCXkQk5KYMejNbZWbtZrZhgvXLzGy/mT0bfHx1zLqtZvZ8sFyd7iIiVTCdQeCrgRuB70+yza/d/YMTrLvc3Y/sDVhFROSoTdmid/e1QPcxqEVERGZApfro32Fm683sF2Z21pjlDtxvZk+b2coKHUtERA5DJe7ffwZY4O59ZvZ+4CfAKcG6S919h5nNBh4ws5eCZwhvEjwQrAQ48cQTK1CWiIjANG+YMrOFwM/c/expbLsVWHJov7yZ/S3Q5+7fnMY+OoBtUxY2vlYgaq8JRPGcIZrnHcVzhmie9+Ge8wJ3bxtvxVG36M1sLrDH3d3MLqLcHdRlZlkg5u69wddXAl+bzj4nKnaa9ayb6O6wsIriOUM0zzuK5wzRPO9KnvOUQW9ma4BlQKuZbQeuB5IA7n4z8IfAZ82sAAwCnwhCfw5wVzA7XAK4zd3vrUTRIiIyfVMGvbuvmGL9jZSHXx66fDNw7pGXJiIilRDGO2NvqXYBVRDFc4ZonncUzxmied4VO+e35OyVIiJSOWFs0YuIyBgKehGRkAtN0JvZcjN72cw2mdmXq13PTDGzE8zsYTN70cxeMLPPB8tnmdkDZvZq8Lm52rVWmpnFzex3Zvaz4PuTzOyJ4Jr/HzNLTbWP3zdm1mRmPzazl8xso5m9I+zX2sz+Q/C3vcHM1phZOozXerwJIye6tlb2reD8nzOzCw7nWKEIejOLA98G3gecCawwszOrW9WMKQB/4+5nApcA1wXn+mXgQXc/BXgw+D5sPg9sHPP93wP/5O6Lgb3AtVWpamb9M3Cvu59OeRTbRkJ8rc1sPvCXlG+6PBuIA58gnNd6NbD8kGUTXdv3UZ5x4BTKMwh853AOFIqgBy4CNrn7ZnfPAz8CPlLlmmaEu+9y92eCr3sp/+PPp3y+/xps9q/AR6tS4Awxs+OBDwDfC7434D3Aj4NNwnjOjcBlwK0A7p53932E/FpTHvZda2YJIAPsIoTXeoIJIye6th8Bvu9ljwNNZjZvuscKS9DPB94Y8/32YFmoBVNTnA88Acxx913Bqt3AnGrVNUP+B/AfgVLwfQuwz91H3tU5jNf8JKAD+F9Bl9X3grvMQ3ut3X0H8E3gdcoBvx94mvBf6xETXdujyriwBH3kmFkdcAfwV+7eM3adl8fMhmbcrJl9EGh396erXcsxlgAuAL7j7ucD/RzSTRPCa91MufV6EnAckOXN3RuRUMlrG5ag3wGcMOb744NloWRmScoh/7/d/c5g8Z6Rp3LB5/Zq1TcDlgIfDibM+xHlp/H/TPnp68jd3WG85tuB7e7+RPD9jykHf5iv9b8Btrh7h7sPA3dSvv5hv9YjJrq2R5VxYQn6p4BTglfmU5RfvLmnyjXNiKBv+lZgo7v/9zGr7gGuCb6+Brj7WNc2U9z9K+5+vLsvpHxtH3L3PwEepjzXEoTsnAHcfTfwhpmdFiy6AniREF9ryl02l5hZJvhbHznnUF/rMSa6tvcAnwxG31wC7B/TxTM1dw/FB/B+4BXgNeA/VbueGTzPSyk/nXsOeDb4eD/lPusHgVeBXwKzql3rDJ3/MspTZgMsAp4ENgG3AzXVrm8Gzvc8YF1wvX8CNIf9WgP/BXgJ2AD8AKgJ47UG1lB+HWKY8rO3aye6toBRHln4GvA85VFJ0z6WpkAQEQm5sHTdiIjIBBT0IiIhp6AXEQk5Bb2ISMgp6EVEQk5BLyIScgp6EZGQ+/+IRnA75Sl88gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qc = TorchCircuit.apply\n",
    "\n",
    "def cost(x):\n",
    "    target = -1\n",
    "    expval = qc(x)[0]\n",
    "    \n",
    "    val = sum([(i + 1) * expval[i] for i in range(2**NUM_QUBITS)]) / 2**NUM_QUBITS\n",
    "    \n",
    "    return torch.abs(target - val) ** 2, expval\n",
    "\n",
    "\n",
    "x = torch.tensor([-np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "opt = optim.Adam([x], lr = 0.1)\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "loss_list = []\n",
    "expval_list = []\n",
    "\n",
    "for i in tqdm(range(num_epoch)):\n",
    "    opt.zero_grad()\n",
    "    loss, expval = cost(x)\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()\n",
    "    loss_list.append(loss.item())\n",
    "    expval_list.append(expval)\n",
    "    \n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "X_train = datasets.FashionMNIST(root='./data', \n",
    "                                train=True,\n",
    "                                download=True,\n",
    "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "idx = np.append(np.where(X_train.targets == 0)[0][:n_samples], \n",
    "                np.where(X_train.targets == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_train.targets == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_train.targets == 3)[0][:n_samples])\n",
    "\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = X_train.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "\n",
    "X_test = datasets.FashionMNIST(root='./data', \n",
    "                               train=False,\n",
    "                               download=True,\n",
    "                               transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "idx = np.append(np.where(X_test.targets == 0)[0][:n_samples], \n",
    "                np.where(X_test.targets == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_test.targets == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(X_test.targets == 3)[0][:n_samples])\n",
    "\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = X_test.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(X_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, NUM_QUBITS)\n",
    "        self.qc = TorchCircuit.apply\n",
    "        self.qcsim = nn.Linear(NUM_QUBITS, 1)\n",
    "        self.fc3 = nn.Linear(1, 3) # TODO: is this correct ???\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = np.pi*torch.tanh(x)\n",
    "        \n",
    "        MODE = 'QC'\n",
    "        \n",
    "        if MODE == 'QC':\n",
    "            x = self.qc(x[0])\n",
    "        else:\n",
    "            x = self.qcsim(x)\n",
    "            \n",
    "        x = torch.sigmoid(x)\n",
    "        x = torch.cat((x, 1-x), -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        pred = self.forward(x)\n",
    "        ans = torch.argmax(pred[0]).item()\n",
    "        \n",
    "        return torch.tensor(ans)\n",
    "    \n",
    "network = Net()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, optimizer, train_loader, val_loader):\n",
    "    epochs = 20\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader\n",
    "    }\n",
    "    \n",
    "    loss_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    acc_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': test_loader\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "        total_acc = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                network.train(True)\n",
    "            else:\n",
    "                network.train(False)\n",
    "\n",
    "            correct = 0\n",
    "            number = 0\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(loaders[phase]):\n",
    "                optimizer.zero_grad()\n",
    "                output = network(data)\n",
    "\n",
    "                loss = loss_func(output, target)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                total_loss[phase].append(loss.item())\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                number += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "            loss_results[phase].append(sum(total_loss[phase]) / len(total_loss[phase]))\n",
    "            acc_results[phase].append(100 * correct / number)\n",
    "\n",
    "        progress = 100.0 * (epoch + 1) / epochs\n",
    "\n",
    "        print(f'Training [{int(progress)}%]\\tTraining loss {loss_results[\"train\"][-1]}\\tValidation loss {loss_results[\"val\"][-1]}\\tTraining acc {acc_results[\"train\"][-1]}\\tValidation acc {acc_results[\"val\"][-1]}')\n",
    "        \n",
    "    return {\n",
    "        'train_loss': loss_results[\"train\"][-1],\n",
    "        'val_loss': loss_results[\"val\"][-1],\n",
    "        'train_acc': acc_results[\"train\"][-1],\n",
    "        'val_acc': acc_results[\"val\"][-1]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 1.9960261317590873\tValidation loss 1.977262465953827\tTraining acc 40.416666666666664\tValidation acc 57.5\n",
      "Training [10%]\tTraining loss 1.958449731518825\tValidation loss 1.912805671095848\tTraining acc 62.916666666666664\tValidation acc 86.0\n",
      "Training [15%]\tTraining loss 1.9297183334827424\tValidation loss 1.8983155620098113\tTraining acc 79.375\tValidation acc 92.0\n",
      "Training [20%]\tTraining loss 1.9189690507948398\tValidation loss 1.8863282656669618\tTraining acc 84.375\tValidation acc 93.5\n",
      "Training [25%]\tTraining loss 1.9092599526047707\tValidation loss 1.8821203577518464\tTraining acc 87.70833333333333\tValidation acc 95.0\n",
      "Training [30%]\tTraining loss 1.912559549510479\tValidation loss 1.8817332410812377\tTraining acc 86.875\tValidation acc 95.5\n",
      "Training [35%]\tTraining loss 1.9063032378753026\tValidation loss 1.8870529687404634\tTraining acc 90.41666666666667\tValidation acc 94.0\n",
      "Training [40%]\tTraining loss 1.9060755302508672\tValidation loss 1.8749783194065095\tTraining acc 89.58333333333333\tValidation acc 96.5\n",
      "Training [45%]\tTraining loss 1.9052818646033605\tValidation loss 1.877068977355957\tTraining acc 89.58333333333333\tValidation acc 96.5\n",
      "Training [50%]\tTraining loss 1.9011063774426777\tValidation loss 1.8844477611780166\tTraining acc 91.875\tValidation acc 95.0\n",
      "Training [55%]\tTraining loss 1.9017138682305812\tValidation loss 1.8770194774866105\tTraining acc 90.625\tValidation acc 96.0\n",
      "Training [60%]\tTraining loss 1.8981983872751396\tValidation loss 1.8768927180767059\tTraining acc 92.91666666666667\tValidation acc 96.0\n",
      "Training [65%]\tTraining loss 1.8978868119418622\tValidation loss 1.8716182470321656\tTraining acc 90.20833333333333\tValidation acc 96.5\n",
      "Training [70%]\tTraining loss 1.8919419492284457\tValidation loss 1.8715626502037048\tTraining acc 93.75\tValidation acc 96.0\n",
      "Training [75%]\tTraining loss 1.8949040333429972\tValidation loss 1.8728190034627914\tTraining acc 92.08333333333333\tValidation acc 95.5\n",
      "Training [80%]\tTraining loss 1.8934626797835032\tValidation loss 1.871851571202278\tTraining acc 93.54166666666667\tValidation acc 97.5\n",
      "Training [85%]\tTraining loss 1.893718346953392\tValidation loss 1.8712534421682359\tTraining acc 92.91666666666667\tValidation acc 96.5\n",
      "Training [90%]\tTraining loss 1.8939943119883538\tValidation loss 1.8709584385156632\tTraining acc 92.29166666666667\tValidation acc 97.0\n",
      "Training [95%]\tTraining loss 1.8909100731213888\tValidation loss 1.8743957161903382\tTraining acc 94.79166666666667\tValidation acc 97.0\n",
      "Training [100%]\tTraining loss 1.891784430295229\tValidation loss 1.8719949054718017\tTraining acc 91.66666666666667\tValidation acc 97.5\n",
      "{'train_loss': 1.891784430295229, 'val_loss': 1.8719949054718017, 'train_acc': 91.66666666666667, 'val_acc': 97.5}\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 1.9970610305666923\tValidation loss 1.9595920419692994\tTraining acc 39.791666666666664\tValidation acc 53.5\n",
      "Training [10%]\tTraining loss 1.962106064458688\tValidation loss 1.9320932912826538\tTraining acc 61.25\tValidation acc 71.0\n",
      "Training [15%]\tTraining loss 1.9457423135638237\tValidation loss 1.9162454116344452\tTraining acc 70.20833333333333\tValidation acc 80.0\n",
      "Training [20%]\tTraining loss 1.9311518790821234\tValidation loss 1.9017369937896729\tTraining acc 79.375\tValidation acc 91.0\n",
      "Training [25%]\tTraining loss 1.9235757055381935\tValidation loss 1.8837052530050278\tTraining acc 82.29166666666667\tValidation acc 93.0\n",
      "Training [30%]\tTraining loss 1.9192206015189488\tValidation loss 1.885473872423172\tTraining acc 85.0\tValidation acc 90.5\n",
      "Training [35%]\tTraining loss 1.9156507541735968\tValidation loss 1.8804146099090575\tTraining acc 85.20833333333333\tValidation acc 94.0\n",
      "Training [40%]\tTraining loss 1.9133495474855104\tValidation loss 1.8817641806602479\tTraining acc 86.25\tValidation acc 94.5\n",
      "Training [45%]\tTraining loss 1.9079475753009318\tValidation loss 1.8754161924123764\tTraining acc 88.125\tValidation acc 94.5\n",
      "Training [50%]\tTraining loss 1.9051785310109457\tValidation loss 1.874995569586754\tTraining acc 88.54166666666667\tValidation acc 95.5\n",
      "Training [55%]\tTraining loss 1.9092123543222745\tValidation loss 1.8800253295898437\tTraining acc 88.54166666666667\tValidation acc 94.0\n",
      "Training [60%]\tTraining loss 1.902858782062928\tValidation loss 1.877840247154236\tTraining acc 88.125\tValidation acc 93.5\n",
      "Training [65%]\tTraining loss 1.9029963140686352\tValidation loss 1.881851287484169\tTraining acc 88.95833333333333\tValidation acc 95.0\n",
      "Training [70%]\tTraining loss 1.9019189702967803\tValidation loss 1.8788140779733657\tTraining acc 89.16666666666667\tValidation acc 94.5\n",
      "Training [75%]\tTraining loss 1.8990797077616055\tValidation loss 1.876138436794281\tTraining acc 90.625\tValidation acc 94.5\n",
      "Training [80%]\tTraining loss 1.8981745208303133\tValidation loss 1.8758513790369034\tTraining acc 90.0\tValidation acc 94.5\n",
      "Training [85%]\tTraining loss 1.898723434160153\tValidation loss 1.8765337240695954\tTraining acc 89.375\tValidation acc 95.0\n",
      "Training [90%]\tTraining loss 1.8973026086886724\tValidation loss 1.8736498695611954\tTraining acc 90.83333333333333\tValidation acc 95.0\n",
      "Training [95%]\tTraining loss 1.8962386262913544\tValidation loss 1.8728004068136215\tTraining acc 92.70833333333333\tValidation acc 95.5\n",
      "Training [100%]\tTraining loss 1.8981210256616274\tValidation loss 1.8737014293670655\tTraining acc 91.45833333333333\tValidation acc 94.5\n",
      "{'train_loss': 1.8981210256616274, 'val_loss': 1.8737014293670655, 'train_acc': 91.45833333333333, 'val_acc': 94.5}\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 2.0200222223997115\tValidation loss 2.0192063200473784\tTraining acc 25.0\tValidation acc 21.0\n",
      "Training [10%]\tTraining loss 2.0201559474070865\tValidation loss 2.019204468727112\tTraining acc 21.875\tValidation acc 24.5\n",
      "Training [15%]\tTraining loss 2.0189956846336523\tValidation loss 2.019090629816055\tTraining acc 27.291666666666668\tValidation acc 31.0\n",
      "Training [20%]\tTraining loss 2.0177819547553857\tValidation loss 2.0191865944862366\tTraining acc 28.125\tValidation acc 28.0\n",
      "Training [25%]\tTraining loss 2.0215209941069285\tValidation loss 2.019341535568237\tTraining acc 22.5\tValidation acc 21.0\n",
      "Training [30%]\tTraining loss 2.018723270545403\tValidation loss 2.019301943778992\tTraining acc 29.166666666666668\tValidation acc 22.0\n",
      "Training [35%]\tTraining loss 2.019323795537154\tValidation loss 2.0191126477718355\tTraining acc 22.5\tValidation acc 20.5\n",
      "Training [40%]\tTraining loss 2.019638010611137\tValidation loss 2.0192487704753876\tTraining acc 24.166666666666668\tValidation acc 22.5\n",
      "Training [45%]\tTraining loss 2.0189372318486374\tValidation loss 2.01915354013443\tTraining acc 28.333333333333332\tValidation acc 25.5\n",
      "Training [50%]\tTraining loss 2.019429925084114\tValidation loss 2.0192162466049193\tTraining acc 25.625\tValidation acc 20.0\n",
      "Training [55%]\tTraining loss 2.019767027099927\tValidation loss 2.019403775930405\tTraining acc 24.583333333333332\tValidation acc 20.5\n",
      "Training [60%]\tTraining loss 2.01982278401653\tValidation loss 2.0191670942306517\tTraining acc 26.25\tValidation acc 28.5\n",
      "Training [65%]\tTraining loss 2.0187961628039677\tValidation loss 2.0193560218811033\tTraining acc 25.833333333333332\tValidation acc 21.5\n",
      "Training [70%]\tTraining loss 2.0191064243515333\tValidation loss 2.0192270565032957\tTraining acc 26.666666666666668\tValidation acc 28.0\n",
      "Training [75%]\tTraining loss 2.019541922956705\tValidation loss 2.0193574154376983\tTraining acc 20.833333333333332\tValidation acc 18.0\n",
      "Training [80%]\tTraining loss 2.0185805350542068\tValidation loss 2.0165441155433657\tTraining acc 25.416666666666668\tValidation acc 32.5\n",
      "Training [85%]\tTraining loss 2.018825319657723\tValidation loss 2.019305431842804\tTraining acc 23.125\tValidation acc 20.5\n",
      "Training [90%]\tTraining loss 2.0204044955472154\tValidation loss 2.0191552770137786\tTraining acc 26.041666666666668\tValidation acc 25.0\n",
      "Training [95%]\tTraining loss 2.0196106414000194\tValidation loss 2.0192505955696105\tTraining acc 24.166666666666668\tValidation acc 24.0\n",
      "Training [100%]\tTraining loss 2.0181556276977064\tValidation loss 2.0191927218437193\tTraining acc 26.25\tValidation acc 23.5\n",
      "{'train_loss': 2.0181556276977064, 'val_loss': 2.0191927218437193, 'train_acc': 26.25, 'val_acc': 23.5}\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [5%]\tTraining loss 1.991338657339414\tValidation loss 1.9561380356550218\tTraining acc 43.75\tValidation acc 53.5\n",
      "Training [10%]\tTraining loss 1.9465832531452179\tValidation loss 1.9041492629051209\tTraining acc 70.20833333333333\tValidation acc 85.0\n",
      "Training [15%]\tTraining loss 1.9276346042752266\tValidation loss 1.8965466386079788\tTraining acc 79.16666666666667\tValidation acc 87.0\n",
      "Training [20%]\tTraining loss 1.9224071842928727\tValidation loss 1.883171892762184\tTraining acc 81.04166666666667\tValidation acc 92.0\n",
      "Training [25%]\tTraining loss 1.9153539019326369\tValidation loss 1.8956623256206513\tTraining acc 87.08333333333333\tValidation acc 90.0\n",
      "Training [30%]\tTraining loss 1.9144542902708053\tValidation loss 1.8910938614606858\tTraining acc 85.625\tValidation acc 94.0\n",
      "Training [35%]\tTraining loss 1.9093256396551928\tValidation loss 1.8795673149824141\tTraining acc 86.04166666666667\tValidation acc 95.5\n",
      "Training [40%]\tTraining loss 1.9100022725760937\tValidation loss 1.8777551525831222\tTraining acc 87.70833333333333\tValidation acc 94.5\n",
      "Training [45%]\tTraining loss 1.902327964703242\tValidation loss 1.8740382182598114\tTraining acc 90.41666666666667\tValidation acc 96.0\n",
      "Training [50%]\tTraining loss 1.9046332488457363\tValidation loss 1.884687789082527\tTraining acc 88.54166666666667\tValidation acc 93.0\n",
      "Training [55%]\tTraining loss 1.9009785383939743\tValidation loss 1.8728051197528839\tTraining acc 90.20833333333333\tValidation acc 95.5\n",
      "Training [60%]\tTraining loss 1.9012600208322208\tValidation loss 1.8745355504751204\tTraining acc 91.25\tValidation acc 95.0\n",
      "Training [65%]\tTraining loss 1.898611590017875\tValidation loss 1.8782074439525605\tTraining acc 91.66666666666667\tValidation acc 95.0\n",
      "Training [70%]\tTraining loss 1.8972951124111812\tValidation loss 1.8713686668872833\tTraining acc 91.45833333333333\tValidation acc 96.0\n",
      "Training [75%]\tTraining loss 1.8989138508836427\tValidation loss 1.8723379045724868\tTraining acc 91.04166666666667\tValidation acc 96.0\n",
      "Training [80%]\tTraining loss 1.9000777860482534\tValidation loss 1.8717868894338607\tTraining acc 91.45833333333333\tValidation acc 96.5\n",
      "Training [85%]\tTraining loss 1.8959384448826313\tValidation loss 1.8732518285512925\tTraining acc 91.875\tValidation acc 95.5\n",
      "Training [90%]\tTraining loss 1.8950830556452274\tValidation loss 1.874656878709793\tTraining acc 93.125\tValidation acc 96.5\n",
      "Training [95%]\tTraining loss 1.8935709881285827\tValidation loss 1.869369659423828\tTraining acc 93.54166666666667\tValidation acc 97.0\n",
      "Training [100%]\tTraining loss 1.8955194873114427\tValidation loss 1.87444715321064\tTraining acc 92.5\tValidation acc 96.0\n",
      "{'train_loss': 1.8955194873114427, 'val_loss': 1.87444715321064, 'train_acc': 92.5, 'val_acc': 96.0}\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 1.9896509724358717\tValidation loss 1.9470614117383958\tTraining acc 44.791666666666664\tValidation acc 61.5\n",
      "Training [10%]\tTraining loss 1.9609843586881957\tValidation loss 1.9270940136909485\tTraining acc 62.083333333333336\tValidation acc 71.0\n",
      "Training [15%]\tTraining loss 1.9459161872665087\tValidation loss 1.914467574954033\tTraining acc 68.33333333333333\tValidation acc 76.5\n",
      "Training [20%]\tTraining loss 1.9311331989864509\tValidation loss 1.8889993053674698\tTraining acc 78.54166666666667\tValidation acc 92.5\n",
      "Training [25%]\tTraining loss 1.9194570330282053\tValidation loss 1.8964981311559677\tTraining acc 81.875\tValidation acc 92.0\n",
      "Training [30%]\tTraining loss 1.9137266762554646\tValidation loss 1.8850732606649399\tTraining acc 86.66666666666667\tValidation acc 96.0\n",
      "Training [35%]\tTraining loss 1.9139841164151827\tValidation loss 1.878988311290741\tTraining acc 86.25\tValidation acc 95.5\n",
      "Training [40%]\tTraining loss 1.9065502936641374\tValidation loss 1.8793015474081038\tTraining acc 87.91666666666667\tValidation acc 95.5\n",
      "Training [45%]\tTraining loss 1.907301872720321\tValidation loss 1.8808459067344665\tTraining acc 88.75\tValidation acc 96.0\n",
      "Training [50%]\tTraining loss 1.9075980218748252\tValidation loss 1.8745377427339553\tTraining acc 89.375\tValidation acc 97.5\n",
      "Training [55%]\tTraining loss 1.9017539707322915\tValidation loss 1.8788432037830354\tTraining acc 91.45833333333333\tValidation acc 95.5\n",
      "Training [60%]\tTraining loss 1.9007937801380952\tValidation loss 1.8794901746511459\tTraining acc 91.25\tValidation acc 97.0\n",
      "Training [65%]\tTraining loss 1.9011133576432864\tValidation loss 1.8744886767864228\tTraining acc 91.25\tValidation acc 97.0\n",
      "Training [70%]\tTraining loss 1.897020581861337\tValidation loss 1.8720537286996841\tTraining acc 92.5\tValidation acc 97.5\n",
      "Training [75%]\tTraining loss 1.8973226949572564\tValidation loss 1.875975831747055\tTraining acc 91.66666666666667\tValidation acc 97.0\n",
      "Training [80%]\tTraining loss 1.8986034395794074\tValidation loss 1.86991745531559\tTraining acc 91.25\tValidation acc 98.0\n",
      "Training [85%]\tTraining loss 1.8959883632759253\tValidation loss 1.8686039495468139\tTraining acc 92.29166666666667\tValidation acc 98.5\n",
      "Training [90%]\tTraining loss 1.8973717346787453\tValidation loss 1.869793837070465\tTraining acc 91.66666666666667\tValidation acc 98.0\n",
      "Training [95%]\tTraining loss 1.8933056741952896\tValidation loss 1.8695873206853866\tTraining acc 92.5\tValidation acc 98.0\n",
      "Training [100%]\tTraining loss 1.8946951086322465\tValidation loss 1.8706822204589844\tTraining acc 92.70833333333333\tValidation acc 96.0\n",
      "{'train_loss': 1.8946951086322465, 'val_loss': 1.8706822204589844, 'train_acc': 92.70833333333333, 'val_acc': 96.0}\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "FINAL RESULTS\n",
      "--------------------------------\n",
      "{0: {'train_loss': 1.891784430295229, 'val_loss': 1.8719949054718017, 'train_acc': 91.66666666666667, 'val_acc': 97.5}, 1: {'train_loss': 1.8981210256616274, 'val_loss': 1.8737014293670655, 'train_acc': 91.45833333333333, 'val_acc': 94.5}, 2: {'train_loss': 2.0181556276977064, 'val_loss': 2.0191927218437193, 'train_acc': 26.25, 'val_acc': 23.5}, 3: {'train_loss': 1.8955194873114427, 'val_loss': 1.87444715321064, 'train_acc': 92.5, 'val_acc': 96.0}, 4: {'train_loss': 1.8946951086322465, 'val_loss': 1.8706822204589844, 'train_acc': 92.70833333333333, 'val_acc': 96.0}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "\n",
    "dataset = ConcatDataset([X_train, X_test])\n",
    "\n",
    "k_folds = 5\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "results = {}\n",
    "\n",
    "print('--------------------------------')\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "  \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=1, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=1, sampler=test_subsampler)\n",
    "\n",
    "    network = Net()\n",
    "    network.apply(reset_weights)\n",
    "    \n",
    "    optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "    \n",
    "    result = train_network(network, optimizer, trainloader, testloader)\n",
    "    \n",
    "    results[fold] = result\n",
    "    \n",
    "    print(result)\n",
    "    print('--------------------------------')\n",
    "\n",
    "print('--------------------------------')\n",
    "print('FINAL RESULTS')\n",
    "print('--------------------------------')\n",
    "print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 25\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# loss_results = {\n",
    "#     'train': [],\n",
    "#     'val': []\n",
    "# }\n",
    "\n",
    "# acc_results = {\n",
    "#     'train': [],\n",
    "#     'val': []\n",
    "# }\n",
    "\n",
    "# loaders = {\n",
    "#     'train': train_loader,\n",
    "#     'val': test_loader\n",
    "# }\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = {\n",
    "#         'train': [],\n",
    "#         'val': []\n",
    "#     }\n",
    "    \n",
    "#     total_acc = {\n",
    "#         'train': [],\n",
    "#         'val': []\n",
    "#     }\n",
    "    \n",
    "    \n",
    "#     for phase in ['train', 'val']:\n",
    "#         if phase == 'train':\n",
    "#             network.train(True)\n",
    "#         else:\n",
    "#             network.train(False)\n",
    "        \n",
    "#         correct = 0\n",
    "#         number = 0\n",
    "        \n",
    "#         for batch_idx, (data, target) in enumerate(loaders[phase]):\n",
    "#             optimizer.zero_grad()\n",
    "#             output = network(data)\n",
    "            \n",
    "#             loss = loss_func(output, target)\n",
    "            \n",
    "#             if phase == 'train':\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             total_loss[phase].append(loss.item())\n",
    "\n",
    "#             _, predicted = torch.max(output.data, 1)\n",
    "#             number += target.size(0)\n",
    "#             correct += (predicted == target).sum().item()\n",
    "\n",
    "#         loss_results[phase].append(sum(total_loss[phase]) / len(total_loss[phase]))\n",
    "#         acc_results[phase].append(100 * correct / number)\n",
    "\n",
    "#     progress = 100.0 * (epoch + 1) / epochs\n",
    "    \n",
    "#     print(f'Training [{int(progress)}%]\\tTraining loss {loss_results[\"train\"][-1]}\\tValidation loss {loss_results[\"val\"][-1]}\\tTraining acc {acc_results[\"train\"][-1]}\\tValidation acc {acc_results[\"val\"][-1]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_results[\"train\"])\n",
    "# plt.title(f'Hybrid NN training convergence for {NUM_QUBITS}-qubits')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_results[\"val\"])\n",
    "# plt.title(f'Hybrid NN training convergence for {NUM_QUBITS}-qubits')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = 0\n",
    "# number = 0\n",
    "# for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#     number += 1\n",
    "#     output = network.predict(data).item()\n",
    "#     accuracy += (output == target[0].item())*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Performance on test data: {accuracy}/{number} = {accuracy/number*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_sample_shape = (8, 6)\n",
    "# count = 0\n",
    "# fig, axes = plt.subplots(nrows=n_sample_shape[0], ncols=n_sample_shape[1], figsize=(10, 2*n_sample_shape[0]))\n",
    "\n",
    "# classes_map = {\n",
    "#     0: 'T-shirt/top',\n",
    "#     1: 'Trouser',\n",
    "#     2: 'Pullover',\n",
    "#     3: 'Dress',\n",
    "#     4: 'Coat',\n",
    "#     5: 'Sandal',\n",
    "#     6: 'Shirt',\n",
    "#     7: 'Sneaker',\n",
    "#     8: 'Bag',\n",
    "#     9: 'Ankle boot'\n",
    "# }\n",
    "\n",
    "# network.eval()\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#         if count == n_sample_shape[0]*n_sample_shape[1]:\n",
    "#             break\n",
    "        \n",
    "#         prediction = network.predict(data).item()\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].imshow(data[0].numpy().squeeze(), cmap='gray')\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_xticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_yticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_title(classes_map[prediction])\n",
    "        \n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
