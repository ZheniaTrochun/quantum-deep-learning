{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: qiskit in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.23.2)\n",
      "Requirement already satisfied: qiskit-ibmq-provider==0.11.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.11.1)\n",
      "Requirement already satisfied: qiskit-aqua==0.8.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.8.1)\n",
      "Requirement already satisfied: qiskit-terra==0.16.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.16.1)\n",
      "Requirement already satisfied: qiskit-aer==0.7.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.7.2)\n",
      "Requirement already satisfied: qiskit-ignis==0.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit) (0.5.1)\n",
      "Requirement already satisfied: websockets>=8 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (8.1)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.25.10)\n",
      "Requirement already satisfied: numpy>=1.13 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.19.2)\n",
      "Requirement already satisfied: nest-asyncio!=1.1.0,>=1.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (2.8.1)\n",
      "Requirement already satisfied: requests>=2.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (2.24.0)\n",
      "Requirement already satisfied: requests-ntlm>=1.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-ibmq-provider==0.11.1->qiskit) (1.1.0)\n",
      "Requirement already satisfied: yfinance in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.1.55)\n",
      "Requirement already satisfied: docplex in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.19.202)\n",
      "Requirement already satisfied: h5py in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (2.10.0)\n",
      "Requirement already satisfied: scipy>=1.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.5.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.23.2)\n",
      "Requirement already satisfied: pandas in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.1.3)\n",
      "Requirement already satisfied: psutil>=5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (5.8.0)\n",
      "Requirement already satisfied: quandl in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (3.6.0)\n",
      "Requirement already satisfied: sympy>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.7.1)\n",
      "Requirement already satisfied: dlx in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (1.0.4)\n",
      "Requirement already satisfied: retworkx>=0.5.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.7.2)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (50.3.0)\n",
      "Requirement already satisfied: fastdtw in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aqua==0.8.1->qiskit) (0.3.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.14.5)\n",
      "Requirement already satisfied: ply>=3.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.11)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (0.3.3)\n",
      "Requirement already satisfied: python-constraint>=1.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (1.4.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (3.2.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-terra==0.16.1->qiskit) (2.5)\n",
      "Requirement already satisfied: cython>=0.27.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (0.29.21)\n",
      "Requirement already satisfied: pybind11>=2.4 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from qiskit-aer==0.7.2->qiskit) (2.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from python-dateutil>=2.8.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.15.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.11.1->qiskit) (2020.6.20)\n",
      "Requirement already satisfied: ntlm-auth>=1.0.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.5.0)\n",
      "Requirement already satisfied: cryptography>=1.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (3.3.1)\n",
      "Requirement already satisfied: lxml>=4.5.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (4.6.2)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from yfinance->qiskit-aqua==0.8.1->qiskit) (0.0.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.20.0->qiskit-aqua==0.8.1->qiskit) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from scikit-learn>=0.20.0->qiskit-aqua==0.8.1->qiskit) (0.17.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from pandas->qiskit-aqua==0.8.1->qiskit) (2020.1)\n",
      "Requirement already satisfied: more-itertools in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (8.6.0)\n",
      "Requirement already satisfied: inflection>=0.3.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from quandl->qiskit-aqua==0.8.1->qiskit) (0.5.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from sympy>=1.3->qiskit-aqua==0.8.1->qiskit) (1.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from jsonschema>=2.6->qiskit-terra==0.16.1->qiskit) (20.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from networkx>=2.2->qiskit-terra==0.16.1->qiskit) (4.4.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (1.14.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.11.1->qiskit) (2.20)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pylatexenc in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (2.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torchvision in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (0.8.2)\n",
      "Requirement already satisfied: torch==1.7.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.7.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from torch==1.7.1->torchvision) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (4.56.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/jupyterlab/2.2.8/libexec/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for itertools\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/2.2.8/libexec/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install qiskit\n",
    "!{sys.executable} -m pip install pylatexenc\n",
    "!{sys.executable} -m pip install torchvision\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import execute\n",
    "from qiskit.circuit import Parameter,ControlledGate\n",
    "from qiskit import Aer\n",
    "import qiskit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "\n",
    "NUM_QUBITS = 3\n",
    "NUM_SHOTS = 5000\n",
    "SHIFT = np.pi/4\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "SIMULATOR = Aer.get_backend('qasm_simulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '001', '010', '011', '100', '101', '110', '111']\n"
     ]
    }
   ],
   "source": [
    "# create list of all possible outputs of quantum circuit (2**NUM_QUBITS possible)\n",
    "import itertools\n",
    "\n",
    "def create_QC_OUTPUTS():\n",
    "    measurements = list(itertools.product([0, 1], repeat=NUM_QUBITS))\n",
    "    return [''.join([str(bit) for bit in measurement]) for measurement in measurements]\n",
    "\n",
    "QC_OUTPUTS = create_QC_OUTPUTS()\n",
    "print(QC_OUTPUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contruct QuantumCircuit QFT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumCircuit:\n",
    "    def __init__(self, n_qubits, backend, shots):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.circuit = qiskit.QuantumCircuit(n_qubits)\n",
    "        self.thetas = { k: Parameter('Theta'+str(k)) for k in range(self.n_qubits) }\n",
    "        \n",
    "        all_qubits = [i for i in range(n_qubits)]\n",
    "        \n",
    "        self.circuit.h(all_qubits)\n",
    "        self.circuit.barrier()\n",
    "        for k in range(n_qubits):\n",
    "            self.circuit.ry(self.thetas[k], k)\n",
    "            \n",
    "        self.circuit.measure_all()\n",
    "        \n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "        \n",
    "    def N_qubits_expectation_Z(self, counts, shots, n_qubits):\n",
    "        expects = np.zeros(len(QC_OUTPUTS))\n",
    "        for k in range(len(QC_OUTPUTS)):\n",
    "            key = QC_OUTPUTS[k]\n",
    "            perc = counts.get(key, 0) / shots\n",
    "            expects[k] = perc\n",
    "        return expects\n",
    "    \n",
    "    def run(self, i):\n",
    "        params = i\n",
    "        \n",
    "        job_sim = execute(self.circuit, \n",
    "                          self.backend, \n",
    "                          shots = self.shots, \n",
    "                          parameter_binds = [{ self.thetas[k]: params[k].item() for k in range(self.n_qubits) }])\n",
    "        \n",
    "        result_sim = job_sim.result()\n",
    "        counts = result_sim.get_counts(self.circuit)\n",
    "        \n",
    "        return self.N_qubits_expectation_Z(counts, self.shots, self.n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value for rotation [pi/4]: [0.0032 0.0214 0.0196 0.1062 0.0142 0.1112 0.1068 0.6174]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAADWCAYAAACe0fZLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAppElEQVR4nO3de1RU5cIG8GcYuYsXJEBQUkBQQG7eTQWEFK+oiXnt0zQU8YhpmRdUtDQzFDqllJpx/DJL8FpLTEyYjmElJiamIaQhakkqCgkml+8PPuc0ymXwjPPuYT+/tVpr3PPu2c+858DD3rP3HkV1dXU1iIiIBDMSHYCIiAhgIRERkUSwkIiISBJYSEREJAksJCIikgQWEhERSQILiYiIJIGFREREksBCIiIiSWAhERGRJLCQiIhIElhIREQkCSwkIiKSBBYSERFJAguJiIgkgYVERESSwEIiIiJJYCEREZEksJCIiEgSmokOQE3H+fPnGxzz3nvvYc6cOfWO6dy5s64iNVmca2qKuIdEerVx40bREWSDc02GhoVERESSwEIiIiJJYCGRXqWkpIiOIBucazI0LCQiIpIEFhLp1dixY0VHkA3ONRkanvYt0J4s4MotMdt2bA2M6S5m2yL8fBQouS5m21a2gPtAMdsWYd68ecjOztb7dn19fZGQkKD37ZLusJAEunILyBf0S1JuSq4DxYWiU8hDdnY2VCqV6BhkgHjIjvQqKipKdATZ4FyToWEhkV41dOcA0h3ONRkaFhLp1YABA0RHkA3ONRkaFhLpVVFRkegIssG5JkPDkxqI/t+CxECc+/U4lEpjGBkpYd+6IyYGL0WAT7joaESywEIivfLw8BAdoV6TQpZhUkgMKisrsD/zPbz5yUS4OvrB0cZVdLRGk/pcEz2Mh+xIr3bv3i06glaUymYY0uslVFZVIP9qtug4j8VQ5vpxKBQKKJVK0TFIx7iHRHq1fPlyrFq1SnSMBt2v+AtfZCYCANrZuAlO83gMYa6VSiWCgoLQq1cv+Pj4oGXLlrh//z7y8vJw8uRJHDp06JHPwhQKBd5//320bt0aEydOREVFhaD0pGssJNKr5ORkSf+S/OSr1UhWxaHsXgmUSmPMD98KZwdvAMCaHRMx0G8iensMBwCsSBqFEX1mo7v7IJGR6yTluTYxMUF0dDTmzJkDJyenOsf99ddf6veRm5urLqOIiAiUlZXBy8tLyF0h6MmQ5SG7qqoqxMXFoVOnTjAzM4OPjw9UKhXc3d0REREhOl6dUt4IxPf73tB6OTXexOCl2Pd6MVJi/0DPzkNxOi9d/VxkWAKSvlyGsnul+PeZPbA0aynZMpIyX19fZGVlYd26dXBycsKFCxewYcMGTJ48GaGhoQgLC8OSJUtw8OBBKJVKTJo0CdnZ2ViwYIFGGQ0fPpxl1MTIcg9p+vTp2LNnD5YtW4Zu3bohMzMTEyZMQFFREebPny86HkmAlUVrzA/fiv9Z64LMnP3o6xWG1s1tMbpfNDbun4v8q9l4K+KI6JgGZ+DAgThw4AAsLS1x4cIFREdH49ChQ6iurtYYd+DAAQCAk5MTVq5cialTpyIuLg4A1GV09OhRveenJ0t2e0g7d+5EUlISDhw4gFdeeQVBQUFYunQp+vTpg4qKCvj7+4uO2KQZ0j3OWlhY47n+87Ht0BJUVVUBAAb3mIrColyMemYuWlhYC05YP6nNtZeXl7qMtm/fDh8fH6Smpj5SRn9XUFCAF198EV9++aV62d69e1lGTZTsCmnNmjUIDQ1FQECAxnJXV1cYGxvD27vm84JLly4hICAAbm5u6Nq1K/7973+LiNvknD17VnSERhndPxo371xD2snt6mUObVwN4jRwKc11s2bN8K9//QuWlpbYsWMHpk6dirKysgbXe/CZ0eDBg3Hv3j1UVlZi/Pjx6Nu3rx5Sk77J6pBdYWEhcnJy8PLLLz/yXEFBATw9PWFqagoAmDlzJp5//nnMnj0bmZmZCA8Px8WLF2FiYtLgdhQKhVZ5nluajnZdAhv1Hr7fvxonD8ZpLLtfXgonr5BGvY5KlYG5g4IatU5DapvXh8XHxzc4Lj4+XleR1OJmpcPHJbDeMesjMx5ZZmnWAntW3fyvtq1SZaDHBPnMdW1eeukl+Pv74+LFi5g1a1a9e0UPPHwCw/DhwxEcHIwlS5Zg06ZN8PX11RivUqm0/tkj/dLmf29AhoUEAPb29hrLy8rKoFKpMGTIEADAH3/8gWPHjqmPY/ft2xcODg5IT0/H4MGD9Rv6IT3DlqLnqBiNZSlvBIoJQ6SlBzd6XbhwIUpLSxscX1sZHT16FMeOHcP06dPh4+ODZ555Bt98882Tjk56JKtCsrGxAQDk5uZi6NCh6uXr1q3DtWvX0K1bNwA1e0t2dnbqvSUA6NixI3799VettqPtXwPvpon7PqSAgECkvKFdTm2dP3++wTHx8fENnsm4YcMGXUVSy/pUd9+HtHB8UqPGBwQEojpRPnMdGBio8fmVv78/PDw8cPXqVezbt6/B9esqI6DmNPAtW7YgJiYGL7zwgkYhBQQEICMjQ9dvh/RIVoXk7OwMb29vrFmzBtbW1nB0dERKSgoOHjwIAOpCoidn5cqVoiPIhlTmumfPngCAtLS0Bi9ira+MHkhNTUVMTAx69OjxxDKTGLI6qcHIyAjJycnw9PREZGQkpk2bBhsbG0RFRUGpVKpPaHBycsLvv/+Oe/fuqde9ePEinn76aVHRm4xx48aJjiAbUpnrrl27AgBOnTpV7zhtygiA+tojLy8vfmbUxMhqDwkA3NzckJ6errFsypQp8PDwgLm5OYCaQ3vPPPMMPvzwQ/VJDVeuXEFQkG4/mG6ssTEZjVouRV26dMG5c+dEx5AFqcz1d999BxMTE2RlZdU7bv78+Q2WEQDcvXsXW7ZsQUVFBZRKJW8d1ITIrpBqk5WVhd69e2sse//99zF16lQkJCTAxMQEO3fu1OoMO5K2P8tuY3lSGAAg7+opuDr4wd66I4L9JsHfrf4zFU/nZ8C2lRPatnGu9fnKygq8vWsafrt5Eb27DMf4gYt0nt8Qbd++Hdu3b29w3ObNmxEaGoo333yzweuMpHxHFXp8sjpkV5vS0lLk5uY+ckGss7Mzvv76a+Tm5iInJ+eR65bIMFmat8T6yAysj8xAR/uuWB+ZAbvW2h2KPZ2fgWs3fqnz+cyfDqC9bWckRB1DzqVjuHnnN13FloWSkhI8++yzvOhVxmS/h9S8eXNUVlaKjiEbgYGBoiPUKu3kdnyavhZ21h2wIHwrikuLsD55OsrulcDJtgsiRybgcFYSvsnZC79OIQjxn4JNB6Jx/345+niGYWLwEpz/9Vv09x4LAPBxCcL5y9+jr+dIYe9JqnNNVBfZ7yGRfiUmJoqOUCtXRz+sm3kE128VoLSsGJ+mr8WEoMWIm5UOc1MrXCg8iUHdp2Lm8PWYNWI92tu6Y/2sDLw79zv8cCEN9+6XobS8GBZmLQAAlmYt8WdZsdD3JNW5JqoLC4n0KjIyUnSEWnWw9wIAtGnhgD/Lb6Pg+jlsTV2EBYmBOJX3FW7cuaox/rebF7H0w6GYnxiAgt/Pobj0OizNWuJu+R0AwN3yO7A0b6Xvt6FBqnNNVBfZH7Ij/ZLqhYsK/Of04erqarR/yh3B/pPh1q7m2rTKygoUFJ1HZXXN4d3Pjyfi+aDX4OMSiHkb+6G6uhpdnu6DU3lfobNTT5zOT0eQ3wQh7+UBqc41UV1YSES1mDBwCRJSIvBn+W0oFEaYH74VPs6B2Ja6GOcLvkOvzsPw3t45cLLzgLGy5uzLPh4j8PZnuzFvYz/07DwUbVq0FfwuiAwLC4lkKyHqGADghUGx6mV/vy1Q7NS9GuPtrTtgw+yv1f/u0Tn0kddcPHGHbkMSyQg/QyK9ksKFmnLBuSZDw0Iivdq1a5foCLLBuSZDw0N2Ajm2lt+2V6xYIeQea1a2et+k8G2LmuuHv6dIG78UXAMAODu11Xj8pLdL0sJCEmhMd9EJ5MN9oOgE8pGQkNDodRa9tRkAsPa1CI3HJC88ZEdERJLAQiK92rRpk+gIssG5JkPDQiK98vT0FB1BNjjXZGhYSKRXvGu6/nCuydCwkIiISBJYSEREJAksJNKrHj16iI4gG5xrMjQsJNKrEydOiI4gG5xrMjQsJCIikgQWEhERSQILifQqJSVFdATZ4FyToWEhERGRJLCQSK/Gjh0rOoJscK7J0PBu3wLtyQKu3BKzbcfW8rrb+M9HgZLrYrZtZcu7jUvdvHnzkJ2dLWTbvr6+j3WH9KaIhSTQlVtAvqBfknJTch0oLhSdgqQqOzsbKpVKdAzZ4yE70quoqCjREWSDc02GhoVEejVnzhzREWSDc02GhoVEejVgwADREWSDc02Ghp8hkV4VFRWJjlCnBYmBOPfrcSiVxjAyUsK+dUdMDF6KAJ9w0dEei5Tnmqg2LCSiv5kUsgyTQmJQWVmB/Znv4c1PJsLV0Q+ONq6ioxE1eTxkR3rl4eEhOoJWlMpmGNLrJVRWVSD/arboOI/FUOaa6AEWEunV7t27RUfQyv2Kv/BFZiIAoJ2Nm+A0j8dQ5tpQKRQKuLq6YuDAgQgJCYG/vz9MTEzqHG9sbIyXX34ZzZrxwFRdWEikV8uXLxcdoV6ffLUao5a1wvAl5vjoyxjMD98KZwdvAMCaHRPx7U9fqMeuSBqFrJ8Pi4raIKnPtaEKCgrCrl27cOvWLVy4cAFfffUV0tLScPLkSZSWliIzMxPTpk2Dubm5eh1jY2N89tln2LBhAxITEwWmlzbZFlJVVRXi4uLQqVMnmJmZwcfHByqVCu7u7oiIiBAdr8lKTk4WHaFeE4OXYt/rxUiJ/QM9Ow/F6bx09XORYQlI+nIZyu6V4t9n9sDSrCW6uw8SmLZ+Up9rQ+Pi4gKVSoWjR48iPDwcLVu2xNWrV5Geno4jR47g3LlzUCqV6NOnD7Zt24a8vDwMHz5cXUajR4/GzZs3sWnTJtFvRbJku+84ffp07NmzB8uWLUO3bt2QmZmJCRMmoKioCPPnzxcdr1YpbwTCySsEPUfFaLWcHp+VRWvMD9+K/1nrgsyc/ejrFYbWzW0xul80Nu6fi/yr2Xgr4ojomKQno0ePxscffwwLCwtcv34dGzduRFJSEgoKCjTGNW/eHGPGjEF0dDT8/f3x+eefIz8/Hy4uLrh58yZCQkJw6tQpQe9C+mS5h7Rz504kJSXhwIEDeOWVVxAUFISlS5eiT58+qKiogL+/v+iIJAEtLKzxXP/52HZoCaqqqgAAg3tMRWFRLkY9MxctLKwFJyR9GDVqFJKTk2FhYYEdO3agc+fOWLVq1SNlBAClpaXYvn07evTogQULFqCyshIuLi4oLy9nGWlBloW0Zs0ahIaGIiAgQGO5q6srjI2N4e1d85nB8uXL4ebmBiMjI363jI4Y2v3CRvePxs0715B2crt6mUMbV4M4DdzQ5lqKnn76aWzfvh1KpRKrV6/G5MmTcetWw3dEViqV6NevH5RKJaqrq2FmZgYvLy89JDZssiukwsJC5OTkIDz80YsdCwoK4OnpCVNTUwBAaGgoDh06xCvedejs2bOiI9RpfWQGJoVoHva0NGuBPatuYnCPqWJC/RekPNeG4oMPPoCVlRVSUlIQE6PdIfGHPzNasWIFAOCdd96BnZ3dk4xr8GT3GVJhYc0tn+3t7TWWl5WVQaVSYciQIeplffv2faxtKBQKrcY9tzQd7boENuq1v9+/GicPxmksu19eCievkEa9jkqVgbmDghq1TkNefvnlBsfEx8c3OC4+Pl5XkdTiZqXDxyVQ56+rDZUqAz0myGeuH8draz8AUPOz8/fHInXr1g2DBw9GcXExZs+erdU6D5fRg8N0ffv2RWhoKGbNmoWVK1dqrKNSqYS/1yeturpaq3Gy20OysbEBAOTm5mosX7duHa5du4Zu3bqJiKW1nmFLEbm5WOM/B7d+omPJysLxSfDqyDlv6mbNmgUA+PDDD7W6DVNdZQQAa9euBQBERERAqVQ+udAGTnZ7SM7OzvD29saaNWtgbW0NR0dHpKSk4ODBgwCgk0LS9q+Bd9PEfR9SQEAgUt7QLqe2zp8/3+CY+Pj4Bk+r37Bhg64iqWV9Ku77kAICAlGdKJ+5fhyL3toMoOZn5++P9SUwMPCRz9yCg4MBANu3b69tFQ31lRFQsxdUUFAAJycndO7cWeNwakBAADIyMnTzRgyc7PaQjIyMkJycDE9PT0RGRmLatGmwsbFBVFQUlEql+oQGejIePlxBTw7n+vFZW1ujY8eOuHv3boOfxTVURg98//33AHTzR29TJbs9JABwc3NDenq6xrIpU6bAw8ND4+pq0r1x48aJjiAbnOvH17ZtWwDApUuXUFlZWec4bcsIAPLy8gAADg4Oug/cRMiykGqTlZWF3r17ayxbtmwZPvroIxQVFeHMmTOYN28eVCoVXFxchGQcG5PRqOVS1KVLF5w7d07Y9v8su43lSWEAgLyrp+Dq4Ad7644I9psEf7f6Tww5nZ8B21ZOaNvGudbn866cwlufvoCyeyX4eMklXUdvNNFzbch++uknWFlZNXjfOVNTU9ja2mp10evrr7+ON998E3fv3tV13CZDdofsalNaWorc3NxHLoh9/fXXUVhYiHv37uHGjRsoLCwUVkakG5bmLbE+MgPrIzPQ0b4r1kdmwK7101qtezo/A9du/FLn8w5tXPHPf3wLm5btdBWXBKmurkZpaSmKi4vrHVdaWoohQ4YgICCgwYte7969izt37qCiokKHSZsW7iGh5nYf9e2WU9OXdnI7Pk1fCzvrDlgQvhXFpUVYnzwdZfdK4GTbBZEjE3A4Kwnf5OyFX6cQhPhPwaYD0bh/vxx9PMMwMXgJLMysRL8NEqCkpAQ5OTmiYzQJ3EMivQoMDBQdoVaujn5YN/MIrt8qQGlZMT5NX4sJQYsRNysd5qZWuFB4EoO6T8XM4esxa8R6tLd1x/pZGXh37nf44UIa7t0vE/0WHiHVuSaqC/eQSK+keuv9DvY1t3Vp08IBf5bfRsH1c9iauggKKFD2Vyk6t++pMf63mxfxwecLUH7/Lgqv/4zi0utaH/rTF6nONVFduIdEehUZGSk6Qq0U+M+V8tXV1Wj/lDtmjdiA9ZEZ2BSdhb6eYVAqjVFZXXNo9/PjiXg+6DVsiFTBwcZVr9fMaEuqc01UF+4hkV4ZygWAEwYuQUJKBP4svw2Fwgjzw7fCxzkQ21IX43zBd+jVeRje2zsHTnYeMFbWfEvo9eLLiPtsGi79loOFH4RgfvhW2Ft3EPYeDGWuiR5gIZFsJUQdAwC8MChWvWzh+CT149ipezXG21t3wIbZX6v/3aNz6COvuW4mvyOJ6HHxkB0REUkCC4n0ihdq6g/nmgwND9kJ5NhaftvetWuXkFvaWNnqfZPCty1qrg2Rr6/vY633S8E1AICzU1uNx/rYdlPEQhJoTHfRCfRvxYoVQn5Jug/U+yaFEzXXhighIeGx1ntwZ/K1r0VoPKbHw0N2REQkCSwkIiKSBBYS6dWmTZtER5ANzjUZGhYS6ZWnp6foCLLBuSZDw0IivQoICBAdQTY412RoWEhERCQJLCTSqx49eoiOIBucazI0LCTSqxMnToiOIBucazI0LCQiIpIEFhIREUkCC4n0KiUlRXQE2eBck6FhIRERkSSwkEivxo4dKzqCbHCuydDwbt8C7ckCrtwSs23H1vK62/jPR4GS62K2bWUrz7uN05M3b948ZGdn6327vr6+j32H9PqwkAS6cgvIF/RLUm5KrgPFhaJTEOlWdnY2VCqV6Bg6w0N2pFdRUVGiI8gG55oMDQuJ9GrOnDmiI8gG55oMDQuJ9GrAgAGiI8gG55oMDQuJ9KqoqEh0BNngXJOh4UkNRP9vQWIgzv16HEqlMYyMlLBv3RETg5ciwCdcdDQiWWAhkV55eHiIjlCvSSHLMCkkBpWVFdif+R7e/GQiXB394GjjKjpao0l9rokexkN2pFe7d+8WHUErSmUzDOn1EiqrKpB/NVt0nMdiKHNN0mZpaYlmzfSz78JCIr1avny56AhauV/xF77ITAQAtLNxE5zm8RjKXJN+tG7dGjNmzMDmzZtx/Phx/Pjjjzhx4gQ+/vhjzJs3Dx06dHhkHUtLS6SmpmLnzp16KSXZFlJVVRXi4uLQqVMnmJmZwcfHByqVCu7u7oiIiBAdr8lKTk4WHaFen3y1GqOWtcLwJeb46MsYzA/fCmcHbwDAmh0T8e1PX6jHrkgahayfD4uK2iCpzzXph62tLbZs2YIrV65gy5YteOmll9C7d2907doV3bt3x6RJkxAfH4/8/HwcOHAAXl5eAP5TRv3790evXr1gZ2f3xLPKtpCmT5+O119/HTNnzkRqairGjRuHCRMm4JdffkG3bt1Ex6tVyhuB+H7fG1ovp8abGLwU+14vRkrsH+jZeShO56Wrn4sMS0DSl8tQdq8U/z6zB5ZmLdHdfZDAtET1e+655/DTTz9hxowZMDc3x5dffol58+YhICAA3t7e6NOnDyIiIvDJJ5/g/v37GDFiBE6ePIkVK1aoy+jy5csICgrClStXnnheWZ7UsHPnTiQlJSEjIwMBAQEAgKCgIPzwww/Ys2cP/P39BSck0awsWmN++Fb8z1oXZObsR1+vMLRubovR/aKxcf9c5F/NxlsRR0THJKrT7NmzsXHjRgDA4cOHMWfOHFy4cOGRcd9++y22bNmC6OhorFq1CpGRkYiNjQUAdRnl5+frJbMs95DWrFmD0NBQdRk94OrqCmNjY3h7e+PWrVsYPnw43Nzc4OPjg0GDBiEvL09Q4qbDkO671cLCGs/1n49th5agqqoKADC4x1QUFuVi1DNz0cLCWnDC+hnSXJNujRw5Ul1Gr776KgYPHlxrGf3dH3/8gVdffRU5OTnqZfv27dNbGQEyLKTCwkLk5OQgPPzRa0sKCgrg6ekJU1NTKBQKzJs3D7m5uTh9+jSGDx+OadOmCUjctJw9e1Z0hEYZ3T8aN+9cQ9rJ7eplDm1cDeI0cEOba9KNNm3aYPPmzQCARYsWIS4uTqv1Hnxm5OXlhaKiIlRVVSEyMlKvH2HI7pBdYWHNLZ/t7e01lpeVlUGlUmHIkCEAgFatWiEkJET9fN++fbFu3TqttqFQKLQa99zSdLTrEqjV2Ae+378aJw9q/h/sfnkpnLxC6lijdipVBuYOCmrUOg15+eWXGxwTHx/f4Lj4+HhdRVKLm5UOH5fAesesj8x4ZJmlWQvsWXXzv9q2SpWBHhPkM9eP47W1HwCo+dn5+2Opk2LumJgY2NnZISMjQ+vfWX8/geHBYbrZs2dj/vz5SEhIQP/+/TXGq1SqRr3P6upqrcbJbg/JxsYGAJCbm6uxfN26dbh27Vqdfw0kJCRg1KhRTzpeg3qGLUXk5mKN/xzc+omORUQSYGFhgalTpwKo+aNFmyKorYzy8/OxfPly3L59G/369YO3t/cTTl5DdntIzs7O8Pb2xpo1a2BtbQ1HR0ekpKTg4MGDAFBrIa1cuRJ5eXk4evSoVtvQ9q+Bd9PEfR9SQEAgUt7QLqe2zp8/3+CY+Pj4Bk+r37Bhg64iqWV9qrvvQ1o4PqlR4wMCAlGdKJ+5fhyL3qo5xFRdXa3xWOpE5w4MDNT4rDA0NBStWrXCt99+q9UX99VVRgDw559/4n//938xZ84cjB8/Hj/++KN6vYCAAGRkZOj67chvD8nIyAjJycnw9PREZGQkpk2bBhsbG0RFRUGpVD7yl8Abb7yBL774AocOHYKFhYWg1E3HypUrRUeQDc61/HTvXvM10GlpaQ2Ora+MHjhy5IjG6z5psttDAgA3Nzekp6drLJsyZQo8PDxgbm6uXrZy5UocPHgQaWlpaNWqlZ5TNk3jxo0THUE2ONfy4+npCQA4ffp0veO0KSMA6r2sB6/7pMmykGqTlZWF3r17q/999uxZxMbGwsXFBYGBgerlIr6//oGxMRmNWi5FXbp0wblz50THkAXOtfzs2rULP/74o8bhtdqsXr26wTICgOvXr2PNmjUoLi5+AmkfxUICUFpaitzcXMyePVu9zNPT0yCOYVPj/Fl2G8uTwgAAeVdPwdXBD/bWHRHsNwn+bvWfqXg6PwO2rZzQto1zrc8f/+lzfPLVaiigQH/vsQgPWKDz/ET12bFjh1bjli1bBgcHByxevLje64zKysqwdOlSXcVrEAsJQPPmzVFZWSk6BumBpXlL9end8zb2w/rIDGw/HKvVuqfzM+DVoV+dheTS1gcJUd/ASGGEBe8HYmjPGbA0b6mj5ES6U1JSIslDuiwk0qu/H/6UkrST2/Fp+lrYWXfAgvCtKC4twvrk6Si7VwIn2y6IHJmAw1lJ+CZnL/w6hSDEfwo2HYjG/fvl6OMZhonBS2Db2kn9ekqjZlAoxJ4zJNW5JqqL7M6yI7ESExNFR6iVq6Mf1s08guu3ClBaVoxP09diQtBixM1Kh7mpFS4UnsSg7lMxc/h6zBqxHu1t3bF+VgbenfsdfriQhnv3y9Sv9f35VDi0cYGFmZXAdyTduSaqCwuJ9CoyMlJ0hFp1sK+55X6bFg74s/w2Cq6fw9bURViQGIhTeV/hxp2rGuN/u3kRSz8civmJASj4/RyKS2suKLt24xfsyliHWSPF3wFBqnNNVBcesiO9ehIX0+mCAv+5DUp1dTXaP+WOYP/JcGtXc6F0ZWUFCorOo7K65rPGz48n4vmg1+DjEoh5G/uhuroad8tL8PZnU/Hq80kwN7EU8j7+TqpzTVQXFhJRLSYMXIKElAj8WX4bCoUR5odvhY9zILalLsb5gu/Qq/MwvLd3DpzsPGCsNAEA7M98D7/dvIi4XS8CAF55/iO0te4o8m0QGRQWEslWQtQxAMALg2LVy/5+W6DYqXs1xttbd8CG2V+r/92jc6jG8xMGLsaEgYt1H5RIJvgZEukVL9TUH841GRoWEunVrl27REeQDc41GRoeshPIsbX8tr1ixQohF+RZ2ep9k8K3LWquSX98fX0bvc4vBdcAAM5ObTUeP+ntaoOFJNAY/dxAlwC4DxSdgEj3EhISGr3Og6/JWPtahMZjKeAhOyIikgQWEunVpk2bREeQDc41GRoWEumVvr5XhTjXZHhYSKRXAQEBoiPIBueaDA0LiYiIJIGFREREksDTvklnOnfu3OCYFStWaDWO6se5pqaIe0ikV7GxsaIjyAbnmgwNC4mIiCSBhURERJLAQiIiIklgIRERkSSwkIiISBJYSEREJAksJAEGDRoEX19fdO3aFWPHjsWdO3dERyIyaBkZGfD09ISrqytmzJiByspK0ZEaFB0djXbt2qFZM8O5HPTy5csIDg5Gly5d4OnpicWLF+v09VlIAiQnJyM7OxtnzpxBu3btsGHDBtGRiAxWVVUVZsyYgeTkZOTl5eHOnTv4+OOPRcdqUHh4OLKyskTHaJRmzZrhrbfewrlz53Dq1CkcO3YM+/fv19nrs5AEaNmyJYCaH6Ty8nIoFArBiYgM14kTJ+Dg4AAPDw8AwPTp07F7927BqRrWr18/2Nvbi47RKG3btkX37jXfLGpiYgI/Pz8UFBTo7PVZSIKMHj0atra2+Pnnn7FgwQLRcYgMVmFhIdq3b6/+t5OTEy5fviwwkTzcvHkT+/btw7PPPquz1zScg5dNzN69e/HXX39h+vTpSElJwdSpU0VHItKb+xUV2L77MErvlmksf+ej3bU+HhrUC506tKv1taqrq59MyFp8f/o8jv9w9pHlteVua2uN8KGBwo+A3C0rx792f4m/7ldoLK9rrsOHBsDBzqbe1/zrr78wduxYREdH6/R+idxDEsjExATjx4/H3r17RUch0ivjZs3Qp5snrl2/gWvXb6iXP/z42vUbaNXCEq5PO9b5Wu3bt9fYIyooKEC7drWX13/Lz9MVVVVVDea+fuMWBvT0EV5GAGBhbgZ/Lzet5rp926caLKPKykpMnDgRvr6+Oj+6w0LSs5KSEly7dg1AzWdIBw4c4Dd7kix5uD6N7t7u9Y6xNDfDmNAB9f5i7969OwoLC/HTTz8BAD788EOMGTNGp1kfMG7WDM+PGAilUf2/Ogf17wH7p6yfSIbH0dOnMzq7ONU7xrqVFYYN7NPga0VERMDKygrr16/XVTw1FpKelZSUYOTIkfD29oa3tzcqKioQExMjOhaRECMG9oF1S6s6nx8TOgBWlhb1voZSqcTWrVsxduxYuLi4oHnz5pgyZYquo6o52LbBs/271/l8x/Zt0b9H1wZfZ+bMmWjXrh0qKyvRrl07REVF6TKmBoVCgedCB8DC3LTO558fFgRTE+N6X+ebb77Btm3bkJWVBT8/P/j6+uKf//yn7nJW6/MALNWrurpaErv4RPp0qfA3fLDjAB7+RdStqxvChwaKiNSgqqoqbN75BS4V/qax3NTEGNEvjq23ZEXK+fkiPt6X9sjyoD6+GDygp4BEmriHJCH7Dh/DgSPfiI5BpFcd2tkjoLevxrLWLa0wIrivmEBaMDIywrhhgTB5aI9iREhfyZYRAHi5d4S/l5vGMge7Ngh+ppugRJokU0ixsbFQKBTIycnBsGHD0Lx5c7Rt2xZvv/02ACA1NRX+/v6wsLCAn58fjh07prF+ZmYmBg8ejJYtW8Lc3Bz9+/d/ZExWVhbGjRsHJycnmJubw9XVFf/4xz9w+/ZtjXF5eXkYO3Ys7O3tYWpqCkdHR4wcORI3btzAk3Kj+A5O/HgeAPeQSH5C+nVDW9s2AGp+AsKHBcLM1ERsqAZYt2qBEcH/+czFo1MHdHvol70UjQzpi1YtmgMAmimVeH5YEJoplYJT1ZBMIT0QHh6OgQMHqs9vX7hwIRYtWoRXX30VCxcuRHJyMqqrqxEWFoaSkhIAwOHDhxEYWHN65UcffYSUlBRYWVkhODgYJ06cUL/2pUuX0LVrV7z33ns4dOgQFi9ejNTUVAwdOlQjw7Bhw/Drr7/i3XffRVpaGuLj42FnZ4eyMs1TVHUp/fgpGCmMENjL54ltg0iqmimVeH54EJRKI/Tv6Q3n9m1FR9JK967u6OL6NJpbmGNMaH+DOORuZmqC8GGBUAAYHNADdhI6+UIynyHFxsZi5cqVSExMxKxZswAA9+7dg52dHe7evYvc3Fx06NABAHD06FEEBwcjJSUFzz33HNzc3GBjY4Njx47B6P/PfqmoqICXlxecnZ1x8ODBWrdZUVGB48ePY8CAATh16hR8fX3xxx9/4KmnnsK+ffsQFhb2WO9l0VubH2s9IqKmaO1rEVqNk9we0t/3VkxNTeHs7IwuXbqoywiA+kKsy5cvIy8vDxcuXMDkyZNRVVWFiooKVFTUXAAWEhIClUqlXq+0tBQxMTHo1KkTzMzMYGxsjAEDBgAAfv75ZwBAmzZt4OzsjEWLFmHz5s04f/78k37LREQECd6pwdpac/fRxMQEZmZmjywDgPLycvz+++8AgKioqDpPmywrK4O5uTlefPFFpKamIjY2Fv7+/rCyssLly5cxZswY9eE4hUKBI0eOYNWqVYiJiUFRUZH6lMzXXntNq11ybf8aAGo+O1q/5TP09vPEyBDpfohLRPSkSa6QGqtNm5oPQmNjYzFs2LBax5iamqK8vBx79+7F8uXLNa4ufviEBgDo2LEjPvroI1RXV+Ps2bPYtm0bFi9eDBsbG8yYMaPBTI9zyC7zZA4yT+Y0ej0iIqnT9o90gy8kd3d3ODs748yZM1ixYkWd4+7du4eKigoYG2ueprlt27Y611EoFPDy8sKGDRvw/vvv48yZMzrLTUREmgy+kBQKBd5//30MGzYMYWFhmDx5MmxtbVFUVIQffvgB9+/fx9tvv42WLVuib9++iIuLg52dHRwcHLBr1y589913Gq/3448/Yu7cuRg3bhw6deoEoOb7i8rKyjB48GCtMmn710BKqgrZZ/OwcOZ4tLCybNwbJyJqYgy+kADg2WefRWZmJlavXo3IyEiUlJTA1tYW/v7+eOmll9TjPvnkE8yZMwfz5s2DUqnE8OHD8dlnn6m/3wMA7O3t0aFDB7zzzjsoLCyEsbExunTpgl27dj1yevh/40bxHfyQk4vefp4sIyIiSOi0b7m5cLEQuw99jdmTw1hIRERgIQlVVVWlvm6KiEjuWEhERCQJ/POciIgkgYVERESSwEIiIiJJYCEREZEksJCIiEgSWEhERCQJLCQiIpIEFhIREUkCC4mIiCSBhURERJLAQiIiIklgIRERkSSwkIiISBJYSEREJAksJCIikgQWEhERSQILiYiIJIGFREREksBCIiIiSWAhERGRJLCQiIhIElhIREQkCSwkIiKSBBYSERFJAguJiIgkgYVERESSwEIiIiJJ+D++wbkiawmhkgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 538.279x264.88 with 1 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, NUM_SHOTS)\n",
    "print(f'Expected value for rotation [pi/4]: {circuit.run(torch.Tensor([np.pi/4] * len(QC_OUTPUTS)))}')\n",
    "circuit.circuit.draw(output='mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCircuit(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        if not hasattr(ctx, 'QuantumCircuit'):\n",
    "            ctx.QuantumCircuit = QuantumCircuit(NUM_QUBITS, SIMULATOR, shots=NUM_SHOTS)\n",
    "        \n",
    "        exp_value = ctx.QuantumCircuit.run(i)\n",
    "        \n",
    "        result = torch.Tensor([exp_value])\n",
    "        \n",
    "        ctx.save_for_backward(result, i)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        forward_tensor,i = ctx.saved_tensors\n",
    "        \n",
    "        input_numbers = i\n",
    "        gradients = torch.Tensor()\n",
    "        \n",
    "        for k in range(NUM_QUBITS):\n",
    "            shift_right = input_numbers.detach().clone()\n",
    "            shift_right[k] = shift_right[k] + SHIFT\n",
    "            \n",
    "            shift_left = input_numbers.detach().clone()\n",
    "            shift_left[k] = shift_left[k] - SHIFT\n",
    "            \n",
    "            expectation_right = ctx.QuantumCircuit.run(shift_right)\n",
    "            expectation_left = ctx.QuantumCircuit.run(shift_left)\n",
    "            \n",
    "            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])\n",
    "            gradient = gradient / torch.norm(gradient)\n",
    "            \n",
    "            gradients = torch.cat((gradients, gradient.float()))\n",
    "            \n",
    "        result = torch.Tensor(gradients)\n",
    "        \n",
    "        return (result.float() * grad_output.float()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 after quantum layer: tensor([[0.0046, 0.0190, 0.0216, 0.0998, 0.0186, 0.1052, 0.1058, 0.6254]],\n",
      "       grad_fn=<TorchCircuitBackward>)\n",
      "x.grad = tensor([-0.1883, -0.0922, -0.0077])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "qc = TorchCircuit.apply\n",
    "\n",
    "y1 = qc(x)\n",
    "print(f'y1 after quantum layer: {y1}')\n",
    "y1 = nn.Linear(2**NUM_QUBITS, 1)(y1.float())\n",
    "\n",
    "y1.backward()\n",
    "print(f'x.grad = {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13f23fd30>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qc = TorchCircuit.apply\n",
    "\n",
    "def cost(x):\n",
    "    target = -1\n",
    "    expval = qc(x)[0]\n",
    "    \n",
    "    val = sum([(i + 1) * expval[i] for i in range(2**NUM_QUBITS)]) / 2**NUM_QUBITS\n",
    "    \n",
    "    return torch.abs(target - val) ** 2, expval\n",
    "\n",
    "\n",
    "x = torch.tensor([-np.pi/4] * NUM_QUBITS, requires_grad=True)\n",
    "opt = optim.Adam([x], lr = 0.1)\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "loss_list = []\n",
    "expval_list = []\n",
    "\n",
    "for i in tqdm(range(num_epoch)):\n",
    "    opt.zero_grad()\n",
    "    loss, expval = cost(x)\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()\n",
    "    loss_list.append(loss.item())\n",
    "    expval_list.append(expval)\n",
    "    \n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "n_samples = 300\n",
    "\n",
    "X_train = datasets.CIFAR10(root='./data', \n",
    "                                train=True,\n",
    "                                download=True,\n",
    "                                transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "\n",
    "idx = np.append(np.where(np.array(X_train.targets) == 0)[0][:n_samples], \n",
    "                np.where(np.array(X_train.targets) == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 3)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 4)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_train.targets) == 5)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 6)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 7)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 8)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_train.targets) == 9)[0][:n_samples])\n",
    "\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = np.array(X_train.targets)[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "n_samples = 50\n",
    "\n",
    "X_test = datasets.CIFAR10(root='./data', \n",
    "                               train=False,\n",
    "                               download=True,\n",
    "                               transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "\n",
    "\n",
    "idx = np.append(np.where(np.array(X_test.targets) == 0)[0][:n_samples], \n",
    "                np.where(np.array(X_test.targets) == 1)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 2)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 3)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 4)[0][:n_samples])\n",
    "idx = np.append(idx, \n",
    "                np.where(np.array(X_test.targets) == 5)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 6)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 7)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 8)[0][:n_samples])\n",
    "# idx = np.append(idx, \n",
    "#                 np.where(np.array(X_test.targets) == 9)[0][:n_samples])\n",
    "\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = np.array(X_test.targets)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(X_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, NUM_QUBITS)\n",
    "        self.qc = TorchCircuit.apply\n",
    "        \n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "#         self.conv2_drop = nn.Dropout2d()\n",
    "# #         self.fc1 = nn.Linear(320, 50)\n",
    "#         self.fc1 = nn.Linear(500, 320)\n",
    "#         self.fc2 = nn.Linear(320, 50)\n",
    "#         self.fc3 = nn.Linear(50, NUM_QUBITS)\n",
    "#         self.qc = TorchCircuit.apply\n",
    "#         self.qcsim = nn.Linear(NUM_QUBITS, 1)\n",
    "#         self.fc4 = nn.Linear(1, 3) # TODO: is this correct ???\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        x = np.pi*torch.tanh(x)\n",
    "        x = self.qc(x[0])\n",
    "        \n",
    "        x = torch.sigmoid(x)\n",
    "        x = torch.cat((x, 1-x), -1)\n",
    "        \n",
    "#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "# #         x = x.view(-1, 320)\n",
    "# #         x = x.view(-1, 500)\n",
    "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.fc3(x)\n",
    "#         x = np.pi*torch.tanh(x)\n",
    "        \n",
    "#         MODE = 'QC'\n",
    "        \n",
    "#         if MODE == 'QC':\n",
    "#             x = self.qc(x[0])\n",
    "#         else:\n",
    "#             x = self.qcsim(x)\n",
    "            \n",
    "#         x = torch.sigmoid(x)\n",
    "#         x = torch.cat((x, 1-x), -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        pred = self.forward(x)\n",
    "        ans = torch.argmax(pred[0]).item()\n",
    "        \n",
    "        return torch.tensor(ans)\n",
    "    \n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, optimizer, train_loader, val_loader):\n",
    "    epochs = 20\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader\n",
    "    }\n",
    "    \n",
    "    loss_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    acc_results = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    loaders = {\n",
    "        'train': train_loader,\n",
    "        'val': test_loader\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "        total_acc = {\n",
    "            'train': [],\n",
    "            'val': []\n",
    "        }\n",
    "\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                network.train(True)\n",
    "            else:\n",
    "                network.train(False)\n",
    "\n",
    "            correct = 0\n",
    "            number = 0\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(loaders[phase]):\n",
    "                optimizer.zero_grad()\n",
    "                output = network(data)\n",
    "\n",
    "                loss = loss_func(output, target)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                total_loss[phase].append(loss.item())\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                number += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "            loss_results[phase].append(sum(total_loss[phase]) / len(total_loss[phase]))\n",
    "            acc_results[phase].append(100 * correct / number)\n",
    "\n",
    "        progress = 100.0 * (epoch + 1) / epochs\n",
    "\n",
    "        print(f'Training [{int(progress)}%]\\tTraining loss {loss_results[\"train\"][-1]}\\tValidation loss {loss_results[\"val\"][-1]}\\tTraining acc {acc_results[\"train\"][-1]}\\tValidation acc {acc_results[\"val\"][-1]}')\n",
    "        \n",
    "    return {\n",
    "        'train_loss': loss_results[\"train\"][-1],\n",
    "        'val_loss': loss_results[\"val\"][-1],\n",
    "        'train_acc': acc_results[\"train\"][-1],\n",
    "        'val_acc': acc_results[\"val\"][-1]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 2.7274805696237654\tValidation loss 2.7190953612327577\tTraining acc 19.821428571428573\tValidation acc 21.333333333333332\n",
      "Training [10%]\tTraining loss 2.7078147852704637\tValidation loss 2.702365806102753\tTraining acc 30.416666666666668\tValidation acc 32.666666666666664\n",
      "Training [15%]\tTraining loss 2.6967708326521374\tValidation loss 2.6892659680048623\tTraining acc 36.30952380952381\tValidation acc 40.333333333333336\n",
      "Training [20%]\tTraining loss 2.6888011201506568\tValidation loss 2.687601314385732\tTraining acc 40.833333333333336\tValidation acc 42.0\n",
      "Training [25%]\tTraining loss 2.6830232139144625\tValidation loss 2.6861025563875836\tTraining acc 42.023809523809526\tValidation acc 41.0\n",
      "Training [30%]\tTraining loss 2.680793339297885\tValidation loss 2.681037576198578\tTraining acc 43.92857142857143\tValidation acc 42.666666666666664\n",
      "Training [35%]\tTraining loss 2.676082209604127\tValidation loss 2.679033055305481\tTraining acc 45.11904761904762\tValidation acc 43.666666666666664\n",
      "Training [40%]\tTraining loss 2.6721460050060637\tValidation loss 2.6833750875790914\tTraining acc 47.44047619047619\tValidation acc 41.666666666666664\n",
      "Training [45%]\tTraining loss 2.66982907752196\tValidation loss 2.6751509507497153\tTraining acc 46.845238095238095\tValidation acc 45.666666666666664\n",
      "Training [50%]\tTraining loss 2.6660197116079782\tValidation loss 2.6713100051879883\tTraining acc 48.98809523809524\tValidation acc 48.333333333333336\n",
      "Training [55%]\tTraining loss 2.664469161487761\tValidation loss 2.662853802839915\tTraining acc 49.226190476190474\tValidation acc 50.333333333333336\n",
      "Training [60%]\tTraining loss 2.6603858592964356\tValidation loss 2.671313842932383\tTraining acc 51.25\tValidation acc 46.0\n",
      "Training [65%]\tTraining loss 2.6562444327842623\tValidation loss 2.661702151298523\tTraining acc 52.73809523809524\tValidation acc 50.333333333333336\n",
      "Training [70%]\tTraining loss 2.654804080157053\tValidation loss 2.664417963027954\tTraining acc 53.57142857142857\tValidation acc 49.333333333333336\n",
      "Training [75%]\tTraining loss 2.652921245637394\tValidation loss 2.661683301925659\tTraining acc 54.464285714285715\tValidation acc 49.333333333333336\n",
      "Training [80%]\tTraining loss 2.6498129851761316\tValidation loss 2.6588135504722596\tTraining acc 54.88095238095238\tValidation acc 52.333333333333336\n",
      "Training [85%]\tTraining loss 2.6469603237651644\tValidation loss 2.657761789162954\tTraining acc 57.083333333333336\tValidation acc 52.0\n",
      "Training [90%]\tTraining loss 2.679039987637883\tValidation loss 2.7358898973464965\tTraining acc 42.32142857142857\tValidation acc 16.666666666666668\n",
      "Training [95%]\tTraining loss 2.7349787320409504\tValidation loss 2.7357677173614503\tTraining acc 17.083333333333332\tValidation acc 16.666666666666668\n",
      "Training [100%]\tTraining loss 2.734945143404461\tValidation loss 2.735814126332601\tTraining acc 17.083333333333332\tValidation acc 16.666666666666668\n",
      "{'train_loss': 2.734945143404461, 'val_loss': 2.735814126332601, 'train_acc': 17.083333333333332, 'val_acc': 16.666666666666668}\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 2.727288224725496\tValidation loss 2.7194656546910605\tTraining acc 20.05952380952381\tValidation acc 23.666666666666668\n",
      "Training [10%]\tTraining loss 2.7067675011498586\tValidation loss 2.7008814295132955\tTraining acc 30.416666666666668\tValidation acc 35.666666666666664\n",
      "Training [15%]\tTraining loss 2.6948490034966244\tValidation loss 2.6918387047449746\tTraining acc 36.964285714285715\tValidation acc 38.333333333333336\n",
      "Training [20%]\tTraining loss 2.685995101219132\tValidation loss 2.6814793443679807\tTraining acc 41.30952380952381\tValidation acc 45.333333333333336\n",
      "Training [25%]\tTraining loss 2.678300251705306\tValidation loss 2.6733513776461284\tTraining acc 45.29761904761905\tValidation acc 47.666666666666664\n",
      "Training [30%]\tTraining loss 2.673744246931303\tValidation loss 2.6725909543037414\tTraining acc 46.607142857142854\tValidation acc 49.0\n",
      "Training [35%]\tTraining loss 2.6703114474103566\tValidation loss 2.664843185742696\tTraining acc 48.63095238095238\tValidation acc 49.666666666666664\n",
      "Training [40%]\tTraining loss 2.6655302074693497\tValidation loss 2.6656813351313273\tTraining acc 50.0\tValidation acc 50.0\n",
      "Training [45%]\tTraining loss 2.6612412117776416\tValidation loss 2.663172690073649\tTraining acc 52.20238095238095\tValidation acc 52.0\n",
      "Training [50%]\tTraining loss 2.6596522236154194\tValidation loss 2.6626710057258607\tTraining acc 52.55952380952381\tValidation acc 50.666666666666664\n",
      "Training [55%]\tTraining loss 2.65639846041089\tValidation loss 2.6568821239471436\tTraining acc 53.45238095238095\tValidation acc 53.333333333333336\n",
      "Training [60%]\tTraining loss 2.726018080966813\tValidation loss 2.7358220394452415\tTraining acc 21.19047619047619\tValidation acc 16.666666666666668\n",
      "Training [65%]\tTraining loss 2.7354079178401403\tValidation loss 2.735821843147278\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [70%]\tTraining loss 2.7353709438017435\tValidation loss 2.735809777577718\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [75%]\tTraining loss 2.7353815582536516\tValidation loss 2.735750472545624\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [80%]\tTraining loss 2.7354145788011097\tValidation loss 2.7357746322949725\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [85%]\tTraining loss 2.7353965073823927\tValidation loss 2.735834990342458\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [90%]\tTraining loss 2.735392723906608\tValidation loss 2.7358276422818504\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [95%]\tTraining loss 2.7353849588405517\tValidation loss 2.73584934314092\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [100%]\tTraining loss 2.7354035573346276\tValidation loss 2.7358613133430483\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "{'train_loss': 2.7354035573346276, 'val_loss': 2.7358613133430483, 'train_acc': 16.785714285714285, 'val_acc': 16.666666666666668}\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 2.7319414622726894\tValidation loss 2.726406486829122\tTraining acc 17.857142857142858\tValidation acc 19.333333333333332\n",
      "Training [10%]\tTraining loss 2.7086871159928188\tValidation loss 2.704592091242472\tTraining acc 30.11904761904762\tValidation acc 33.0\n",
      "Training [15%]\tTraining loss 2.6933920357908523\tValidation loss 2.6898325022061664\tTraining acc 37.38095238095238\tValidation acc 40.0\n",
      "Training [20%]\tTraining loss 2.6850565972782316\tValidation loss 2.680628933906555\tTraining acc 42.023809523809526\tValidation acc 43.333333333333336\n",
      "Training [25%]\tTraining loss 2.678525919999395\tValidation loss 2.678964759508769\tTraining acc 44.464285714285715\tValidation acc 43.666666666666664\n",
      "Training [30%]\tTraining loss 2.674191688923609\tValidation loss 2.6716900396347047\tTraining acc 45.357142857142854\tValidation acc 47.666666666666664\n",
      "Training [35%]\tTraining loss 2.6697750708886554\tValidation loss 2.6729159212112426\tTraining acc 47.857142857142854\tValidation acc 47.666666666666664\n",
      "Training [40%]\tTraining loss 2.681001003867104\tValidation loss 2.7357524196306864\tTraining acc 42.32142857142857\tValidation acc 16.666666666666668\n",
      "Training [45%]\tTraining loss 2.736136518489747\tValidation loss 2.735818173090617\tTraining acc 16.547619047619047\tValidation acc 16.666666666666668\n",
      "Training [50%]\tTraining loss 2.736109924032575\tValidation loss 2.735814407666524\tTraining acc 16.547619047619047\tValidation acc 16.666666666666668\n",
      "Training [55%]\tTraining loss 2.7361418251480374\tValidation loss 2.7358308259646096\tTraining acc 16.547619047619047\tValidation acc 16.666666666666668\n",
      "Training [60%]\tTraining loss 2.736137091403916\tValidation loss 2.7358116141955056\tTraining acc 16.547619047619047\tValidation acc 16.666666666666668\n",
      "Training [65%]\tTraining loss 2.736147386403311\tValidation loss 2.735871535142263\tTraining acc 16.547619047619047\tValidation acc 16.666666666666668\n",
      "Training [70%]\tTraining loss 2.7361023779426303\tValidation loss 2.7357553966840107\tTraining acc 16.547619047619047\tValidation acc 16.666666666666668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [75%]\tTraining loss 2.736133692945753\tValidation loss 2.735773481527964\tTraining acc 16.547619047619047\tValidation acc 16.666666666666668\n",
      "Training [80%]\tTraining loss 2.736146230073202\tValidation loss 2.735729615688324\tTraining acc 16.547619047619047\tValidation acc 16.666666666666668\n",
      "Training [85%]\tTraining loss 2.736101995337577\tValidation loss 2.735801690419515\tTraining acc 16.547619047619047\tValidation acc 16.666666666666668\n",
      "Training [90%]\tTraining loss 2.736139126760619\tValidation loss 2.735820545355479\tTraining acc 16.547619047619047\tValidation acc 16.666666666666668\n",
      "Training [95%]\tTraining loss 2.73613056682405\tValidation loss 2.7358499828974407\tTraining acc 16.547619047619047\tValidation acc 16.666666666666668\n",
      "Training [100%]\tTraining loss 2.736116959793227\tValidation loss 2.7357837557792664\tTraining acc 16.547619047619047\tValidation acc 16.666666666666668\n",
      "{'train_loss': 2.736116959793227, 'val_loss': 2.7357837557792664, 'train_acc': 16.547619047619047, 'val_acc': 16.666666666666668}\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 2.728392535022327\tValidation loss 2.71158388376236\tTraining acc 19.047619047619047\tValidation acc 32.333333333333336\n",
      "Training [10%]\tTraining loss 2.706424887691225\tValidation loss 2.698162804444631\tTraining acc 31.25\tValidation acc 33.666666666666664\n",
      "Training [15%]\tTraining loss 2.6934554525784082\tValidation loss 2.69299556016922\tTraining acc 37.79761904761905\tValidation acc 37.666666666666664\n",
      "Training [20%]\tTraining loss 2.6835072474820274\tValidation loss 2.6869819418589276\tTraining acc 43.154761904761905\tValidation acc 39.333333333333336\n",
      "Training [25%]\tTraining loss 2.6800715091682616\tValidation loss 2.6844215099016826\tTraining acc 43.392857142857146\tValidation acc 41.0\n",
      "Training [30%]\tTraining loss 2.6759193389188676\tValidation loss 2.678636349042257\tTraining acc 45.29761904761905\tValidation acc 43.333333333333336\n",
      "Training [35%]\tTraining loss 2.6729429023606435\tValidation loss 2.6840712014834085\tTraining acc 46.19047619047619\tValidation acc 40.333333333333336\n",
      "Training [40%]\tTraining loss 2.6696401603165127\tValidation loss 2.6772047392527263\tTraining acc 47.61904761904762\tValidation acc 43.666666666666664\n",
      "Training [45%]\tTraining loss 2.6666276620967047\tValidation loss 2.6761624455451964\tTraining acc 48.63095238095238\tValidation acc 44.333333333333336\n",
      "Training [50%]\tTraining loss 2.6630073970272425\tValidation loss 2.6690968902905783\tTraining acc 50.17857142857143\tValidation acc 47.666666666666664\n",
      "Training [55%]\tTraining loss 2.6610465024198806\tValidation loss 2.664902896086375\tTraining acc 50.95238095238095\tValidation acc 48.333333333333336\n",
      "Training [60%]\tTraining loss 2.6767547031243644\tValidation loss 2.7357894706726076\tTraining acc 43.51190476190476\tValidation acc 16.666666666666668\n",
      "Training [65%]\tTraining loss 2.7369666112320763\tValidation loss 2.735773626168569\tTraining acc 16.13095238095238\tValidation acc 16.666666666666668\n",
      "Training [70%]\tTraining loss 2.736990827463922\tValidation loss 2.7357816076278687\tTraining acc 16.13095238095238\tValidation acc 16.666666666666668\n",
      "Training [75%]\tTraining loss 2.736987504930723\tValidation loss 2.7357834076881407\tTraining acc 16.13095238095238\tValidation acc 16.666666666666668\n",
      "Training [80%]\tTraining loss 2.736960558664231\tValidation loss 2.735759121576945\tTraining acc 16.13095238095238\tValidation acc 16.666666666666668\n",
      "Training [85%]\tTraining loss 2.7369773712896164\tValidation loss 2.7358262022336324\tTraining acc 16.13095238095238\tValidation acc 16.666666666666668\n",
      "Training [90%]\tTraining loss 2.736983746290207\tValidation loss 2.735738573074341\tTraining acc 16.13095238095238\tValidation acc 16.666666666666668\n",
      "Training [95%]\tTraining loss 2.736971155802409\tValidation loss 2.7357617584864298\tTraining acc 16.13095238095238\tValidation acc 16.666666666666668\n",
      "Training [100%]\tTraining loss 2.7369849493106204\tValidation loss 2.7358179235458375\tTraining acc 16.13095238095238\tValidation acc 16.666666666666668\n",
      "{'train_loss': 2.7369849493106204, 'val_loss': 2.7358179235458375, 'train_acc': 16.13095238095238, 'val_acc': 16.666666666666668}\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Training [5%]\tTraining loss 2.72604886335986\tValidation loss 2.7071402351061504\tTraining acc 21.845238095238095\tValidation acc 31.0\n",
      "Training [10%]\tTraining loss 2.698835102575166\tValidation loss 2.697500054041545\tTraining acc 36.25\tValidation acc 35.0\n",
      "Training [15%]\tTraining loss 2.689485260560399\tValidation loss 2.6997819089889528\tTraining acc 40.535714285714285\tValidation acc 38.0\n",
      "Training [20%]\tTraining loss 2.680843797326088\tValidation loss 2.6856504519780477\tTraining acc 44.404761904761905\tValidation acc 44.0\n",
      "Training [25%]\tTraining loss 2.67852641187963\tValidation loss 2.6829972449938455\tTraining acc 45.0\tValidation acc 45.333333333333336\n",
      "Training [30%]\tTraining loss 2.6720217159816197\tValidation loss 2.697845847606659\tTraining acc 47.976190476190474\tValidation acc 38.333333333333336\n",
      "Training [35%]\tTraining loss 2.668946706113361\tValidation loss 2.6683566037813824\tTraining acc 49.166666666666664\tValidation acc 49.333333333333336\n",
      "Training [40%]\tTraining loss 2.6653387806245257\tValidation loss 2.6698185404141745\tTraining acc 50.05952380952381\tValidation acc 48.0\n",
      "Training [45%]\tTraining loss 2.6616908262173333\tValidation loss 2.669408197402954\tTraining acc 51.607142857142854\tValidation acc 48.333333333333336\n",
      "Training [50%]\tTraining loss 2.691325688787869\tValidation loss 2.7358220156033832\tTraining acc 37.26190476190476\tValidation acc 16.666666666666668\n",
      "Training [55%]\tTraining loss 2.735578411391803\tValidation loss 2.73579483906428\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [60%]\tTraining loss 2.7355485903365273\tValidation loss 2.7357325744628906\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [65%]\tTraining loss 2.7355423231919604\tValidation loss 2.7358231703440348\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [70%]\tTraining loss 2.7355615876969837\tValidation loss 2.735797941684723\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [75%]\tTraining loss 2.735582464507648\tValidation loss 2.7358510597546895\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [80%]\tTraining loss 2.735569539382344\tValidation loss 2.7358085680007935\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [85%]\tTraining loss 2.7355714783782052\tValidation loss 2.735862547556559\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [90%]\tTraining loss 2.735556710192135\tValidation loss 2.735823856989543\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [95%]\tTraining loss 2.735541227885655\tValidation loss 2.7358009894688924\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "Training [100%]\tTraining loss 2.735564965009689\tValidation loss 2.7357614064216613\tTraining acc 16.785714285714285\tValidation acc 16.666666666666668\n",
      "{'train_loss': 2.735564965009689, 'val_loss': 2.7357614064216613, 'train_acc': 16.785714285714285, 'val_acc': 16.666666666666668}\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "FINAL RESULTS\n",
      "--------------------------------\n",
      "{0: {'train_loss': 2.734945143404461, 'val_loss': 2.735814126332601, 'train_acc': 17.083333333333332, 'val_acc': 16.666666666666668}, 1: {'train_loss': 2.7354035573346276, 'val_loss': 2.7358613133430483, 'train_acc': 16.785714285714285, 'val_acc': 16.666666666666668}, 2: {'train_loss': 2.736116959793227, 'val_loss': 2.7357837557792664, 'train_acc': 16.547619047619047, 'val_acc': 16.666666666666668}, 3: {'train_loss': 2.7369849493106204, 'val_loss': 2.7358179235458375, 'train_acc': 16.13095238095238, 'val_acc': 16.666666666666668}, 4: {'train_loss': 2.735564965009689, 'val_loss': 2.7357614064216613, 'train_acc': 16.785714285714285, 'val_acc': 16.666666666666668}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "\n",
    "dataset = ConcatDataset([X_train, X_test])\n",
    "\n",
    "k_folds = 5\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "results = {}\n",
    "\n",
    "print('--------------------------------')\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "  \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=1, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=1, sampler=test_subsampler)\n",
    "\n",
    "    network = Net()\n",
    "    network.apply(reset_weights)\n",
    "    \n",
    "    optimizer = optim.SGD(network.parameters(), lr =0.001, momentum=0.9)\n",
    "    \n",
    "    result = train_network(network, optimizer, trainloader, testloader)\n",
    "    \n",
    "    results[fold] = result\n",
    "    \n",
    "    print(result)\n",
    "    print('--------------------------------')\n",
    "\n",
    "print('--------------------------------')\n",
    "print('FINAL RESULTS')\n",
    "print('--------------------------------')\n",
    "print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 30\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# loss_results = {\n",
    "#     'train': [],\n",
    "#     'val': []\n",
    "# }\n",
    "\n",
    "# acc_results = {\n",
    "#     'train': [],\n",
    "#     'val': []\n",
    "# }\n",
    "\n",
    "# loaders = {\n",
    "#     'train': train_loader,\n",
    "#     'val': test_loader\n",
    "# }\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = {\n",
    "#         'train': [],\n",
    "#         'val': []\n",
    "#     }\n",
    "    \n",
    "#     total_acc = {\n",
    "#         'train': [],\n",
    "#         'val': []\n",
    "#     }\n",
    "    \n",
    "    \n",
    "#     for phase in ['train', 'val']:\n",
    "#         if phase == 'train':\n",
    "#             network.train(True)\n",
    "#         else:\n",
    "#             network.train(False)\n",
    "        \n",
    "#         correct = 0\n",
    "#         number = 0\n",
    "        \n",
    "#         for batch_idx, (data, target) in enumerate(loaders[phase]):\n",
    "#             optimizer.zero_grad()\n",
    "#             output = network(data)\n",
    "            \n",
    "#             loss = loss_func(output, target)\n",
    "            \n",
    "#             if phase == 'train':\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             total_loss[phase].append(loss.item())\n",
    "\n",
    "#             _, predicted = torch.max(output.data, 1)\n",
    "#             number += target.size(0)\n",
    "#             correct += (predicted == target).sum().item()\n",
    "            \n",
    "#         loss_results[phase].append(sum(total_loss[phase]) / len(total_loss[phase]))\n",
    "#         acc_results[phase].append(100 * correct / number)\n",
    "\n",
    "#     progress = 100.0 * (epoch + 1) / epochs\n",
    "    \n",
    "#     print(f'Training [{int(progress)}%]\\tTraining loss {loss_results[\"train\"][-1]}\\tValidation loss {loss_results[\"val\"][-1]}\\tTraining acc {acc_results[\"train\"][-1]}\\tValidation acc {acc_results[\"val\"][-1]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_results[\"train\"])\n",
    "# plt.title(f'Hybrid NN training convergence for {NUM_QUBITS}-qubits')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_results[\"val\"])\n",
    "# plt.title(f'Hybrid NN training convergence for {NUM_QUBITS}-qubits')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = 0\n",
    "# number = 0\n",
    "# for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#     number += 1\n",
    "#     output = network.predict(data).item()\n",
    "#     accuracy += (output == target[0].item())*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Performance on test data: {accuracy}/{number} = {accuracy/number*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_shape = (8, 6)\n",
    "# count = 0\n",
    "# fig, axes = plt.subplots(nrows=n_sample_shape[0], ncols=n_sample_shape[1], figsize=(10, 2*n_sample_shape[0]))\n",
    "\n",
    "# classes_map = {\n",
    "#     0: 'airplane',\n",
    "#     1: 'automobile',\n",
    "#     2: 'bird',\n",
    "#     3: 'cat',\n",
    "#     4: 'deer',\n",
    "#     5: 'dog',\n",
    "#     6: 'frog',\n",
    "#     7: 'horse',\n",
    "#     8: 'ship',\n",
    "#     9: 'truck'\n",
    "# }\n",
    "\n",
    "# network.eval()\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#         if count == n_sample_shape[0]*n_sample_shape[1]:\n",
    "#             break\n",
    "        \n",
    "#         prediction = network.predict(data).item()\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].imshow(data[0].numpy().squeeze().swapaxes(0,1).swapaxes(1,2))\n",
    "        \n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_xticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_yticks([])\n",
    "#         axes[count // n_sample_shape[1]][count % n_sample_shape[1]].set_title(classes_map[prediction])\n",
    "        \n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
